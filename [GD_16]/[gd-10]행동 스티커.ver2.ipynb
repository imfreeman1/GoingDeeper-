{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412e93a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'a')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4c91a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43077ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c473e1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761b8fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504da2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f1f7677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0494d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9f81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ddc9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 08:47:27,871\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.85gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=434)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=432)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e4bee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6592ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a9ebefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ced6cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=433)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7002cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c345507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e770fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "955faa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ad0a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "912925cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27afe9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4a81a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.04585862 epoch total loss 2.04585862\n",
      "Trained batch 2 batch loss 1.86478984 epoch total loss 1.95532417\n",
      "Trained batch 3 batch loss 2.19289804 epoch total loss 2.03451538\n",
      "Trained batch 4 batch loss 2.30355406 epoch total loss 2.10177517\n",
      "Trained batch 5 batch loss 2.38760185 epoch total loss 2.15894055\n",
      "Trained batch 6 batch loss 2.12926817 epoch total loss 2.15399528\n",
      "Trained batch 7 batch loss 2.21286535 epoch total loss 2.16240525\n",
      "Trained batch 8 batch loss 2.03682566 epoch total loss 2.14670777\n",
      "Trained batch 9 batch loss 2.17191029 epoch total loss 2.14950824\n",
      "Trained batch 10 batch loss 2.17840052 epoch total loss 2.15239739\n",
      "Trained batch 11 batch loss 2.1982882 epoch total loss 2.15656924\n",
      "Trained batch 12 batch loss 2.18599176 epoch total loss 2.15902114\n",
      "Trained batch 13 batch loss 2.15188217 epoch total loss 2.15847182\n",
      "Trained batch 14 batch loss 2.10197306 epoch total loss 2.15443635\n",
      "Trained batch 15 batch loss 2.06124973 epoch total loss 2.14822388\n",
      "Trained batch 16 batch loss 2.02271414 epoch total loss 2.14037943\n",
      "Trained batch 17 batch loss 1.94418573 epoch total loss 2.12883878\n",
      "Trained batch 18 batch loss 1.95115948 epoch total loss 2.11896777\n",
      "Trained batch 19 batch loss 1.99251652 epoch total loss 2.11231232\n",
      "Trained batch 20 batch loss 1.96892595 epoch total loss 2.10514307\n",
      "Trained batch 21 batch loss 1.64067209 epoch total loss 2.08302522\n",
      "Trained batch 22 batch loss 1.8615917 epoch total loss 2.07296014\n",
      "Trained batch 23 batch loss 1.91722298 epoch total loss 2.06618905\n",
      "Trained batch 24 batch loss 1.88276088 epoch total loss 2.05854607\n",
      "Trained batch 25 batch loss 1.96258855 epoch total loss 2.05470777\n",
      "Trained batch 26 batch loss 1.99768 epoch total loss 2.05251455\n",
      "Trained batch 27 batch loss 1.95140553 epoch total loss 2.04876971\n",
      "Trained batch 28 batch loss 1.86732197 epoch total loss 2.04228926\n",
      "Trained batch 29 batch loss 1.86540616 epoch total loss 2.03618979\n",
      "Trained batch 30 batch loss 1.75352788 epoch total loss 2.02676797\n",
      "Trained batch 31 batch loss 1.88024688 epoch total loss 2.02204132\n",
      "Trained batch 32 batch loss 1.85039675 epoch total loss 2.01667738\n",
      "Trained batch 33 batch loss 1.84571874 epoch total loss 2.01149678\n",
      "Trained batch 34 batch loss 1.80521655 epoch total loss 2.00542974\n",
      "Trained batch 35 batch loss 1.89859247 epoch total loss 2.00237703\n",
      "Trained batch 36 batch loss 1.89628494 epoch total loss 1.99943018\n",
      "Trained batch 37 batch loss 1.8337245 epoch total loss 1.99495161\n",
      "Trained batch 38 batch loss 1.86500525 epoch total loss 1.99153197\n",
      "Trained batch 39 batch loss 1.88645959 epoch total loss 1.98883784\n",
      "Trained batch 40 batch loss 1.80724645 epoch total loss 1.98429799\n",
      "Trained batch 41 batch loss 1.72657394 epoch total loss 1.97801208\n",
      "Trained batch 42 batch loss 1.85925269 epoch total loss 1.97518444\n",
      "Trained batch 43 batch loss 1.85594857 epoch total loss 1.97241163\n",
      "Trained batch 44 batch loss 1.64739966 epoch total loss 1.96502495\n",
      "Trained batch 45 batch loss 1.7336973 epoch total loss 1.95988429\n",
      "Trained batch 46 batch loss 1.75985026 epoch total loss 1.95553577\n",
      "Trained batch 47 batch loss 1.78385901 epoch total loss 1.95188308\n",
      "Trained batch 48 batch loss 1.77685356 epoch total loss 1.94823658\n",
      "Trained batch 49 batch loss 1.70748734 epoch total loss 1.94332337\n",
      "Trained batch 50 batch loss 1.79305482 epoch total loss 1.94031799\n",
      "Trained batch 51 batch loss 1.76718569 epoch total loss 1.93692327\n",
      "Trained batch 52 batch loss 1.75084543 epoch total loss 1.93334496\n",
      "Trained batch 53 batch loss 1.75474429 epoch total loss 1.92997515\n",
      "Trained batch 54 batch loss 1.70431447 epoch total loss 1.92579627\n",
      "Trained batch 55 batch loss 1.79352856 epoch total loss 1.92339134\n",
      "Trained batch 56 batch loss 1.77574849 epoch total loss 1.92075479\n",
      "Trained batch 57 batch loss 1.63808799 epoch total loss 1.91579568\n",
      "Trained batch 58 batch loss 1.58345425 epoch total loss 1.91006577\n",
      "Trained batch 59 batch loss 1.54689932 epoch total loss 1.9039104\n",
      "Trained batch 60 batch loss 1.50442338 epoch total loss 1.89725232\n",
      "Trained batch 61 batch loss 1.65748107 epoch total loss 1.89332151\n",
      "Trained batch 62 batch loss 1.69815612 epoch total loss 1.89017379\n",
      "Trained batch 63 batch loss 1.78129828 epoch total loss 1.8884455\n",
      "Trained batch 64 batch loss 1.76234627 epoch total loss 1.88647521\n",
      "Trained batch 65 batch loss 1.64998484 epoch total loss 1.88283694\n",
      "Trained batch 66 batch loss 1.60376942 epoch total loss 1.87860858\n",
      "Trained batch 67 batch loss 1.55107546 epoch total loss 1.87372\n",
      "Trained batch 68 batch loss 1.72434688 epoch total loss 1.8715235\n",
      "Trained batch 69 batch loss 1.72695804 epoch total loss 1.86942828\n",
      "Trained batch 70 batch loss 1.80382872 epoch total loss 1.86849129\n",
      "Trained batch 71 batch loss 1.65024018 epoch total loss 1.86541724\n",
      "Trained batch 72 batch loss 1.69795549 epoch total loss 1.86309135\n",
      "Trained batch 73 batch loss 1.52119184 epoch total loss 1.85840786\n",
      "Trained batch 74 batch loss 1.72874403 epoch total loss 1.8566556\n",
      "Trained batch 75 batch loss 1.75760698 epoch total loss 1.85533512\n",
      "Trained batch 76 batch loss 1.75071251 epoch total loss 1.85395849\n",
      "Trained batch 77 batch loss 1.70945466 epoch total loss 1.85208189\n",
      "Trained batch 78 batch loss 1.6255933 epoch total loss 1.84917819\n",
      "Trained batch 79 batch loss 1.60407341 epoch total loss 1.84607565\n",
      "Trained batch 80 batch loss 1.71377218 epoch total loss 1.84442198\n",
      "Trained batch 81 batch loss 1.73123765 epoch total loss 1.84302449\n",
      "Trained batch 82 batch loss 1.73473811 epoch total loss 1.84170401\n",
      "Trained batch 83 batch loss 1.71804762 epoch total loss 1.84021413\n",
      "Trained batch 84 batch loss 1.64678419 epoch total loss 1.83791149\n",
      "Trained batch 85 batch loss 1.72634804 epoch total loss 1.83659899\n",
      "Trained batch 86 batch loss 1.74444628 epoch total loss 1.83552742\n",
      "Trained batch 87 batch loss 1.73077905 epoch total loss 1.83432341\n",
      "Trained batch 88 batch loss 1.71852958 epoch total loss 1.83300769\n",
      "Trained batch 89 batch loss 1.70349395 epoch total loss 1.83155239\n",
      "Trained batch 90 batch loss 1.46098197 epoch total loss 1.82743502\n",
      "Trained batch 91 batch loss 1.50305486 epoch total loss 1.8238703\n",
      "Trained batch 92 batch loss 1.62330925 epoch total loss 1.82169032\n",
      "Trained batch 93 batch loss 1.61131811 epoch total loss 1.81942821\n",
      "Trained batch 94 batch loss 1.59952891 epoch total loss 1.81708884\n",
      "Trained batch 95 batch loss 1.58786011 epoch total loss 1.81467593\n",
      "Trained batch 96 batch loss 1.57844305 epoch total loss 1.81221521\n",
      "Trained batch 97 batch loss 1.67020071 epoch total loss 1.81075108\n",
      "Trained batch 98 batch loss 1.70143557 epoch total loss 1.80963552\n",
      "Trained batch 99 batch loss 1.69714117 epoch total loss 1.80849922\n",
      "Trained batch 100 batch loss 1.73160195 epoch total loss 1.8077302\n",
      "Trained batch 101 batch loss 1.72738814 epoch total loss 1.80693471\n",
      "Trained batch 102 batch loss 1.40533531 epoch total loss 1.80299747\n",
      "Trained batch 103 batch loss 1.27648008 epoch total loss 1.79788566\n",
      "Trained batch 104 batch loss 1.46959782 epoch total loss 1.79472911\n",
      "Trained batch 105 batch loss 1.65397704 epoch total loss 1.79338861\n",
      "Trained batch 106 batch loss 1.82320762 epoch total loss 1.79366994\n",
      "Trained batch 107 batch loss 1.74941611 epoch total loss 1.7932564\n",
      "Trained batch 108 batch loss 1.76857924 epoch total loss 1.793028\n",
      "Trained batch 109 batch loss 1.68990171 epoch total loss 1.79208183\n",
      "Trained batch 110 batch loss 1.69108129 epoch total loss 1.79116368\n",
      "Trained batch 111 batch loss 1.65398335 epoch total loss 1.78992772\n",
      "Trained batch 112 batch loss 1.72703838 epoch total loss 1.78936613\n",
      "Trained batch 113 batch loss 1.74141157 epoch total loss 1.78894174\n",
      "Trained batch 114 batch loss 1.74726117 epoch total loss 1.78857625\n",
      "Trained batch 115 batch loss 1.81582451 epoch total loss 1.78881323\n",
      "Trained batch 116 batch loss 1.72790468 epoch total loss 1.78828812\n",
      "Trained batch 117 batch loss 1.45224321 epoch total loss 1.78541589\n",
      "Trained batch 118 batch loss 1.56761742 epoch total loss 1.78357017\n",
      "Trained batch 119 batch loss 1.62314463 epoch total loss 1.78222191\n",
      "Trained batch 120 batch loss 1.72308731 epoch total loss 1.7817291\n",
      "Trained batch 121 batch loss 1.73319507 epoch total loss 1.78132808\n",
      "Trained batch 122 batch loss 1.66424942 epoch total loss 1.78036833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 123 batch loss 1.7440083 epoch total loss 1.78007269\n",
      "Trained batch 124 batch loss 1.73437846 epoch total loss 1.77970421\n",
      "Trained batch 125 batch loss 1.71477163 epoch total loss 1.7791847\n",
      "Trained batch 126 batch loss 1.73154235 epoch total loss 1.77880657\n",
      "Trained batch 127 batch loss 1.71863818 epoch total loss 1.77833283\n",
      "Trained batch 128 batch loss 1.71428227 epoch total loss 1.77783239\n",
      "Trained batch 129 batch loss 1.69200814 epoch total loss 1.77716708\n",
      "Trained batch 130 batch loss 1.70019817 epoch total loss 1.77657497\n",
      "Trained batch 131 batch loss 1.6800046 epoch total loss 1.77583778\n",
      "Trained batch 132 batch loss 1.60178268 epoch total loss 1.77451909\n",
      "Trained batch 133 batch loss 1.48971665 epoch total loss 1.77237773\n",
      "Trained batch 134 batch loss 1.49176121 epoch total loss 1.77028358\n",
      "Trained batch 135 batch loss 1.47361517 epoch total loss 1.76808608\n",
      "Trained batch 136 batch loss 1.6986171 epoch total loss 1.76757526\n",
      "Trained batch 137 batch loss 1.77264118 epoch total loss 1.76761234\n",
      "Trained batch 138 batch loss 1.74272895 epoch total loss 1.76743197\n",
      "Trained batch 139 batch loss 1.704916 epoch total loss 1.7669822\n",
      "Trained batch 140 batch loss 1.75032842 epoch total loss 1.76686323\n",
      "Trained batch 141 batch loss 1.7486434 epoch total loss 1.766734\n",
      "Trained batch 142 batch loss 1.65598869 epoch total loss 1.76595414\n",
      "Trained batch 143 batch loss 1.66015363 epoch total loss 1.76521432\n",
      "Trained batch 144 batch loss 1.62698042 epoch total loss 1.76425433\n",
      "Trained batch 145 batch loss 1.66510081 epoch total loss 1.76357055\n",
      "Trained batch 146 batch loss 1.66581011 epoch total loss 1.76290095\n",
      "Trained batch 147 batch loss 1.72543502 epoch total loss 1.76264608\n",
      "Trained batch 148 batch loss 1.67800558 epoch total loss 1.76207423\n",
      "Trained batch 149 batch loss 1.65480387 epoch total loss 1.76135433\n",
      "Trained batch 150 batch loss 1.6774404 epoch total loss 1.76079488\n",
      "Trained batch 151 batch loss 1.66853797 epoch total loss 1.76018393\n",
      "Trained batch 152 batch loss 1.59988618 epoch total loss 1.75912941\n",
      "Trained batch 153 batch loss 1.62671494 epoch total loss 1.75826383\n",
      "Trained batch 154 batch loss 1.56373608 epoch total loss 1.7570008\n",
      "Trained batch 155 batch loss 1.65785289 epoch total loss 1.75636125\n",
      "Trained batch 156 batch loss 1.64315867 epoch total loss 1.75563562\n",
      "Trained batch 157 batch loss 1.50272632 epoch total loss 1.75402462\n",
      "Trained batch 158 batch loss 1.59512556 epoch total loss 1.75301898\n",
      "Trained batch 159 batch loss 1.58959544 epoch total loss 1.75199115\n",
      "Trained batch 160 batch loss 1.6910795 epoch total loss 1.7516104\n",
      "Trained batch 161 batch loss 1.72741115 epoch total loss 1.75146008\n",
      "Trained batch 162 batch loss 1.63659382 epoch total loss 1.75075102\n",
      "Trained batch 163 batch loss 1.64809322 epoch total loss 1.75012136\n",
      "Trained batch 164 batch loss 1.63447094 epoch total loss 1.74941611\n",
      "Trained batch 165 batch loss 1.62359285 epoch total loss 1.74865353\n",
      "Trained batch 166 batch loss 1.61392128 epoch total loss 1.74784184\n",
      "Trained batch 167 batch loss 1.54153311 epoch total loss 1.74660647\n",
      "Trained batch 168 batch loss 1.64489782 epoch total loss 1.74600112\n",
      "Trained batch 169 batch loss 1.6073885 epoch total loss 1.74518096\n",
      "Trained batch 170 batch loss 1.63980293 epoch total loss 1.74456108\n",
      "Trained batch 171 batch loss 1.66206717 epoch total loss 1.74407876\n",
      "Trained batch 172 batch loss 1.60241187 epoch total loss 1.74325514\n",
      "Trained batch 173 batch loss 1.59733784 epoch total loss 1.74241173\n",
      "Trained batch 174 batch loss 1.46119928 epoch total loss 1.74079561\n",
      "Trained batch 175 batch loss 1.31178653 epoch total loss 1.73834419\n",
      "Trained batch 176 batch loss 1.43299508 epoch total loss 1.73660922\n",
      "Trained batch 177 batch loss 1.55529106 epoch total loss 1.73558486\n",
      "Trained batch 178 batch loss 1.64728856 epoch total loss 1.73508871\n",
      "Trained batch 179 batch loss 1.6710093 epoch total loss 1.73473084\n",
      "Trained batch 180 batch loss 1.65117693 epoch total loss 1.73426664\n",
      "Trained batch 181 batch loss 1.5926553 epoch total loss 1.73348427\n",
      "Trained batch 182 batch loss 1.71045375 epoch total loss 1.73335767\n",
      "Trained batch 183 batch loss 1.62308717 epoch total loss 1.73275506\n",
      "Trained batch 184 batch loss 1.6041472 epoch total loss 1.73205614\n",
      "Trained batch 185 batch loss 1.64368 epoch total loss 1.73157847\n",
      "Trained batch 186 batch loss 1.51996255 epoch total loss 1.73044074\n",
      "Trained batch 187 batch loss 1.5545541 epoch total loss 1.72950017\n",
      "Trained batch 188 batch loss 1.66645718 epoch total loss 1.72916472\n",
      "Trained batch 189 batch loss 1.6525929 epoch total loss 1.72875965\n",
      "Trained batch 190 batch loss 1.66938603 epoch total loss 1.72844708\n",
      "Trained batch 191 batch loss 1.73362303 epoch total loss 1.72847414\n",
      "Trained batch 192 batch loss 1.58480525 epoch total loss 1.72772586\n",
      "Trained batch 193 batch loss 1.4853828 epoch total loss 1.72647011\n",
      "Trained batch 194 batch loss 1.51006019 epoch total loss 1.72535467\n",
      "Trained batch 195 batch loss 1.58710361 epoch total loss 1.72464573\n",
      "Trained batch 196 batch loss 1.59743202 epoch total loss 1.72399664\n",
      "Trained batch 197 batch loss 1.55222905 epoch total loss 1.72312474\n",
      "Trained batch 198 batch loss 1.65514421 epoch total loss 1.72278142\n",
      "Trained batch 199 batch loss 1.61269248 epoch total loss 1.72222829\n",
      "Trained batch 200 batch loss 1.69362473 epoch total loss 1.72208524\n",
      "Trained batch 201 batch loss 1.74490404 epoch total loss 1.72219884\n",
      "Trained batch 202 batch loss 1.62777317 epoch total loss 1.72173131\n",
      "Trained batch 203 batch loss 1.58831298 epoch total loss 1.7210741\n",
      "Trained batch 204 batch loss 1.61013842 epoch total loss 1.72053039\n",
      "Trained batch 205 batch loss 1.53718233 epoch total loss 1.71963584\n",
      "Trained batch 206 batch loss 1.62914383 epoch total loss 1.71919668\n",
      "Trained batch 207 batch loss 1.70343471 epoch total loss 1.7191205\n",
      "Trained batch 208 batch loss 1.58513331 epoch total loss 1.71847641\n",
      "Trained batch 209 batch loss 1.58441293 epoch total loss 1.71783495\n",
      "Trained batch 210 batch loss 1.6906451 epoch total loss 1.71770537\n",
      "Trained batch 211 batch loss 1.69955 epoch total loss 1.71761942\n",
      "Trained batch 212 batch loss 1.52307224 epoch total loss 1.71670175\n",
      "Trained batch 213 batch loss 1.41748 epoch total loss 1.71529698\n",
      "Trained batch 214 batch loss 1.6585753 epoch total loss 1.71503186\n",
      "Trained batch 215 batch loss 1.6732285 epoch total loss 1.71483731\n",
      "Trained batch 216 batch loss 1.67766953 epoch total loss 1.71466529\n",
      "Trained batch 217 batch loss 1.6673919 epoch total loss 1.7144475\n",
      "Trained batch 218 batch loss 1.68015218 epoch total loss 1.71429014\n",
      "Trained batch 219 batch loss 1.66997862 epoch total loss 1.71408772\n",
      "Trained batch 220 batch loss 1.68297374 epoch total loss 1.71394634\n",
      "Trained batch 221 batch loss 1.68161941 epoch total loss 1.71380007\n",
      "Trained batch 222 batch loss 1.63924885 epoch total loss 1.71346426\n",
      "Trained batch 223 batch loss 1.64357495 epoch total loss 1.71315086\n",
      "Trained batch 224 batch loss 1.6277504 epoch total loss 1.71276963\n",
      "Trained batch 225 batch loss 1.59790838 epoch total loss 1.71225917\n",
      "Trained batch 226 batch loss 1.5911845 epoch total loss 1.71172345\n",
      "Trained batch 227 batch loss 1.59930849 epoch total loss 1.71122813\n",
      "Trained batch 228 batch loss 1.53569758 epoch total loss 1.71045828\n",
      "Trained batch 229 batch loss 1.56923771 epoch total loss 1.70984161\n",
      "Trained batch 230 batch loss 1.54776049 epoch total loss 1.70913696\n",
      "Trained batch 231 batch loss 1.58048987 epoch total loss 1.70858\n",
      "Trained batch 232 batch loss 1.59175622 epoch total loss 1.70807648\n",
      "Trained batch 233 batch loss 1.5789839 epoch total loss 1.70752239\n",
      "Trained batch 234 batch loss 1.51557636 epoch total loss 1.70670211\n",
      "Trained batch 235 batch loss 1.52670646 epoch total loss 1.70593607\n",
      "Trained batch 236 batch loss 1.57134962 epoch total loss 1.7053659\n",
      "Trained batch 237 batch loss 1.57151306 epoch total loss 1.70480096\n",
      "Trained batch 238 batch loss 1.60483396 epoch total loss 1.70438099\n",
      "Trained batch 239 batch loss 1.6313293 epoch total loss 1.70407522\n",
      "Trained batch 240 batch loss 1.48655498 epoch total loss 1.70316887\n",
      "Trained batch 241 batch loss 1.58657575 epoch total loss 1.70268512\n",
      "Trained batch 242 batch loss 1.70421481 epoch total loss 1.70269144\n",
      "Trained batch 243 batch loss 1.64592755 epoch total loss 1.7024579\n",
      "Trained batch 244 batch loss 1.54997206 epoch total loss 1.70183289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 245 batch loss 1.57867074 epoch total loss 1.70133018\n",
      "Trained batch 246 batch loss 1.49781311 epoch total loss 1.70050287\n",
      "Trained batch 247 batch loss 1.59310281 epoch total loss 1.700068\n",
      "Trained batch 248 batch loss 1.47807598 epoch total loss 1.69917297\n",
      "Trained batch 249 batch loss 1.52534831 epoch total loss 1.69847488\n",
      "Trained batch 250 batch loss 1.53746247 epoch total loss 1.69783092\n",
      "Trained batch 251 batch loss 1.47447085 epoch total loss 1.69694102\n",
      "Trained batch 252 batch loss 1.44728398 epoch total loss 1.69595039\n",
      "Trained batch 253 batch loss 1.48843086 epoch total loss 1.69513011\n",
      "Trained batch 254 batch loss 1.53604269 epoch total loss 1.69450378\n",
      "Trained batch 255 batch loss 1.52986455 epoch total loss 1.69385815\n",
      "Trained batch 256 batch loss 1.59321165 epoch total loss 1.69346499\n",
      "Trained batch 257 batch loss 1.63832057 epoch total loss 1.69325042\n",
      "Trained batch 258 batch loss 1.62003732 epoch total loss 1.69296658\n",
      "Trained batch 259 batch loss 1.62097 epoch total loss 1.69268858\n",
      "Trained batch 260 batch loss 1.63958859 epoch total loss 1.69248438\n",
      "Trained batch 261 batch loss 1.63066983 epoch total loss 1.69224751\n",
      "Trained batch 262 batch loss 1.53061831 epoch total loss 1.6916306\n",
      "Trained batch 263 batch loss 1.59470439 epoch total loss 1.69126201\n",
      "Trained batch 264 batch loss 1.48100662 epoch total loss 1.69046569\n",
      "Trained batch 265 batch loss 1.49975026 epoch total loss 1.68974602\n",
      "Trained batch 266 batch loss 1.53391 epoch total loss 1.68916011\n",
      "Trained batch 267 batch loss 1.58329594 epoch total loss 1.68876362\n",
      "Trained batch 268 batch loss 1.38588941 epoch total loss 1.68763351\n",
      "Trained batch 269 batch loss 1.42165518 epoch total loss 1.68664467\n",
      "Trained batch 270 batch loss 1.39663279 epoch total loss 1.6855706\n",
      "Trained batch 271 batch loss 1.44537354 epoch total loss 1.68468428\n",
      "Trained batch 272 batch loss 1.40098357 epoch total loss 1.6836412\n",
      "Trained batch 273 batch loss 1.48240387 epoch total loss 1.68290401\n",
      "Trained batch 274 batch loss 1.58065176 epoch total loss 1.68253088\n",
      "Trained batch 275 batch loss 1.52163339 epoch total loss 1.6819458\n",
      "Trained batch 276 batch loss 1.53638613 epoch total loss 1.68141842\n",
      "Trained batch 277 batch loss 1.47176182 epoch total loss 1.68066156\n",
      "Trained batch 278 batch loss 1.52424252 epoch total loss 1.68009877\n",
      "Trained batch 279 batch loss 1.56789517 epoch total loss 1.67969668\n",
      "Trained batch 280 batch loss 1.66057491 epoch total loss 1.67962837\n",
      "Trained batch 281 batch loss 1.65697408 epoch total loss 1.67954779\n",
      "Trained batch 282 batch loss 1.69974852 epoch total loss 1.67961943\n",
      "Trained batch 283 batch loss 1.52684855 epoch total loss 1.67907965\n",
      "Trained batch 284 batch loss 1.55595088 epoch total loss 1.67864609\n",
      "Trained batch 285 batch loss 1.63357711 epoch total loss 1.6784879\n",
      "Trained batch 286 batch loss 1.59845865 epoch total loss 1.67820799\n",
      "Trained batch 287 batch loss 1.4609766 epoch total loss 1.67745113\n",
      "Trained batch 288 batch loss 1.46136355 epoch total loss 1.67670083\n",
      "Trained batch 289 batch loss 1.43244755 epoch total loss 1.67585564\n",
      "Trained batch 290 batch loss 1.44145322 epoch total loss 1.6750474\n",
      "Trained batch 291 batch loss 1.55303955 epoch total loss 1.67462814\n",
      "Trained batch 292 batch loss 1.61045992 epoch total loss 1.67440844\n",
      "Trained batch 293 batch loss 1.64464784 epoch total loss 1.67430687\n",
      "Trained batch 294 batch loss 1.5531441 epoch total loss 1.67389464\n",
      "Trained batch 295 batch loss 1.45533335 epoch total loss 1.67315376\n",
      "Trained batch 296 batch loss 1.54086101 epoch total loss 1.67270684\n",
      "Trained batch 297 batch loss 1.6076597 epoch total loss 1.67248785\n",
      "Trained batch 298 batch loss 1.6511873 epoch total loss 1.67241633\n",
      "Trained batch 299 batch loss 1.536479 epoch total loss 1.67196167\n",
      "Trained batch 300 batch loss 1.57988584 epoch total loss 1.67165482\n",
      "Trained batch 301 batch loss 1.57390535 epoch total loss 1.67133009\n",
      "Trained batch 302 batch loss 1.55155945 epoch total loss 1.67093337\n",
      "Trained batch 303 batch loss 1.62439573 epoch total loss 1.67077982\n",
      "Trained batch 304 batch loss 1.53488529 epoch total loss 1.67033279\n",
      "Trained batch 305 batch loss 1.67253506 epoch total loss 1.67034006\n",
      "Trained batch 306 batch loss 1.6081903 epoch total loss 1.67013693\n",
      "Trained batch 307 batch loss 1.65049958 epoch total loss 1.67007291\n",
      "Trained batch 308 batch loss 1.67029929 epoch total loss 1.67007363\n",
      "Trained batch 309 batch loss 1.68062449 epoch total loss 1.67010772\n",
      "Trained batch 310 batch loss 1.55496526 epoch total loss 1.66973639\n",
      "Trained batch 311 batch loss 1.59887075 epoch total loss 1.66950858\n",
      "Trained batch 312 batch loss 1.54267073 epoch total loss 1.66910195\n",
      "Trained batch 313 batch loss 1.63784504 epoch total loss 1.66900206\n",
      "Trained batch 314 batch loss 1.63459158 epoch total loss 1.66889238\n",
      "Trained batch 315 batch loss 1.58643341 epoch total loss 1.6686306\n",
      "Trained batch 316 batch loss 1.48417008 epoch total loss 1.66804695\n",
      "Trained batch 317 batch loss 1.50425506 epoch total loss 1.6675303\n",
      "Trained batch 318 batch loss 1.61081028 epoch total loss 1.66735208\n",
      "Trained batch 319 batch loss 1.57657325 epoch total loss 1.66706753\n",
      "Trained batch 320 batch loss 1.61491883 epoch total loss 1.66690469\n",
      "Trained batch 321 batch loss 1.55522919 epoch total loss 1.66655672\n",
      "Trained batch 322 batch loss 1.50457239 epoch total loss 1.66605377\n",
      "Trained batch 323 batch loss 1.59680057 epoch total loss 1.66583931\n",
      "Trained batch 324 batch loss 1.50257063 epoch total loss 1.66533542\n",
      "Trained batch 325 batch loss 1.56584036 epoch total loss 1.66502929\n",
      "Trained batch 326 batch loss 1.61657643 epoch total loss 1.66488063\n",
      "Trained batch 327 batch loss 1.63773847 epoch total loss 1.66479766\n",
      "Trained batch 328 batch loss 1.55778229 epoch total loss 1.66447151\n",
      "Trained batch 329 batch loss 1.54569364 epoch total loss 1.66411054\n",
      "Trained batch 330 batch loss 1.65042949 epoch total loss 1.66406918\n",
      "Trained batch 331 batch loss 1.57008648 epoch total loss 1.66378522\n",
      "Trained batch 332 batch loss 1.56348419 epoch total loss 1.66348302\n",
      "Trained batch 333 batch loss 1.56852126 epoch total loss 1.66319788\n",
      "Trained batch 334 batch loss 1.52946579 epoch total loss 1.66279757\n",
      "Trained batch 335 batch loss 1.60567951 epoch total loss 1.66262698\n",
      "Trained batch 336 batch loss 1.55816627 epoch total loss 1.66231608\n",
      "Trained batch 337 batch loss 1.5874455 epoch total loss 1.662094\n",
      "Trained batch 338 batch loss 1.58532095 epoch total loss 1.6618669\n",
      "Trained batch 339 batch loss 1.59905565 epoch total loss 1.66168153\n",
      "Trained batch 340 batch loss 1.60169101 epoch total loss 1.6615051\n",
      "Trained batch 341 batch loss 1.68435931 epoch total loss 1.66157222\n",
      "Trained batch 342 batch loss 1.69241905 epoch total loss 1.66166246\n",
      "Trained batch 343 batch loss 1.60095894 epoch total loss 1.66148555\n",
      "Trained batch 344 batch loss 1.62913465 epoch total loss 1.6613915\n",
      "Trained batch 345 batch loss 1.61659074 epoch total loss 1.66126156\n",
      "Trained batch 346 batch loss 1.60047245 epoch total loss 1.66108584\n",
      "Trained batch 347 batch loss 1.65601861 epoch total loss 1.6610713\n",
      "Trained batch 348 batch loss 1.70342755 epoch total loss 1.66119301\n",
      "Trained batch 349 batch loss 1.50488579 epoch total loss 1.66074514\n",
      "Trained batch 350 batch loss 1.37950873 epoch total loss 1.65994155\n",
      "Trained batch 351 batch loss 1.37862635 epoch total loss 1.65914\n",
      "Trained batch 352 batch loss 1.55656719 epoch total loss 1.65884864\n",
      "Trained batch 353 batch loss 1.52711284 epoch total loss 1.6584754\n",
      "Trained batch 354 batch loss 1.60562038 epoch total loss 1.65832603\n",
      "Trained batch 355 batch loss 1.5893749 epoch total loss 1.65813172\n",
      "Trained batch 356 batch loss 1.57472205 epoch total loss 1.65789747\n",
      "Trained batch 357 batch loss 1.61049139 epoch total loss 1.65776455\n",
      "Trained batch 358 batch loss 1.53820908 epoch total loss 1.65743065\n",
      "Trained batch 359 batch loss 1.50052762 epoch total loss 1.65699363\n",
      "Trained batch 360 batch loss 1.50641763 epoch total loss 1.65657532\n",
      "Trained batch 361 batch loss 1.52165735 epoch total loss 1.6562016\n",
      "Trained batch 362 batch loss 1.55190516 epoch total loss 1.65591347\n",
      "Trained batch 363 batch loss 1.53367484 epoch total loss 1.65557671\n",
      "Trained batch 364 batch loss 1.61705983 epoch total loss 1.65547097\n",
      "Trained batch 365 batch loss 1.60346127 epoch total loss 1.65532851\n",
      "Trained batch 366 batch loss 1.58384907 epoch total loss 1.65513325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 367 batch loss 1.57864833 epoch total loss 1.65492487\n",
      "Trained batch 368 batch loss 1.63946688 epoch total loss 1.65488279\n",
      "Trained batch 369 batch loss 1.60139894 epoch total loss 1.65473783\n",
      "Trained batch 370 batch loss 1.57688737 epoch total loss 1.65452754\n",
      "Trained batch 371 batch loss 1.59118605 epoch total loss 1.65435672\n",
      "Trained batch 372 batch loss 1.55043161 epoch total loss 1.65407729\n",
      "Trained batch 373 batch loss 1.59206176 epoch total loss 1.65391099\n",
      "Trained batch 374 batch loss 1.60476816 epoch total loss 1.65377975\n",
      "Trained batch 375 batch loss 1.57472682 epoch total loss 1.65356886\n",
      "Trained batch 376 batch loss 1.56433761 epoch total loss 1.65333152\n",
      "Trained batch 377 batch loss 1.55760372 epoch total loss 1.6530776\n",
      "Trained batch 378 batch loss 1.54600632 epoch total loss 1.65279436\n",
      "Trained batch 379 batch loss 1.56342065 epoch total loss 1.65255857\n",
      "Trained batch 380 batch loss 1.56863785 epoch total loss 1.65233779\n",
      "Trained batch 381 batch loss 1.48803973 epoch total loss 1.65190661\n",
      "Trained batch 382 batch loss 1.60617495 epoch total loss 1.65178692\n",
      "Trained batch 383 batch loss 1.64392471 epoch total loss 1.65176642\n",
      "Trained batch 384 batch loss 1.62652898 epoch total loss 1.65170062\n",
      "Trained batch 385 batch loss 1.67884803 epoch total loss 1.65177119\n",
      "Trained batch 386 batch loss 1.81738293 epoch total loss 1.65220022\n",
      "Trained batch 387 batch loss 1.69144142 epoch total loss 1.65230167\n",
      "Trained batch 388 batch loss 1.71412313 epoch total loss 1.65246093\n",
      "Trained batch 389 batch loss 1.66493416 epoch total loss 1.652493\n",
      "Trained batch 390 batch loss 1.60075402 epoch total loss 1.65236032\n",
      "Trained batch 391 batch loss 1.67505968 epoch total loss 1.65241838\n",
      "Trained batch 392 batch loss 1.69026208 epoch total loss 1.65251482\n",
      "Trained batch 393 batch loss 1.62337804 epoch total loss 1.65244067\n",
      "Trained batch 394 batch loss 1.64249277 epoch total loss 1.65241551\n",
      "Trained batch 395 batch loss 1.64144754 epoch total loss 1.65238762\n",
      "Trained batch 396 batch loss 1.58181739 epoch total loss 1.6522094\n",
      "Trained batch 397 batch loss 1.58219385 epoch total loss 1.65203309\n",
      "Trained batch 398 batch loss 1.56233859 epoch total loss 1.65180767\n",
      "Trained batch 399 batch loss 1.54824662 epoch total loss 1.65154803\n",
      "Trained batch 400 batch loss 1.54776239 epoch total loss 1.65128863\n",
      "Trained batch 401 batch loss 1.58563554 epoch total loss 1.65112484\n",
      "Trained batch 402 batch loss 1.63685369 epoch total loss 1.65108931\n",
      "Trained batch 403 batch loss 1.68016994 epoch total loss 1.65116155\n",
      "Trained batch 404 batch loss 1.62865281 epoch total loss 1.65110588\n",
      "Trained batch 405 batch loss 1.69491386 epoch total loss 1.65121388\n",
      "Trained batch 406 batch loss 1.6185019 epoch total loss 1.65113342\n",
      "Trained batch 407 batch loss 1.55313015 epoch total loss 1.6508925\n",
      "Trained batch 408 batch loss 1.60590911 epoch total loss 1.65078223\n",
      "Trained batch 409 batch loss 1.47138917 epoch total loss 1.65034366\n",
      "Trained batch 410 batch loss 1.54872918 epoch total loss 1.6500957\n",
      "Trained batch 411 batch loss 1.38443637 epoch total loss 1.64944935\n",
      "Trained batch 412 batch loss 1.60218728 epoch total loss 1.64933467\n",
      "Trained batch 413 batch loss 1.60917342 epoch total loss 1.64923751\n",
      "Trained batch 414 batch loss 1.55079699 epoch total loss 1.64899969\n",
      "Trained batch 415 batch loss 1.55525017 epoch total loss 1.64877367\n",
      "Trained batch 416 batch loss 1.47089577 epoch total loss 1.64834607\n",
      "Trained batch 417 batch loss 1.59861314 epoch total loss 1.64822686\n",
      "Trained batch 418 batch loss 1.64228833 epoch total loss 1.64821267\n",
      "Trained batch 419 batch loss 1.69922388 epoch total loss 1.64833438\n",
      "Trained batch 420 batch loss 1.69301414 epoch total loss 1.64844072\n",
      "Trained batch 421 batch loss 1.67329717 epoch total loss 1.64849973\n",
      "Trained batch 422 batch loss 1.56051862 epoch total loss 1.64829123\n",
      "Trained batch 423 batch loss 1.62943876 epoch total loss 1.64824677\n",
      "Trained batch 424 batch loss 1.52210057 epoch total loss 1.64794922\n",
      "Trained batch 425 batch loss 1.62867749 epoch total loss 1.6479038\n",
      "Trained batch 426 batch loss 1.5995537 epoch total loss 1.64779031\n",
      "Trained batch 427 batch loss 1.53066897 epoch total loss 1.64751601\n",
      "Trained batch 428 batch loss 1.59956419 epoch total loss 1.64740384\n",
      "Trained batch 429 batch loss 1.71168268 epoch total loss 1.64755368\n",
      "Trained batch 430 batch loss 1.65375113 epoch total loss 1.64756811\n",
      "Trained batch 431 batch loss 1.52084708 epoch total loss 1.64727414\n",
      "Trained batch 432 batch loss 1.56711376 epoch total loss 1.64708865\n",
      "Trained batch 433 batch loss 1.57955503 epoch total loss 1.6469326\n",
      "Trained batch 434 batch loss 1.60489392 epoch total loss 1.6468358\n",
      "Trained batch 435 batch loss 1.69883013 epoch total loss 1.64695537\n",
      "Trained batch 436 batch loss 1.66676056 epoch total loss 1.64700079\n",
      "Trained batch 437 batch loss 1.53461051 epoch total loss 1.64674366\n",
      "Trained batch 438 batch loss 1.52254224 epoch total loss 1.64645994\n",
      "Trained batch 439 batch loss 1.56631351 epoch total loss 1.64627731\n",
      "Trained batch 440 batch loss 1.43954706 epoch total loss 1.64580762\n",
      "Trained batch 441 batch loss 1.52085972 epoch total loss 1.64552426\n",
      "Trained batch 442 batch loss 1.52404535 epoch total loss 1.64524949\n",
      "Trained batch 443 batch loss 1.57972693 epoch total loss 1.64510155\n",
      "Trained batch 444 batch loss 1.66729593 epoch total loss 1.6451515\n",
      "Trained batch 445 batch loss 1.59319901 epoch total loss 1.64503479\n",
      "Trained batch 446 batch loss 1.58781743 epoch total loss 1.64490652\n",
      "Trained batch 447 batch loss 1.59809065 epoch total loss 1.64480174\n",
      "Trained batch 448 batch loss 1.55065215 epoch total loss 1.64459157\n",
      "Trained batch 449 batch loss 1.58532834 epoch total loss 1.64445961\n",
      "Trained batch 450 batch loss 1.57361889 epoch total loss 1.64430213\n",
      "Trained batch 451 batch loss 1.54692268 epoch total loss 1.64408624\n",
      "Trained batch 452 batch loss 1.4898963 epoch total loss 1.64374506\n",
      "Trained batch 453 batch loss 1.48609269 epoch total loss 1.64339709\n",
      "Trained batch 454 batch loss 1.49865353 epoch total loss 1.64307821\n",
      "Trained batch 455 batch loss 1.4820528 epoch total loss 1.64272439\n",
      "Trained batch 456 batch loss 1.55111885 epoch total loss 1.64252353\n",
      "Trained batch 457 batch loss 1.55395472 epoch total loss 1.64232969\n",
      "Trained batch 458 batch loss 1.54475129 epoch total loss 1.64211667\n",
      "Trained batch 459 batch loss 1.48123145 epoch total loss 1.64176607\n",
      "Trained batch 460 batch loss 1.59358108 epoch total loss 1.64166129\n",
      "Trained batch 461 batch loss 1.55880511 epoch total loss 1.64148152\n",
      "Trained batch 462 batch loss 1.53144658 epoch total loss 1.64124322\n",
      "Trained batch 463 batch loss 1.56076336 epoch total loss 1.64106953\n",
      "Trained batch 464 batch loss 1.4943248 epoch total loss 1.64075327\n",
      "Trained batch 465 batch loss 1.56307483 epoch total loss 1.64058614\n",
      "Trained batch 466 batch loss 1.59289515 epoch total loss 1.64048386\n",
      "Trained batch 467 batch loss 1.56260228 epoch total loss 1.64031708\n",
      "Trained batch 468 batch loss 1.55070567 epoch total loss 1.64012563\n",
      "Trained batch 469 batch loss 1.6556648 epoch total loss 1.64015865\n",
      "Trained batch 470 batch loss 1.61112607 epoch total loss 1.64009702\n",
      "Trained batch 471 batch loss 1.5980401 epoch total loss 1.64000762\n",
      "Trained batch 472 batch loss 1.55395818 epoch total loss 1.63982534\n",
      "Trained batch 473 batch loss 1.55849838 epoch total loss 1.63965333\n",
      "Trained batch 474 batch loss 1.56086755 epoch total loss 1.63948715\n",
      "Trained batch 475 batch loss 1.54583454 epoch total loss 1.63929\n",
      "Trained batch 476 batch loss 1.44483256 epoch total loss 1.63888144\n",
      "Trained batch 477 batch loss 1.49481094 epoch total loss 1.63857937\n",
      "Trained batch 478 batch loss 1.57415962 epoch total loss 1.63844454\n",
      "Trained batch 479 batch loss 1.56166291 epoch total loss 1.63828421\n",
      "Trained batch 480 batch loss 1.61371017 epoch total loss 1.63823307\n",
      "Trained batch 481 batch loss 1.61161041 epoch total loss 1.63817775\n",
      "Trained batch 482 batch loss 1.55490053 epoch total loss 1.6380049\n",
      "Trained batch 483 batch loss 1.57865345 epoch total loss 1.63788211\n",
      "Trained batch 484 batch loss 1.57663774 epoch total loss 1.63775563\n",
      "Trained batch 485 batch loss 1.59587288 epoch total loss 1.63766921\n",
      "Trained batch 486 batch loss 1.63052595 epoch total loss 1.63765466\n",
      "Trained batch 487 batch loss 1.58439541 epoch total loss 1.63754535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 488 batch loss 1.54762709 epoch total loss 1.63736105\n",
      "Trained batch 489 batch loss 1.55412316 epoch total loss 1.63719082\n",
      "Trained batch 490 batch loss 1.59906888 epoch total loss 1.63711298\n",
      "Trained batch 491 batch loss 1.49268746 epoch total loss 1.63681877\n",
      "Trained batch 492 batch loss 1.54766738 epoch total loss 1.63663757\n",
      "Trained batch 493 batch loss 1.53076434 epoch total loss 1.63642287\n",
      "Trained batch 494 batch loss 1.58570528 epoch total loss 1.63632011\n",
      "Trained batch 495 batch loss 1.63217759 epoch total loss 1.63631189\n",
      "Trained batch 496 batch loss 1.60075867 epoch total loss 1.63624024\n",
      "Trained batch 497 batch loss 1.56746209 epoch total loss 1.63610172\n",
      "Trained batch 498 batch loss 1.54062116 epoch total loss 1.63591015\n",
      "Trained batch 499 batch loss 1.62925839 epoch total loss 1.6358968\n",
      "Trained batch 500 batch loss 1.73223805 epoch total loss 1.63608944\n",
      "Trained batch 501 batch loss 1.71331096 epoch total loss 1.63624358\n",
      "Trained batch 502 batch loss 1.66233587 epoch total loss 1.63629568\n",
      "Trained batch 503 batch loss 1.55766201 epoch total loss 1.63613939\n",
      "Trained batch 504 batch loss 1.50339437 epoch total loss 1.63587606\n",
      "Trained batch 505 batch loss 1.53008032 epoch total loss 1.63566649\n",
      "Trained batch 506 batch loss 1.50757158 epoch total loss 1.63541341\n",
      "Trained batch 507 batch loss 1.53205585 epoch total loss 1.63520944\n",
      "Trained batch 508 batch loss 1.58518243 epoch total loss 1.63511109\n",
      "Trained batch 509 batch loss 1.60233867 epoch total loss 1.63504672\n",
      "Trained batch 510 batch loss 1.55305 epoch total loss 1.63488591\n",
      "Trained batch 511 batch loss 1.58479106 epoch total loss 1.6347878\n",
      "Trained batch 512 batch loss 1.60115671 epoch total loss 1.63472211\n",
      "Trained batch 513 batch loss 1.56887603 epoch total loss 1.63459373\n",
      "Trained batch 514 batch loss 1.57309723 epoch total loss 1.63447416\n",
      "Trained batch 515 batch loss 1.56339586 epoch total loss 1.63433611\n",
      "Trained batch 516 batch loss 1.56552875 epoch total loss 1.63420284\n",
      "Trained batch 517 batch loss 1.52462649 epoch total loss 1.63399076\n",
      "Trained batch 518 batch loss 1.50971198 epoch total loss 1.63375092\n",
      "Trained batch 519 batch loss 1.5278132 epoch total loss 1.63354683\n",
      "Trained batch 520 batch loss 1.62402964 epoch total loss 1.63352847\n",
      "Trained batch 521 batch loss 1.53143346 epoch total loss 1.63333249\n",
      "Trained batch 522 batch loss 1.57691479 epoch total loss 1.63322449\n",
      "Trained batch 523 batch loss 1.51007032 epoch total loss 1.63298893\n",
      "Trained batch 524 batch loss 1.51577616 epoch total loss 1.63276517\n",
      "Trained batch 525 batch loss 1.43916988 epoch total loss 1.63239646\n",
      "Trained batch 526 batch loss 1.51462889 epoch total loss 1.63217258\n",
      "Trained batch 527 batch loss 1.44427526 epoch total loss 1.63181603\n",
      "Trained batch 528 batch loss 1.44745684 epoch total loss 1.63146687\n",
      "Trained batch 529 batch loss 1.51669168 epoch total loss 1.63124979\n",
      "Trained batch 530 batch loss 1.38866937 epoch total loss 1.63079214\n",
      "Trained batch 531 batch loss 1.40740156 epoch total loss 1.63037145\n",
      "Trained batch 532 batch loss 1.33614039 epoch total loss 1.62981832\n",
      "Trained batch 533 batch loss 1.37009788 epoch total loss 1.62933111\n",
      "Trained batch 534 batch loss 1.44174671 epoch total loss 1.6289798\n",
      "Trained batch 535 batch loss 1.52805829 epoch total loss 1.62879121\n",
      "Trained batch 536 batch loss 1.54404283 epoch total loss 1.62863314\n",
      "Trained batch 537 batch loss 1.51852322 epoch total loss 1.6284281\n",
      "Trained batch 538 batch loss 1.59074938 epoch total loss 1.62835813\n",
      "Trained batch 539 batch loss 1.59818673 epoch total loss 1.6283021\n",
      "Trained batch 540 batch loss 1.56873369 epoch total loss 1.62819183\n",
      "Trained batch 541 batch loss 1.58368683 epoch total loss 1.62810957\n",
      "Trained batch 542 batch loss 1.57895923 epoch total loss 1.62801886\n",
      "Trained batch 543 batch loss 1.50808978 epoch total loss 1.62779808\n",
      "Trained batch 544 batch loss 1.57867408 epoch total loss 1.62770772\n",
      "Trained batch 545 batch loss 1.54593408 epoch total loss 1.62755775\n",
      "Trained batch 546 batch loss 1.5617938 epoch total loss 1.62743723\n",
      "Trained batch 547 batch loss 1.55035424 epoch total loss 1.62729633\n",
      "Trained batch 548 batch loss 1.55556536 epoch total loss 1.62716544\n",
      "Trained batch 549 batch loss 1.6203264 epoch total loss 1.62715292\n",
      "Trained batch 550 batch loss 1.54601705 epoch total loss 1.62700534\n",
      "Trained batch 551 batch loss 1.47456312 epoch total loss 1.62672865\n",
      "Trained batch 552 batch loss 1.59809256 epoch total loss 1.6266768\n",
      "Trained batch 553 batch loss 1.64879966 epoch total loss 1.62671685\n",
      "Trained batch 554 batch loss 1.57671976 epoch total loss 1.62662661\n",
      "Trained batch 555 batch loss 1.50351834 epoch total loss 1.62640476\n",
      "Trained batch 556 batch loss 1.55003369 epoch total loss 1.62626743\n",
      "Trained batch 557 batch loss 1.58094823 epoch total loss 1.62618613\n",
      "Trained batch 558 batch loss 1.63410532 epoch total loss 1.6262002\n",
      "Trained batch 559 batch loss 1.52863503 epoch total loss 1.62602568\n",
      "Trained batch 560 batch loss 1.67353928 epoch total loss 1.62611055\n",
      "Trained batch 561 batch loss 1.42172861 epoch total loss 1.62574625\n",
      "Trained batch 562 batch loss 1.60939455 epoch total loss 1.62571716\n",
      "Trained batch 563 batch loss 1.58358335 epoch total loss 1.62564218\n",
      "Trained batch 564 batch loss 1.68467391 epoch total loss 1.62574697\n",
      "Trained batch 565 batch loss 1.61887765 epoch total loss 1.62573481\n",
      "Trained batch 566 batch loss 1.6046586 epoch total loss 1.62569761\n",
      "Trained batch 567 batch loss 1.4811126 epoch total loss 1.62544262\n",
      "Trained batch 568 batch loss 1.62523353 epoch total loss 1.62544227\n",
      "Trained batch 569 batch loss 1.58336353 epoch total loss 1.62536836\n",
      "Trained batch 570 batch loss 1.53619385 epoch total loss 1.62521195\n",
      "Trained batch 571 batch loss 1.57307243 epoch total loss 1.62512052\n",
      "Trained batch 572 batch loss 1.55538023 epoch total loss 1.62499857\n",
      "Trained batch 573 batch loss 1.56677425 epoch total loss 1.624897\n",
      "Trained batch 574 batch loss 1.57682049 epoch total loss 1.62481332\n",
      "Trained batch 575 batch loss 1.63819098 epoch total loss 1.62483656\n",
      "Trained batch 576 batch loss 1.65148914 epoch total loss 1.62488282\n",
      "Trained batch 577 batch loss 1.63192439 epoch total loss 1.62489498\n",
      "Trained batch 578 batch loss 1.61756122 epoch total loss 1.62488222\n",
      "Trained batch 579 batch loss 1.6602366 epoch total loss 1.62494326\n",
      "Trained batch 580 batch loss 1.60821486 epoch total loss 1.62491441\n",
      "Trained batch 581 batch loss 1.5896281 epoch total loss 1.62485361\n",
      "Trained batch 582 batch loss 1.54328513 epoch total loss 1.62471354\n",
      "Trained batch 583 batch loss 1.6055938 epoch total loss 1.62468064\n",
      "Trained batch 584 batch loss 1.54538751 epoch total loss 1.62454498\n",
      "Trained batch 585 batch loss 1.53978527 epoch total loss 1.62440014\n",
      "Trained batch 586 batch loss 1.57520902 epoch total loss 1.6243161\n",
      "Trained batch 587 batch loss 1.41803694 epoch total loss 1.62396467\n",
      "Trained batch 588 batch loss 1.59407473 epoch total loss 1.62391376\n",
      "Trained batch 589 batch loss 1.50759983 epoch total loss 1.62371635\n",
      "Trained batch 590 batch loss 1.60259032 epoch total loss 1.62368059\n",
      "Trained batch 591 batch loss 1.59703135 epoch total loss 1.62363553\n",
      "Trained batch 592 batch loss 1.61399329 epoch total loss 1.62361932\n",
      "Trained batch 593 batch loss 1.4465487 epoch total loss 1.6233207\n",
      "Trained batch 594 batch loss 1.52551496 epoch total loss 1.62315595\n",
      "Trained batch 595 batch loss 1.41682482 epoch total loss 1.62280917\n",
      "Trained batch 596 batch loss 1.57369876 epoch total loss 1.6227268\n",
      "Trained batch 597 batch loss 1.55779088 epoch total loss 1.62261796\n",
      "Trained batch 598 batch loss 1.54487455 epoch total loss 1.62248802\n",
      "Trained batch 599 batch loss 1.61777949 epoch total loss 1.62248015\n",
      "Trained batch 600 batch loss 1.6327945 epoch total loss 1.62249732\n",
      "Trained batch 601 batch loss 1.63872826 epoch total loss 1.62252438\n",
      "Trained batch 602 batch loss 1.65079808 epoch total loss 1.62257135\n",
      "Trained batch 603 batch loss 1.55284476 epoch total loss 1.62245572\n",
      "Trained batch 604 batch loss 1.63226795 epoch total loss 1.62247205\n",
      "Trained batch 605 batch loss 1.57355046 epoch total loss 1.6223911\n",
      "Trained batch 606 batch loss 1.57586837 epoch total loss 1.62231433\n",
      "Trained batch 607 batch loss 1.47938931 epoch total loss 1.6220789\n",
      "Trained batch 608 batch loss 1.57045841 epoch total loss 1.6219939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 609 batch loss 1.50325584 epoch total loss 1.62179887\n",
      "Trained batch 610 batch loss 1.62623453 epoch total loss 1.62180614\n",
      "Trained batch 611 batch loss 1.57244563 epoch total loss 1.62172532\n",
      "Trained batch 612 batch loss 1.54980421 epoch total loss 1.6216079\n",
      "Trained batch 613 batch loss 1.53162074 epoch total loss 1.62146103\n",
      "Trained batch 614 batch loss 1.47958183 epoch total loss 1.62122989\n",
      "Trained batch 615 batch loss 1.52055955 epoch total loss 1.62106621\n",
      "Trained batch 616 batch loss 1.54580605 epoch total loss 1.62094402\n",
      "Trained batch 617 batch loss 1.58974206 epoch total loss 1.62089348\n",
      "Trained batch 618 batch loss 1.51488185 epoch total loss 1.62072194\n",
      "Trained batch 619 batch loss 1.4605751 epoch total loss 1.62046313\n",
      "Trained batch 620 batch loss 1.48904705 epoch total loss 1.6202513\n",
      "Trained batch 621 batch loss 1.52727199 epoch total loss 1.62010157\n",
      "Trained batch 622 batch loss 1.58668327 epoch total loss 1.62004781\n",
      "Trained batch 623 batch loss 1.53496432 epoch total loss 1.61991131\n",
      "Trained batch 624 batch loss 1.58565915 epoch total loss 1.61985636\n",
      "Trained batch 625 batch loss 1.65470481 epoch total loss 1.61991215\n",
      "Trained batch 626 batch loss 1.62590337 epoch total loss 1.61992168\n",
      "Trained batch 627 batch loss 1.57424986 epoch total loss 1.61984885\n",
      "Trained batch 628 batch loss 1.56487942 epoch total loss 1.61976135\n",
      "Trained batch 629 batch loss 1.51626015 epoch total loss 1.61959684\n",
      "Trained batch 630 batch loss 1.46146131 epoch total loss 1.61934578\n",
      "Trained batch 631 batch loss 1.48708129 epoch total loss 1.61913621\n",
      "Trained batch 632 batch loss 1.47576523 epoch total loss 1.61890936\n",
      "Trained batch 633 batch loss 1.4187336 epoch total loss 1.6185931\n",
      "Trained batch 634 batch loss 1.56558132 epoch total loss 1.61850953\n",
      "Trained batch 635 batch loss 1.40726209 epoch total loss 1.6181767\n",
      "Trained batch 636 batch loss 1.45219421 epoch total loss 1.61791575\n",
      "Trained batch 637 batch loss 1.44551301 epoch total loss 1.61764514\n",
      "Trained batch 638 batch loss 1.47805238 epoch total loss 1.61742628\n",
      "Trained batch 639 batch loss 1.60706484 epoch total loss 1.61741006\n",
      "Trained batch 640 batch loss 1.50120437 epoch total loss 1.61722851\n",
      "Trained batch 641 batch loss 1.35224652 epoch total loss 1.61681521\n",
      "Trained batch 642 batch loss 1.48135066 epoch total loss 1.61660409\n",
      "Trained batch 643 batch loss 1.55506921 epoch total loss 1.61650848\n",
      "Trained batch 644 batch loss 1.53938687 epoch total loss 1.61638868\n",
      "Trained batch 645 batch loss 1.55111432 epoch total loss 1.61628759\n",
      "Trained batch 646 batch loss 1.52456617 epoch total loss 1.61614561\n",
      "Trained batch 647 batch loss 1.44193149 epoch total loss 1.6158762\n",
      "Trained batch 648 batch loss 1.41590071 epoch total loss 1.61556756\n",
      "Trained batch 649 batch loss 1.50228739 epoch total loss 1.61539316\n",
      "Trained batch 650 batch loss 1.44920766 epoch total loss 1.61513746\n",
      "Trained batch 651 batch loss 1.39083564 epoch total loss 1.61479294\n",
      "Trained batch 652 batch loss 1.35401845 epoch total loss 1.614393\n",
      "Trained batch 653 batch loss 1.64873433 epoch total loss 1.61444545\n",
      "Trained batch 654 batch loss 1.44179404 epoch total loss 1.61418152\n",
      "Trained batch 655 batch loss 1.54309618 epoch total loss 1.61407292\n",
      "Trained batch 656 batch loss 1.46349669 epoch total loss 1.61384344\n",
      "Trained batch 657 batch loss 1.54370701 epoch total loss 1.61373663\n",
      "Trained batch 658 batch loss 1.50581992 epoch total loss 1.61357272\n",
      "Trained batch 659 batch loss 1.58128774 epoch total loss 1.61352372\n",
      "Trained batch 660 batch loss 1.63696158 epoch total loss 1.61355925\n",
      "Trained batch 661 batch loss 1.53531146 epoch total loss 1.61344075\n",
      "Trained batch 662 batch loss 1.54861748 epoch total loss 1.61334288\n",
      "Trained batch 663 batch loss 1.55581057 epoch total loss 1.61325598\n",
      "Trained batch 664 batch loss 1.40716517 epoch total loss 1.61294556\n",
      "Trained batch 665 batch loss 1.49482441 epoch total loss 1.61276805\n",
      "Trained batch 666 batch loss 1.60243368 epoch total loss 1.61275244\n",
      "Trained batch 667 batch loss 1.63431311 epoch total loss 1.61278474\n",
      "Trained batch 668 batch loss 1.56354022 epoch total loss 1.61271107\n",
      "Trained batch 669 batch loss 1.5415808 epoch total loss 1.61260486\n",
      "Trained batch 670 batch loss 1.41686249 epoch total loss 1.61231267\n",
      "Trained batch 671 batch loss 1.40712261 epoch total loss 1.6120069\n",
      "Trained batch 672 batch loss 1.54886961 epoch total loss 1.61191285\n",
      "Trained batch 673 batch loss 1.51779032 epoch total loss 1.61177301\n",
      "Trained batch 674 batch loss 1.5500139 epoch total loss 1.61168146\n",
      "Trained batch 675 batch loss 1.50376332 epoch total loss 1.6115216\n",
      "Trained batch 676 batch loss 1.60521591 epoch total loss 1.6115123\n",
      "Trained batch 677 batch loss 1.49669766 epoch total loss 1.61134279\n",
      "Trained batch 678 batch loss 1.44457078 epoch total loss 1.61109674\n",
      "Trained batch 679 batch loss 1.45841336 epoch total loss 1.61087179\n",
      "Trained batch 680 batch loss 1.43522787 epoch total loss 1.61061347\n",
      "Trained batch 681 batch loss 1.54450703 epoch total loss 1.61051643\n",
      "Trained batch 682 batch loss 1.50921297 epoch total loss 1.61036789\n",
      "Trained batch 683 batch loss 1.5116514 epoch total loss 1.61022329\n",
      "Trained batch 684 batch loss 1.55874813 epoch total loss 1.61014795\n",
      "Trained batch 685 batch loss 1.43709826 epoch total loss 1.60989535\n",
      "Trained batch 686 batch loss 1.52220893 epoch total loss 1.60976756\n",
      "Trained batch 687 batch loss 1.53495598 epoch total loss 1.6096586\n",
      "Trained batch 688 batch loss 1.53454065 epoch total loss 1.6095494\n",
      "Trained batch 689 batch loss 1.59207129 epoch total loss 1.60952401\n",
      "Trained batch 690 batch loss 1.55961859 epoch total loss 1.60945165\n",
      "Trained batch 691 batch loss 1.52261806 epoch total loss 1.60932589\n",
      "Trained batch 692 batch loss 1.44046104 epoch total loss 1.60908186\n",
      "Trained batch 693 batch loss 1.53873372 epoch total loss 1.6089803\n",
      "Trained batch 694 batch loss 1.52607131 epoch total loss 1.60886085\n",
      "Trained batch 695 batch loss 1.51946354 epoch total loss 1.6087321\n",
      "Trained batch 696 batch loss 1.50626075 epoch total loss 1.60858488\n",
      "Trained batch 697 batch loss 1.58078742 epoch total loss 1.60854506\n",
      "Trained batch 698 batch loss 1.60626388 epoch total loss 1.60854185\n",
      "Trained batch 699 batch loss 1.50858307 epoch total loss 1.6083988\n",
      "Trained batch 700 batch loss 1.56072795 epoch total loss 1.60833061\n",
      "Trained batch 701 batch loss 1.54068637 epoch total loss 1.60823405\n",
      "Trained batch 702 batch loss 1.54807472 epoch total loss 1.60814834\n",
      "Trained batch 703 batch loss 1.57572317 epoch total loss 1.6081022\n",
      "Trained batch 704 batch loss 1.56417131 epoch total loss 1.60803986\n",
      "Trained batch 705 batch loss 1.55097711 epoch total loss 1.60795903\n",
      "Trained batch 706 batch loss 1.51029587 epoch total loss 1.60782063\n",
      "Trained batch 707 batch loss 1.54348326 epoch total loss 1.60772955\n",
      "Trained batch 708 batch loss 1.55724669 epoch total loss 1.60765827\n",
      "Trained batch 709 batch loss 1.52067912 epoch total loss 1.60753548\n",
      "Trained batch 710 batch loss 1.49080133 epoch total loss 1.60737121\n",
      "Trained batch 711 batch loss 1.513695 epoch total loss 1.60723937\n",
      "Trained batch 712 batch loss 1.60539317 epoch total loss 1.60723674\n",
      "Trained batch 713 batch loss 1.52951503 epoch total loss 1.60712779\n",
      "Trained batch 714 batch loss 1.39899778 epoch total loss 1.60683632\n",
      "Trained batch 715 batch loss 1.37424111 epoch total loss 1.606511\n",
      "Trained batch 716 batch loss 1.44511676 epoch total loss 1.60628557\n",
      "Trained batch 717 batch loss 1.52049577 epoch total loss 1.60616589\n",
      "Trained batch 718 batch loss 1.48699677 epoch total loss 1.60599983\n",
      "Trained batch 719 batch loss 1.42529595 epoch total loss 1.60574853\n",
      "Trained batch 720 batch loss 1.55555749 epoch total loss 1.6056788\n",
      "Trained batch 721 batch loss 1.55859232 epoch total loss 1.60561347\n",
      "Trained batch 722 batch loss 1.55852115 epoch total loss 1.60554826\n",
      "Trained batch 723 batch loss 1.41946626 epoch total loss 1.60529077\n",
      "Trained batch 724 batch loss 1.39663243 epoch total loss 1.60500252\n",
      "Trained batch 725 batch loss 1.46839356 epoch total loss 1.60481417\n",
      "Trained batch 726 batch loss 1.46675456 epoch total loss 1.60462403\n",
      "Trained batch 727 batch loss 1.50242257 epoch total loss 1.60448349\n",
      "Trained batch 728 batch loss 1.55552697 epoch total loss 1.60441625\n",
      "Trained batch 729 batch loss 1.60591733 epoch total loss 1.6044184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 730 batch loss 1.62277174 epoch total loss 1.60444355\n",
      "Trained batch 731 batch loss 1.67483532 epoch total loss 1.60453975\n",
      "Trained batch 732 batch loss 1.61577535 epoch total loss 1.60455501\n",
      "Trained batch 733 batch loss 1.53985822 epoch total loss 1.60446692\n",
      "Trained batch 734 batch loss 1.36180437 epoch total loss 1.60413623\n",
      "Trained batch 735 batch loss 1.28953886 epoch total loss 1.60370827\n",
      "Trained batch 736 batch loss 1.28839731 epoch total loss 1.60328\n",
      "Trained batch 737 batch loss 1.31489158 epoch total loss 1.6028887\n",
      "Trained batch 738 batch loss 1.38022804 epoch total loss 1.60258698\n",
      "Trained batch 739 batch loss 1.49996912 epoch total loss 1.60244823\n",
      "Trained batch 740 batch loss 1.49794579 epoch total loss 1.60230696\n",
      "Trained batch 741 batch loss 1.51773798 epoch total loss 1.60219276\n",
      "Trained batch 742 batch loss 1.49753761 epoch total loss 1.60205173\n",
      "Trained batch 743 batch loss 1.5129199 epoch total loss 1.60193181\n",
      "Trained batch 744 batch loss 1.52150953 epoch total loss 1.60182369\n",
      "Trained batch 745 batch loss 1.57610869 epoch total loss 1.60178912\n",
      "Trained batch 746 batch loss 1.53751981 epoch total loss 1.60170293\n",
      "Trained batch 747 batch loss 1.52026272 epoch total loss 1.60159385\n",
      "Trained batch 748 batch loss 1.5145936 epoch total loss 1.60147762\n",
      "Trained batch 749 batch loss 1.40893281 epoch total loss 1.60122061\n",
      "Trained batch 750 batch loss 1.3476162 epoch total loss 1.60088253\n",
      "Trained batch 751 batch loss 1.34037471 epoch total loss 1.60053551\n",
      "Trained batch 752 batch loss 1.44936574 epoch total loss 1.60033453\n",
      "Trained batch 753 batch loss 1.50524426 epoch total loss 1.60020816\n",
      "Trained batch 754 batch loss 1.45340455 epoch total loss 1.60001349\n",
      "Trained batch 755 batch loss 1.38525319 epoch total loss 1.59972906\n",
      "Trained batch 756 batch loss 1.40391302 epoch total loss 1.59947\n",
      "Trained batch 757 batch loss 1.51204252 epoch total loss 1.59935462\n",
      "Trained batch 758 batch loss 1.4347235 epoch total loss 1.59913731\n",
      "Trained batch 759 batch loss 1.35324883 epoch total loss 1.59881341\n",
      "Trained batch 760 batch loss 1.51580131 epoch total loss 1.5987041\n",
      "Trained batch 761 batch loss 1.46404862 epoch total loss 1.59852707\n",
      "Trained batch 762 batch loss 1.56444597 epoch total loss 1.59848237\n",
      "Trained batch 763 batch loss 1.43176889 epoch total loss 1.59826386\n",
      "Trained batch 764 batch loss 1.49547982 epoch total loss 1.59812939\n",
      "Trained batch 765 batch loss 1.44499278 epoch total loss 1.59792912\n",
      "Trained batch 766 batch loss 1.46454668 epoch total loss 1.59775507\n",
      "Trained batch 767 batch loss 1.42907286 epoch total loss 1.59753513\n",
      "Trained batch 768 batch loss 1.46998477 epoch total loss 1.59736907\n",
      "Trained batch 769 batch loss 1.40615726 epoch total loss 1.5971204\n",
      "Trained batch 770 batch loss 1.44297409 epoch total loss 1.59692013\n",
      "Trained batch 771 batch loss 1.65266895 epoch total loss 1.59699249\n",
      "Trained batch 772 batch loss 1.51660764 epoch total loss 1.59688842\n",
      "Trained batch 773 batch loss 1.49406552 epoch total loss 1.59675539\n",
      "Trained batch 774 batch loss 1.45758474 epoch total loss 1.59657562\n",
      "Trained batch 775 batch loss 1.42194843 epoch total loss 1.59635031\n",
      "Trained batch 776 batch loss 1.43690908 epoch total loss 1.5961448\n",
      "Trained batch 777 batch loss 1.46429038 epoch total loss 1.59597504\n",
      "Trained batch 778 batch loss 1.35951817 epoch total loss 1.59567106\n",
      "Trained batch 779 batch loss 1.58090425 epoch total loss 1.59565222\n",
      "Trained batch 780 batch loss 1.5237689 epoch total loss 1.59556007\n",
      "Trained batch 781 batch loss 1.47948623 epoch total loss 1.59541142\n",
      "Trained batch 782 batch loss 1.45681894 epoch total loss 1.59523416\n",
      "Trained batch 783 batch loss 1.4788667 epoch total loss 1.59508562\n",
      "Trained batch 784 batch loss 1.42396581 epoch total loss 1.59486735\n",
      "Trained batch 785 batch loss 1.54523218 epoch total loss 1.59480417\n",
      "Trained batch 786 batch loss 1.57888722 epoch total loss 1.5947839\n",
      "Trained batch 787 batch loss 1.52533615 epoch total loss 1.59469569\n",
      "Trained batch 788 batch loss 1.52914608 epoch total loss 1.5946126\n",
      "Trained batch 789 batch loss 1.51177359 epoch total loss 1.59450746\n",
      "Trained batch 790 batch loss 1.52849507 epoch total loss 1.59442389\n",
      "Trained batch 791 batch loss 1.61536324 epoch total loss 1.59445035\n",
      "Trained batch 792 batch loss 1.55246544 epoch total loss 1.59439731\n",
      "Trained batch 793 batch loss 1.49672031 epoch total loss 1.59427416\n",
      "Trained batch 794 batch loss 1.49453235 epoch total loss 1.59414852\n",
      "Trained batch 795 batch loss 1.47825408 epoch total loss 1.59400272\n",
      "Trained batch 796 batch loss 1.36134136 epoch total loss 1.59371042\n",
      "Trained batch 797 batch loss 1.49393535 epoch total loss 1.59358525\n",
      "Trained batch 798 batch loss 1.48630369 epoch total loss 1.59345078\n",
      "Trained batch 799 batch loss 1.52519178 epoch total loss 1.59336531\n",
      "Trained batch 800 batch loss 1.50512707 epoch total loss 1.59325504\n",
      "Trained batch 801 batch loss 1.45790386 epoch total loss 1.593086\n",
      "Trained batch 802 batch loss 1.44522214 epoch total loss 1.59290159\n",
      "Trained batch 803 batch loss 1.40285611 epoch total loss 1.59266484\n",
      "Trained batch 804 batch loss 1.52507985 epoch total loss 1.5925808\n",
      "Trained batch 805 batch loss 1.53419542 epoch total loss 1.5925082\n",
      "Trained batch 806 batch loss 1.60249496 epoch total loss 1.59252071\n",
      "Trained batch 807 batch loss 1.84360087 epoch total loss 1.59283185\n",
      "Trained batch 808 batch loss 1.63227868 epoch total loss 1.59288073\n",
      "Trained batch 809 batch loss 1.5864625 epoch total loss 1.59287274\n",
      "Trained batch 810 batch loss 1.5787065 epoch total loss 1.59285522\n",
      "Trained batch 811 batch loss 1.47786808 epoch total loss 1.59271348\n",
      "Trained batch 812 batch loss 1.44453859 epoch total loss 1.59253109\n",
      "Trained batch 813 batch loss 1.54884 epoch total loss 1.59247732\n",
      "Trained batch 814 batch loss 1.55660605 epoch total loss 1.59243333\n",
      "Trained batch 815 batch loss 1.62593877 epoch total loss 1.59247446\n",
      "Trained batch 816 batch loss 1.43427217 epoch total loss 1.59228063\n",
      "Trained batch 817 batch loss 1.4153266 epoch total loss 1.59206402\n",
      "Trained batch 818 batch loss 1.39696348 epoch total loss 1.59182549\n",
      "Trained batch 819 batch loss 1.52745092 epoch total loss 1.59174693\n",
      "Trained batch 820 batch loss 1.40456474 epoch total loss 1.59151864\n",
      "Trained batch 821 batch loss 1.56450868 epoch total loss 1.59148562\n",
      "Trained batch 822 batch loss 1.43169403 epoch total loss 1.59129119\n",
      "Trained batch 823 batch loss 1.48826158 epoch total loss 1.59116602\n",
      "Trained batch 824 batch loss 1.5468967 epoch total loss 1.59111226\n",
      "Trained batch 825 batch loss 1.54424214 epoch total loss 1.59105539\n",
      "Trained batch 826 batch loss 1.50260782 epoch total loss 1.59094834\n",
      "Trained batch 827 batch loss 1.63711476 epoch total loss 1.59100413\n",
      "Trained batch 828 batch loss 1.53908813 epoch total loss 1.59094131\n",
      "Trained batch 829 batch loss 1.41187644 epoch total loss 1.5907253\n",
      "Trained batch 830 batch loss 1.48122621 epoch total loss 1.59059334\n",
      "Trained batch 831 batch loss 1.52693725 epoch total loss 1.59051681\n",
      "Trained batch 832 batch loss 1.4750036 epoch total loss 1.59037793\n",
      "Trained batch 833 batch loss 1.49010491 epoch total loss 1.59025764\n",
      "Trained batch 834 batch loss 1.39092565 epoch total loss 1.59001851\n",
      "Trained batch 835 batch loss 1.39312625 epoch total loss 1.5897826\n",
      "Trained batch 836 batch loss 1.53372324 epoch total loss 1.5897156\n",
      "Trained batch 837 batch loss 1.43975067 epoch total loss 1.58953631\n",
      "Trained batch 838 batch loss 1.45106578 epoch total loss 1.58937109\n",
      "Trained batch 839 batch loss 1.47562909 epoch total loss 1.58923542\n",
      "Trained batch 840 batch loss 1.41461992 epoch total loss 1.58902764\n",
      "Trained batch 841 batch loss 1.51521659 epoch total loss 1.58893991\n",
      "Trained batch 842 batch loss 1.59830904 epoch total loss 1.58895099\n",
      "Trained batch 843 batch loss 1.70149 epoch total loss 1.58908451\n",
      "Trained batch 844 batch loss 1.57576978 epoch total loss 1.58906877\n",
      "Trained batch 845 batch loss 1.46501136 epoch total loss 1.5889219\n",
      "Trained batch 846 batch loss 1.45262015 epoch total loss 1.58876085\n",
      "Trained batch 847 batch loss 1.2931484 epoch total loss 1.58841181\n",
      "Trained batch 848 batch loss 1.38124561 epoch total loss 1.58816743\n",
      "Trained batch 849 batch loss 1.46876526 epoch total loss 1.58802676\n",
      "Trained batch 850 batch loss 1.54287577 epoch total loss 1.58797359\n",
      "Trained batch 851 batch loss 1.42385292 epoch total loss 1.58778071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 852 batch loss 1.61717129 epoch total loss 1.58781528\n",
      "Trained batch 853 batch loss 1.5634253 epoch total loss 1.58778667\n",
      "Trained batch 854 batch loss 1.52757525 epoch total loss 1.58771622\n",
      "Trained batch 855 batch loss 1.59313881 epoch total loss 1.58772254\n",
      "Trained batch 856 batch loss 1.56265521 epoch total loss 1.58769321\n",
      "Trained batch 857 batch loss 1.47532129 epoch total loss 1.5875622\n",
      "Trained batch 858 batch loss 1.46170545 epoch total loss 1.58741546\n",
      "Trained batch 859 batch loss 1.53228426 epoch total loss 1.5873512\n",
      "Trained batch 860 batch loss 1.58901727 epoch total loss 1.58735311\n",
      "Trained batch 861 batch loss 1.51455879 epoch total loss 1.58726847\n",
      "Trained batch 862 batch loss 1.40023065 epoch total loss 1.58705151\n",
      "Trained batch 863 batch loss 1.48335934 epoch total loss 1.58693147\n",
      "Trained batch 864 batch loss 1.47117257 epoch total loss 1.58679748\n",
      "Trained batch 865 batch loss 1.4266305 epoch total loss 1.58661234\n",
      "Trained batch 866 batch loss 1.4564817 epoch total loss 1.58646202\n",
      "Trained batch 867 batch loss 1.51502812 epoch total loss 1.58637965\n",
      "Trained batch 868 batch loss 1.49615812 epoch total loss 1.5862757\n",
      "Trained batch 869 batch loss 1.55601549 epoch total loss 1.58624089\n",
      "Trained batch 870 batch loss 1.62990642 epoch total loss 1.58629107\n",
      "Trained batch 871 batch loss 1.60469604 epoch total loss 1.58631229\n",
      "Trained batch 872 batch loss 1.645895 epoch total loss 1.5863806\n",
      "Trained batch 873 batch loss 1.70763302 epoch total loss 1.58651948\n",
      "Trained batch 874 batch loss 1.53866827 epoch total loss 1.58646476\n",
      "Trained batch 875 batch loss 1.44847822 epoch total loss 1.58630705\n",
      "Trained batch 876 batch loss 1.43815398 epoch total loss 1.58613789\n",
      "Trained batch 877 batch loss 1.27251697 epoch total loss 1.58578026\n",
      "Trained batch 878 batch loss 1.32872343 epoch total loss 1.58548748\n",
      "Trained batch 879 batch loss 1.51111698 epoch total loss 1.58540285\n",
      "Trained batch 880 batch loss 1.24874187 epoch total loss 1.5850203\n",
      "Trained batch 881 batch loss 1.24278498 epoch total loss 1.58463192\n",
      "Trained batch 882 batch loss 1.27231836 epoch total loss 1.58427775\n",
      "Trained batch 883 batch loss 1.29553246 epoch total loss 1.58395076\n",
      "Trained batch 884 batch loss 1.40831435 epoch total loss 1.58375216\n",
      "Trained batch 885 batch loss 1.52831578 epoch total loss 1.58368945\n",
      "Trained batch 886 batch loss 1.52116036 epoch total loss 1.58361888\n",
      "Trained batch 887 batch loss 1.61381876 epoch total loss 1.58365285\n",
      "Trained batch 888 batch loss 1.63653636 epoch total loss 1.58371246\n",
      "Trained batch 889 batch loss 1.49593282 epoch total loss 1.58361375\n",
      "Trained batch 890 batch loss 1.50743246 epoch total loss 1.58352816\n",
      "Trained batch 891 batch loss 1.46342087 epoch total loss 1.58339334\n",
      "Trained batch 892 batch loss 1.54018033 epoch total loss 1.58334482\n",
      "Trained batch 893 batch loss 1.52537167 epoch total loss 1.58328\n",
      "Trained batch 894 batch loss 1.57250679 epoch total loss 1.58326793\n",
      "Trained batch 895 batch loss 1.62648 epoch total loss 1.58331621\n",
      "Trained batch 896 batch loss 1.51124144 epoch total loss 1.58323574\n",
      "Trained batch 897 batch loss 1.50010645 epoch total loss 1.58314312\n",
      "Trained batch 898 batch loss 1.48288023 epoch total loss 1.58303142\n",
      "Trained batch 899 batch loss 1.49337363 epoch total loss 1.58293176\n",
      "Trained batch 900 batch loss 1.54778564 epoch total loss 1.58289266\n",
      "Trained batch 901 batch loss 1.61057651 epoch total loss 1.58292341\n",
      "Trained batch 902 batch loss 1.62122846 epoch total loss 1.58296585\n",
      "Trained batch 903 batch loss 1.59336066 epoch total loss 1.58297741\n",
      "Trained batch 904 batch loss 1.66329718 epoch total loss 1.58306623\n",
      "Trained batch 905 batch loss 1.62818885 epoch total loss 1.58311617\n",
      "Trained batch 906 batch loss 1.64217 epoch total loss 1.58318138\n",
      "Trained batch 907 batch loss 1.6370039 epoch total loss 1.58324063\n",
      "Trained batch 908 batch loss 1.65618491 epoch total loss 1.58332098\n",
      "Trained batch 909 batch loss 1.55065262 epoch total loss 1.58328497\n",
      "Trained batch 910 batch loss 1.48312974 epoch total loss 1.58317494\n",
      "Trained batch 911 batch loss 1.47240734 epoch total loss 1.58305335\n",
      "Trained batch 912 batch loss 1.48491192 epoch total loss 1.5829457\n",
      "Trained batch 913 batch loss 1.44460928 epoch total loss 1.58279419\n",
      "Trained batch 914 batch loss 1.41195607 epoch total loss 1.58260727\n",
      "Trained batch 915 batch loss 1.36367905 epoch total loss 1.58236802\n",
      "Trained batch 916 batch loss 1.3897624 epoch total loss 1.58215773\n",
      "Trained batch 917 batch loss 1.44610751 epoch total loss 1.58200943\n",
      "Trained batch 918 batch loss 1.42831099 epoch total loss 1.58184206\n",
      "Trained batch 919 batch loss 1.51889 epoch total loss 1.58177352\n",
      "Trained batch 920 batch loss 1.48542452 epoch total loss 1.58166885\n",
      "Trained batch 921 batch loss 1.51257992 epoch total loss 1.58159387\n",
      "Trained batch 922 batch loss 1.49653459 epoch total loss 1.5815016\n",
      "Trained batch 923 batch loss 1.56793845 epoch total loss 1.58148706\n",
      "Trained batch 924 batch loss 1.48334408 epoch total loss 1.58138084\n",
      "Trained batch 925 batch loss 1.48263335 epoch total loss 1.58127415\n",
      "Trained batch 926 batch loss 1.47368717 epoch total loss 1.58115792\n",
      "Trained batch 927 batch loss 1.46335638 epoch total loss 1.58103085\n",
      "Trained batch 928 batch loss 1.40373349 epoch total loss 1.58083975\n",
      "Trained batch 929 batch loss 1.43174684 epoch total loss 1.5806793\n",
      "Trained batch 930 batch loss 1.47110534 epoch total loss 1.5805614\n",
      "Trained batch 931 batch loss 1.40780687 epoch total loss 1.58037591\n",
      "Trained batch 932 batch loss 1.4006145 epoch total loss 1.58018303\n",
      "Trained batch 933 batch loss 1.45227528 epoch total loss 1.58004594\n",
      "Trained batch 934 batch loss 1.54526043 epoch total loss 1.58000875\n",
      "Trained batch 935 batch loss 1.40393 epoch total loss 1.57982039\n",
      "Trained batch 936 batch loss 1.41796947 epoch total loss 1.57964754\n",
      "Trained batch 937 batch loss 1.41761494 epoch total loss 1.57947457\n",
      "Trained batch 938 batch loss 1.46282959 epoch total loss 1.57935011\n",
      "Trained batch 939 batch loss 1.48444855 epoch total loss 1.57924914\n",
      "Trained batch 940 batch loss 1.46593964 epoch total loss 1.57912862\n",
      "Trained batch 941 batch loss 1.34286594 epoch total loss 1.57887757\n",
      "Trained batch 942 batch loss 1.47185767 epoch total loss 1.57876384\n",
      "Trained batch 943 batch loss 1.47735691 epoch total loss 1.57865644\n",
      "Trained batch 944 batch loss 1.47441912 epoch total loss 1.57854593\n",
      "Trained batch 945 batch loss 1.51022053 epoch total loss 1.57847369\n",
      "Trained batch 946 batch loss 1.36217856 epoch total loss 1.57824504\n",
      "Trained batch 947 batch loss 1.46942508 epoch total loss 1.57813013\n",
      "Trained batch 948 batch loss 1.33886504 epoch total loss 1.57787776\n",
      "Trained batch 949 batch loss 1.47962987 epoch total loss 1.57777417\n",
      "Trained batch 950 batch loss 1.56929922 epoch total loss 1.57776535\n",
      "Trained batch 951 batch loss 1.56965148 epoch total loss 1.57775688\n",
      "Trained batch 952 batch loss 1.52866721 epoch total loss 1.57770526\n",
      "Trained batch 953 batch loss 1.48145747 epoch total loss 1.57760429\n",
      "Trained batch 954 batch loss 1.36955523 epoch total loss 1.57738614\n",
      "Trained batch 955 batch loss 1.47592831 epoch total loss 1.57727993\n",
      "Trained batch 956 batch loss 1.45544171 epoch total loss 1.57715249\n",
      "Trained batch 957 batch loss 1.49576151 epoch total loss 1.57706749\n",
      "Trained batch 958 batch loss 1.39223623 epoch total loss 1.57687449\n",
      "Trained batch 959 batch loss 1.41239 epoch total loss 1.57670295\n",
      "Trained batch 960 batch loss 1.33439887 epoch total loss 1.57645047\n",
      "Trained batch 961 batch loss 1.48759174 epoch total loss 1.57635796\n",
      "Trained batch 962 batch loss 1.49046648 epoch total loss 1.57626867\n",
      "Trained batch 963 batch loss 1.48519301 epoch total loss 1.57617414\n",
      "Trained batch 964 batch loss 1.51006293 epoch total loss 1.57610548\n",
      "Trained batch 965 batch loss 1.52156878 epoch total loss 1.57604909\n",
      "Trained batch 966 batch loss 1.44516706 epoch total loss 1.57591355\n",
      "Trained batch 967 batch loss 1.48357427 epoch total loss 1.57581806\n",
      "Trained batch 968 batch loss 1.45666599 epoch total loss 1.57569492\n",
      "Trained batch 969 batch loss 1.39604974 epoch total loss 1.57550955\n",
      "Trained batch 970 batch loss 1.37003767 epoch total loss 1.57529759\n",
      "Trained batch 971 batch loss 1.49975491 epoch total loss 1.57521987\n",
      "Trained batch 972 batch loss 1.50270641 epoch total loss 1.57514524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 973 batch loss 1.48116267 epoch total loss 1.57504869\n",
      "Trained batch 974 batch loss 1.41630602 epoch total loss 1.57488561\n",
      "Trained batch 975 batch loss 1.42174387 epoch total loss 1.57472861\n",
      "Trained batch 976 batch loss 1.36723375 epoch total loss 1.57451594\n",
      "Trained batch 977 batch loss 1.43262506 epoch total loss 1.57437062\n",
      "Trained batch 978 batch loss 1.50829673 epoch total loss 1.57430315\n",
      "Trained batch 979 batch loss 1.54714179 epoch total loss 1.57427537\n",
      "Trained batch 980 batch loss 1.48655713 epoch total loss 1.57418585\n",
      "Trained batch 981 batch loss 1.55176687 epoch total loss 1.57416296\n",
      "Trained batch 982 batch loss 1.4672178 epoch total loss 1.574054\n",
      "Trained batch 983 batch loss 1.46071768 epoch total loss 1.57393873\n",
      "Trained batch 984 batch loss 1.46115541 epoch total loss 1.57382417\n",
      "Trained batch 985 batch loss 1.5131942 epoch total loss 1.57376254\n",
      "Trained batch 986 batch loss 1.53245485 epoch total loss 1.57372069\n",
      "Trained batch 987 batch loss 1.30793428 epoch total loss 1.5734514\n",
      "Trained batch 988 batch loss 1.55235553 epoch total loss 1.57343006\n",
      "Trained batch 989 batch loss 1.43124962 epoch total loss 1.57328641\n",
      "Trained batch 990 batch loss 1.54553163 epoch total loss 1.57325828\n",
      "Trained batch 991 batch loss 1.40992188 epoch total loss 1.57309353\n",
      "Trained batch 992 batch loss 1.4896698 epoch total loss 1.57300937\n",
      "Trained batch 993 batch loss 1.48335874 epoch total loss 1.57291913\n",
      "Trained batch 994 batch loss 1.45490384 epoch total loss 1.5728004\n",
      "Trained batch 995 batch loss 1.60812879 epoch total loss 1.57283592\n",
      "Trained batch 996 batch loss 1.5406065 epoch total loss 1.57280362\n",
      "Trained batch 997 batch loss 1.44782734 epoch total loss 1.57267833\n",
      "Trained batch 998 batch loss 1.42406178 epoch total loss 1.57252944\n",
      "Trained batch 999 batch loss 1.41122377 epoch total loss 1.57236803\n",
      "Trained batch 1000 batch loss 1.45498574 epoch total loss 1.5722506\n",
      "Trained batch 1001 batch loss 1.40142906 epoch total loss 1.57208\n",
      "Trained batch 1002 batch loss 1.45753109 epoch total loss 1.57196569\n",
      "Trained batch 1003 batch loss 1.47333872 epoch total loss 1.57186735\n",
      "Trained batch 1004 batch loss 1.51913869 epoch total loss 1.57181489\n",
      "Trained batch 1005 batch loss 1.4746294 epoch total loss 1.57171822\n",
      "Trained batch 1006 batch loss 1.58562744 epoch total loss 1.57173193\n",
      "Trained batch 1007 batch loss 1.5099256 epoch total loss 1.57167053\n",
      "Trained batch 1008 batch loss 1.53907907 epoch total loss 1.57163823\n",
      "Trained batch 1009 batch loss 1.50497127 epoch total loss 1.57157218\n",
      "Trained batch 1010 batch loss 1.3843255 epoch total loss 1.57138669\n",
      "Trained batch 1011 batch loss 1.33865428 epoch total loss 1.5711565\n",
      "Trained batch 1012 batch loss 1.39275515 epoch total loss 1.57098019\n",
      "Trained batch 1013 batch loss 1.46067667 epoch total loss 1.57087123\n",
      "Trained batch 1014 batch loss 1.52500272 epoch total loss 1.57082605\n",
      "Trained batch 1015 batch loss 1.48936641 epoch total loss 1.57074583\n",
      "Trained batch 1016 batch loss 1.48510456 epoch total loss 1.57066154\n",
      "Trained batch 1017 batch loss 1.40957427 epoch total loss 1.57050312\n",
      "Trained batch 1018 batch loss 1.33578634 epoch total loss 1.57027256\n",
      "Trained batch 1019 batch loss 1.47732902 epoch total loss 1.57018137\n",
      "Trained batch 1020 batch loss 1.4819603 epoch total loss 1.57009482\n",
      "Trained batch 1021 batch loss 1.46404493 epoch total loss 1.56999087\n",
      "Trained batch 1022 batch loss 1.50098693 epoch total loss 1.5699234\n",
      "Trained batch 1023 batch loss 1.3653636 epoch total loss 1.56972337\n",
      "Trained batch 1024 batch loss 1.43105221 epoch total loss 1.56958795\n",
      "Trained batch 1025 batch loss 1.45613456 epoch total loss 1.56947732\n",
      "Trained batch 1026 batch loss 1.47043717 epoch total loss 1.56938076\n",
      "Trained batch 1027 batch loss 1.28060603 epoch total loss 1.56909966\n",
      "Trained batch 1028 batch loss 1.25900185 epoch total loss 1.56879807\n",
      "Trained batch 1029 batch loss 1.20657396 epoch total loss 1.56844592\n",
      "Trained batch 1030 batch loss 1.26196599 epoch total loss 1.56814837\n",
      "Trained batch 1031 batch loss 1.58457243 epoch total loss 1.56816435\n",
      "Trained batch 1032 batch loss 1.49017406 epoch total loss 1.56808889\n",
      "Trained batch 1033 batch loss 1.5303036 epoch total loss 1.56805229\n",
      "Trained batch 1034 batch loss 1.43066108 epoch total loss 1.56791937\n",
      "Trained batch 1035 batch loss 1.50342524 epoch total loss 1.56785703\n",
      "Trained batch 1036 batch loss 1.37044919 epoch total loss 1.56766653\n",
      "Trained batch 1037 batch loss 1.34842455 epoch total loss 1.56745505\n",
      "Trained batch 1038 batch loss 1.43743145 epoch total loss 1.56732976\n",
      "Trained batch 1039 batch loss 1.48316407 epoch total loss 1.5672487\n",
      "Trained batch 1040 batch loss 1.3836503 epoch total loss 1.56707227\n",
      "Trained batch 1041 batch loss 1.44560659 epoch total loss 1.56695557\n",
      "Trained batch 1042 batch loss 1.53128457 epoch total loss 1.56692123\n",
      "Trained batch 1043 batch loss 1.51328135 epoch total loss 1.56686985\n",
      "Trained batch 1044 batch loss 1.5210309 epoch total loss 1.56682587\n",
      "Trained batch 1045 batch loss 1.49689054 epoch total loss 1.56675899\n",
      "Trained batch 1046 batch loss 1.60436618 epoch total loss 1.56679499\n",
      "Trained batch 1047 batch loss 1.5215826 epoch total loss 1.56675184\n",
      "Trained batch 1048 batch loss 1.4907155 epoch total loss 1.56667924\n",
      "Trained batch 1049 batch loss 1.44306958 epoch total loss 1.56656146\n",
      "Trained batch 1050 batch loss 1.33468795 epoch total loss 1.56634068\n",
      "Trained batch 1051 batch loss 1.32475734 epoch total loss 1.56611073\n",
      "Trained batch 1052 batch loss 1.33827734 epoch total loss 1.56589413\n",
      "Trained batch 1053 batch loss 1.39788342 epoch total loss 1.56573462\n",
      "Trained batch 1054 batch loss 1.27235281 epoch total loss 1.56545615\n",
      "Trained batch 1055 batch loss 1.229918 epoch total loss 1.5651381\n",
      "Trained batch 1056 batch loss 1.21933484 epoch total loss 1.56481063\n",
      "Trained batch 1057 batch loss 1.19500685 epoch total loss 1.56446075\n",
      "Trained batch 1058 batch loss 1.39907086 epoch total loss 1.56430435\n",
      "Trained batch 1059 batch loss 1.50530589 epoch total loss 1.56424868\n",
      "Trained batch 1060 batch loss 1.48779047 epoch total loss 1.56417656\n",
      "Trained batch 1061 batch loss 1.46207058 epoch total loss 1.56408024\n",
      "Trained batch 1062 batch loss 1.45000458 epoch total loss 1.56397283\n",
      "Trained batch 1063 batch loss 1.31919372 epoch total loss 1.56374252\n",
      "Trained batch 1064 batch loss 1.41747308 epoch total loss 1.56360507\n",
      "Trained batch 1065 batch loss 1.65032411 epoch total loss 1.56368649\n",
      "Trained batch 1066 batch loss 1.60392916 epoch total loss 1.56372416\n",
      "Trained batch 1067 batch loss 1.58574784 epoch total loss 1.56374478\n",
      "Trained batch 1068 batch loss 1.59627593 epoch total loss 1.56377518\n",
      "Trained batch 1069 batch loss 1.56782091 epoch total loss 1.563779\n",
      "Trained batch 1070 batch loss 1.49938583 epoch total loss 1.56371891\n",
      "Trained batch 1071 batch loss 1.3920728 epoch total loss 1.56355858\n",
      "Trained batch 1072 batch loss 1.44831014 epoch total loss 1.56345117\n",
      "Trained batch 1073 batch loss 1.54895258 epoch total loss 1.5634377\n",
      "Trained batch 1074 batch loss 1.4914031 epoch total loss 1.56337059\n",
      "Trained batch 1075 batch loss 1.51216602 epoch total loss 1.56332302\n",
      "Trained batch 1076 batch loss 1.48585176 epoch total loss 1.56325102\n",
      "Trained batch 1077 batch loss 1.48778653 epoch total loss 1.56318092\n",
      "Trained batch 1078 batch loss 1.57410455 epoch total loss 1.56319106\n",
      "Trained batch 1079 batch loss 1.60646939 epoch total loss 1.56323123\n",
      "Trained batch 1080 batch loss 1.62040031 epoch total loss 1.56328404\n",
      "Trained batch 1081 batch loss 1.55320656 epoch total loss 1.56327474\n",
      "Trained batch 1082 batch loss 1.63398862 epoch total loss 1.56334019\n",
      "Trained batch 1083 batch loss 1.55136681 epoch total loss 1.5633291\n",
      "Trained batch 1084 batch loss 1.54360485 epoch total loss 1.56331086\n",
      "Trained batch 1085 batch loss 1.49510717 epoch total loss 1.56324804\n",
      "Trained batch 1086 batch loss 1.52777815 epoch total loss 1.56321549\n",
      "Trained batch 1087 batch loss 1.51291335 epoch total loss 1.56316924\n",
      "Trained batch 1088 batch loss 1.47264576 epoch total loss 1.56308603\n",
      "Trained batch 1089 batch loss 1.44286501 epoch total loss 1.56297565\n",
      "Trained batch 1090 batch loss 1.40012205 epoch total loss 1.56282628\n",
      "Trained batch 1091 batch loss 1.54010487 epoch total loss 1.56280541\n",
      "Trained batch 1092 batch loss 1.53143215 epoch total loss 1.56277668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1093 batch loss 1.56468904 epoch total loss 1.56277847\n",
      "Trained batch 1094 batch loss 1.70669818 epoch total loss 1.56291\n",
      "Trained batch 1095 batch loss 1.50701308 epoch total loss 1.56285882\n",
      "Trained batch 1096 batch loss 1.58472145 epoch total loss 1.56287885\n",
      "Trained batch 1097 batch loss 1.50713444 epoch total loss 1.56282794\n",
      "Trained batch 1098 batch loss 1.40889406 epoch total loss 1.56268775\n",
      "Trained batch 1099 batch loss 1.38647151 epoch total loss 1.56252742\n",
      "Trained batch 1100 batch loss 1.46411169 epoch total loss 1.56243801\n",
      "Trained batch 1101 batch loss 1.47928131 epoch total loss 1.56236243\n",
      "Trained batch 1102 batch loss 1.48293781 epoch total loss 1.56229031\n",
      "Trained batch 1103 batch loss 1.54103398 epoch total loss 1.562271\n",
      "Trained batch 1104 batch loss 1.49000311 epoch total loss 1.56220555\n",
      "Trained batch 1105 batch loss 1.49658287 epoch total loss 1.56214619\n",
      "Trained batch 1106 batch loss 1.48359609 epoch total loss 1.56207514\n",
      "Trained batch 1107 batch loss 1.4005928 epoch total loss 1.56192935\n",
      "Trained batch 1108 batch loss 1.38344 epoch total loss 1.56176829\n",
      "Trained batch 1109 batch loss 1.40150619 epoch total loss 1.56162369\n",
      "Trained batch 1110 batch loss 1.3853457 epoch total loss 1.56146491\n",
      "Trained batch 1111 batch loss 1.48440719 epoch total loss 1.56139553\n",
      "Trained batch 1112 batch loss 1.51366746 epoch total loss 1.56135261\n",
      "Trained batch 1113 batch loss 1.49495602 epoch total loss 1.56129301\n",
      "Trained batch 1114 batch loss 1.39912796 epoch total loss 1.56114745\n",
      "Trained batch 1115 batch loss 1.33055937 epoch total loss 1.56094062\n",
      "Trained batch 1116 batch loss 1.48505306 epoch total loss 1.56087267\n",
      "Trained batch 1117 batch loss 1.31747293 epoch total loss 1.56065488\n",
      "Trained batch 1118 batch loss 1.38473368 epoch total loss 1.56049752\n",
      "Trained batch 1119 batch loss 1.52682471 epoch total loss 1.56046748\n",
      "Trained batch 1120 batch loss 1.49715805 epoch total loss 1.56041098\n",
      "Trained batch 1121 batch loss 1.44828093 epoch total loss 1.56031096\n",
      "Trained batch 1122 batch loss 1.47814965 epoch total loss 1.56023765\n",
      "Trained batch 1123 batch loss 1.47253013 epoch total loss 1.56015956\n",
      "Trained batch 1124 batch loss 1.47965026 epoch total loss 1.56008792\n",
      "Trained batch 1125 batch loss 1.4086597 epoch total loss 1.55995333\n",
      "Trained batch 1126 batch loss 1.43937302 epoch total loss 1.55984616\n",
      "Trained batch 1127 batch loss 1.59187019 epoch total loss 1.55987465\n",
      "Trained batch 1128 batch loss 1.46199012 epoch total loss 1.55978799\n",
      "Trained batch 1129 batch loss 1.52701879 epoch total loss 1.5597589\n",
      "Trained batch 1130 batch loss 1.46022546 epoch total loss 1.55967081\n",
      "Trained batch 1131 batch loss 1.46755564 epoch total loss 1.55958927\n",
      "Trained batch 1132 batch loss 1.49017286 epoch total loss 1.55952787\n",
      "Trained batch 1133 batch loss 1.31892288 epoch total loss 1.55931556\n",
      "Trained batch 1134 batch loss 1.27684593 epoch total loss 1.55906653\n",
      "Trained batch 1135 batch loss 1.41900194 epoch total loss 1.55894303\n",
      "Trained batch 1136 batch loss 1.57093525 epoch total loss 1.55895364\n",
      "Trained batch 1137 batch loss 1.49055982 epoch total loss 1.55889356\n",
      "Trained batch 1138 batch loss 1.56320286 epoch total loss 1.55889738\n",
      "Trained batch 1139 batch loss 1.5983 epoch total loss 1.55893183\n",
      "Trained batch 1140 batch loss 1.55165696 epoch total loss 1.55892551\n",
      "Trained batch 1141 batch loss 1.50269485 epoch total loss 1.55887616\n",
      "Trained batch 1142 batch loss 1.50431371 epoch total loss 1.55882835\n",
      "Trained batch 1143 batch loss 1.4775424 epoch total loss 1.55875731\n",
      "Trained batch 1144 batch loss 1.43321681 epoch total loss 1.55864751\n",
      "Trained batch 1145 batch loss 1.46079803 epoch total loss 1.55856204\n",
      "Trained batch 1146 batch loss 1.60100245 epoch total loss 1.55859911\n",
      "Trained batch 1147 batch loss 1.49464369 epoch total loss 1.55854332\n",
      "Trained batch 1148 batch loss 1.43563509 epoch total loss 1.55843627\n",
      "Trained batch 1149 batch loss 1.54206967 epoch total loss 1.55842209\n",
      "Trained batch 1150 batch loss 1.39175773 epoch total loss 1.55827713\n",
      "Trained batch 1151 batch loss 1.43197417 epoch total loss 1.55816746\n",
      "Trained batch 1152 batch loss 1.51383197 epoch total loss 1.55812883\n",
      "Trained batch 1153 batch loss 1.39855301 epoch total loss 1.55799055\n",
      "Trained batch 1154 batch loss 1.4021157 epoch total loss 1.55785537\n",
      "Trained batch 1155 batch loss 1.4001168 epoch total loss 1.55771887\n",
      "Trained batch 1156 batch loss 1.36713362 epoch total loss 1.55755401\n",
      "Trained batch 1157 batch loss 1.38987803 epoch total loss 1.55740917\n",
      "Trained batch 1158 batch loss 1.52514029 epoch total loss 1.55738127\n",
      "Trained batch 1159 batch loss 1.28980517 epoch total loss 1.55715036\n",
      "Trained batch 1160 batch loss 1.21906543 epoch total loss 1.55685902\n",
      "Trained batch 1161 batch loss 1.38170457 epoch total loss 1.5567081\n",
      "Trained batch 1162 batch loss 1.31854463 epoch total loss 1.55650318\n",
      "Trained batch 1163 batch loss 1.33104897 epoch total loss 1.55630934\n",
      "Trained batch 1164 batch loss 1.35093594 epoch total loss 1.55613291\n",
      "Trained batch 1165 batch loss 1.47807956 epoch total loss 1.55606592\n",
      "Trained batch 1166 batch loss 1.50514531 epoch total loss 1.55602217\n",
      "Trained batch 1167 batch loss 1.46654296 epoch total loss 1.55594552\n",
      "Trained batch 1168 batch loss 1.4668026 epoch total loss 1.55586922\n",
      "Trained batch 1169 batch loss 1.6445204 epoch total loss 1.55594504\n",
      "Trained batch 1170 batch loss 1.67281485 epoch total loss 1.55604494\n",
      "Trained batch 1171 batch loss 1.58425379 epoch total loss 1.55606902\n",
      "Trained batch 1172 batch loss 1.52356458 epoch total loss 1.55604136\n",
      "Trained batch 1173 batch loss 1.52605844 epoch total loss 1.55601573\n",
      "Trained batch 1174 batch loss 1.42490983 epoch total loss 1.55590403\n",
      "Trained batch 1175 batch loss 1.43738937 epoch total loss 1.55580318\n",
      "Trained batch 1176 batch loss 1.41072142 epoch total loss 1.5556798\n",
      "Trained batch 1177 batch loss 1.34602034 epoch total loss 1.5555017\n",
      "Trained batch 1178 batch loss 1.34735727 epoch total loss 1.55532515\n",
      "Trained batch 1179 batch loss 1.36340785 epoch total loss 1.55516231\n",
      "Trained batch 1180 batch loss 1.38366413 epoch total loss 1.55501699\n",
      "Trained batch 1181 batch loss 1.43823171 epoch total loss 1.55491805\n",
      "Trained batch 1182 batch loss 1.37275 epoch total loss 1.55476403\n",
      "Trained batch 1183 batch loss 1.46158874 epoch total loss 1.55468524\n",
      "Trained batch 1184 batch loss 1.433532 epoch total loss 1.55458283\n",
      "Trained batch 1185 batch loss 1.46976447 epoch total loss 1.55451119\n",
      "Trained batch 1186 batch loss 1.43452692 epoch total loss 1.5544101\n",
      "Trained batch 1187 batch loss 1.43015039 epoch total loss 1.55430543\n",
      "Trained batch 1188 batch loss 1.35523367 epoch total loss 1.55413783\n",
      "Trained batch 1189 batch loss 1.48244739 epoch total loss 1.55407751\n",
      "Trained batch 1190 batch loss 1.39647305 epoch total loss 1.55394518\n",
      "Trained batch 1191 batch loss 1.53015161 epoch total loss 1.55392516\n",
      "Trained batch 1192 batch loss 1.53447247 epoch total loss 1.55390882\n",
      "Trained batch 1193 batch loss 1.53388286 epoch total loss 1.55389202\n",
      "Trained batch 1194 batch loss 1.52839601 epoch total loss 1.55387068\n",
      "Trained batch 1195 batch loss 1.52148831 epoch total loss 1.55384362\n",
      "Trained batch 1196 batch loss 1.36233664 epoch total loss 1.55368352\n",
      "Trained batch 1197 batch loss 1.36284423 epoch total loss 1.55352402\n",
      "Trained batch 1198 batch loss 1.38455033 epoch total loss 1.55338299\n",
      "Trained batch 1199 batch loss 1.45384121 epoch total loss 1.5532999\n",
      "Trained batch 1200 batch loss 1.35481155 epoch total loss 1.55313456\n",
      "Trained batch 1201 batch loss 1.47063613 epoch total loss 1.55306578\n",
      "Trained batch 1202 batch loss 1.54849184 epoch total loss 1.55306196\n",
      "Trained batch 1203 batch loss 1.34555399 epoch total loss 1.55288947\n",
      "Trained batch 1204 batch loss 1.50272703 epoch total loss 1.55284786\n",
      "Trained batch 1205 batch loss 1.46719551 epoch total loss 1.55277669\n",
      "Trained batch 1206 batch loss 1.56263959 epoch total loss 1.55278492\n",
      "Trained batch 1207 batch loss 1.62402356 epoch total loss 1.55284393\n",
      "Trained batch 1208 batch loss 1.60147643 epoch total loss 1.5528841\n",
      "Trained batch 1209 batch loss 1.48959041 epoch total loss 1.55283177\n",
      "Trained batch 1210 batch loss 1.29773283 epoch total loss 1.55262101\n",
      "Trained batch 1211 batch loss 1.33772576 epoch total loss 1.5524435\n",
      "Trained batch 1212 batch loss 1.56251681 epoch total loss 1.55245185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1213 batch loss 1.59696305 epoch total loss 1.55248857\n",
      "Trained batch 1214 batch loss 1.59315681 epoch total loss 1.55252206\n",
      "Trained batch 1215 batch loss 1.40675092 epoch total loss 1.55240202\n",
      "Trained batch 1216 batch loss 1.51913774 epoch total loss 1.55237472\n",
      "Trained batch 1217 batch loss 1.41410768 epoch total loss 1.55226099\n",
      "Trained batch 1218 batch loss 1.40420032 epoch total loss 1.5521394\n",
      "Trained batch 1219 batch loss 1.39140856 epoch total loss 1.55200756\n",
      "Trained batch 1220 batch loss 1.49282992 epoch total loss 1.55195904\n",
      "Trained batch 1221 batch loss 1.43977046 epoch total loss 1.55186713\n",
      "Trained batch 1222 batch loss 1.52531266 epoch total loss 1.55184543\n",
      "Trained batch 1223 batch loss 1.44603193 epoch total loss 1.55175889\n",
      "Trained batch 1224 batch loss 1.46372879 epoch total loss 1.551687\n",
      "Trained batch 1225 batch loss 1.44044852 epoch total loss 1.55159616\n",
      "Trained batch 1226 batch loss 1.376544 epoch total loss 1.55145347\n",
      "Trained batch 1227 batch loss 1.42598116 epoch total loss 1.55135119\n",
      "Trained batch 1228 batch loss 1.32456481 epoch total loss 1.55116653\n",
      "Trained batch 1229 batch loss 1.39554858 epoch total loss 1.55103993\n",
      "Trained batch 1230 batch loss 1.45320737 epoch total loss 1.55096042\n",
      "Trained batch 1231 batch loss 1.45021188 epoch total loss 1.55087852\n",
      "Trained batch 1232 batch loss 1.43073201 epoch total loss 1.55078101\n",
      "Trained batch 1233 batch loss 1.47029901 epoch total loss 1.5507158\n",
      "Trained batch 1234 batch loss 1.60182261 epoch total loss 1.55075717\n",
      "Trained batch 1235 batch loss 1.51458383 epoch total loss 1.55072784\n",
      "Trained batch 1236 batch loss 1.33491135 epoch total loss 1.55055332\n",
      "Trained batch 1237 batch loss 1.4727242 epoch total loss 1.55049038\n",
      "Trained batch 1238 batch loss 1.42211807 epoch total loss 1.55038679\n",
      "Trained batch 1239 batch loss 1.2091831 epoch total loss 1.55011141\n",
      "Trained batch 1240 batch loss 1.25775945 epoch total loss 1.54987562\n",
      "Trained batch 1241 batch loss 1.26562333 epoch total loss 1.54964662\n",
      "Trained batch 1242 batch loss 1.38603187 epoch total loss 1.54951489\n",
      "Trained batch 1243 batch loss 1.42106009 epoch total loss 1.54941142\n",
      "Trained batch 1244 batch loss 1.58404255 epoch total loss 1.54943931\n",
      "Trained batch 1245 batch loss 1.56190944 epoch total loss 1.54944921\n",
      "Trained batch 1246 batch loss 1.55819166 epoch total loss 1.54945636\n",
      "Trained batch 1247 batch loss 1.47901475 epoch total loss 1.54939985\n",
      "Trained batch 1248 batch loss 1.53020883 epoch total loss 1.54938436\n",
      "Trained batch 1249 batch loss 1.46994615 epoch total loss 1.54932082\n",
      "Trained batch 1250 batch loss 1.55229402 epoch total loss 1.5493232\n",
      "Trained batch 1251 batch loss 1.52785397 epoch total loss 1.54930592\n",
      "Trained batch 1252 batch loss 1.54933453 epoch total loss 1.54930592\n",
      "Trained batch 1253 batch loss 1.53755939 epoch total loss 1.54929662\n",
      "Trained batch 1254 batch loss 1.53478706 epoch total loss 1.54928505\n",
      "Trained batch 1255 batch loss 1.54871297 epoch total loss 1.54928458\n",
      "Trained batch 1256 batch loss 1.40437174 epoch total loss 1.5491693\n",
      "Trained batch 1257 batch loss 1.33168173 epoch total loss 1.54899621\n",
      "Trained batch 1258 batch loss 1.27899635 epoch total loss 1.54878163\n",
      "Trained batch 1259 batch loss 1.20340574 epoch total loss 1.54850733\n",
      "Trained batch 1260 batch loss 1.28588164 epoch total loss 1.54829884\n",
      "Trained batch 1261 batch loss 1.54076052 epoch total loss 1.54829288\n",
      "Trained batch 1262 batch loss 1.48435283 epoch total loss 1.54824221\n",
      "Trained batch 1263 batch loss 1.54071546 epoch total loss 1.54823637\n",
      "Trained batch 1264 batch loss 1.49779391 epoch total loss 1.54819643\n",
      "Trained batch 1265 batch loss 1.58726764 epoch total loss 1.54822731\n",
      "Trained batch 1266 batch loss 1.52011323 epoch total loss 1.54820514\n",
      "Trained batch 1267 batch loss 1.46950901 epoch total loss 1.54814303\n",
      "Trained batch 1268 batch loss 1.57953548 epoch total loss 1.54816782\n",
      "Trained batch 1269 batch loss 1.50296855 epoch total loss 1.54813218\n",
      "Trained batch 1270 batch loss 1.44663239 epoch total loss 1.54805231\n",
      "Trained batch 1271 batch loss 1.46232295 epoch total loss 1.54798472\n",
      "Trained batch 1272 batch loss 1.3853277 epoch total loss 1.54785693\n",
      "Trained batch 1273 batch loss 1.29428124 epoch total loss 1.54765773\n",
      "Trained batch 1274 batch loss 1.3844142 epoch total loss 1.54752958\n",
      "Trained batch 1275 batch loss 1.44062865 epoch total loss 1.54744577\n",
      "Trained batch 1276 batch loss 1.37039399 epoch total loss 1.54730701\n",
      "Trained batch 1277 batch loss 1.5116787 epoch total loss 1.54727912\n",
      "Trained batch 1278 batch loss 1.65146744 epoch total loss 1.54736066\n",
      "Trained batch 1279 batch loss 1.45623386 epoch total loss 1.54728937\n",
      "Trained batch 1280 batch loss 1.43955445 epoch total loss 1.54720521\n",
      "Trained batch 1281 batch loss 1.48502111 epoch total loss 1.54715669\n",
      "Trained batch 1282 batch loss 1.4645493 epoch total loss 1.54709232\n",
      "Trained batch 1283 batch loss 1.53476858 epoch total loss 1.54708266\n",
      "Trained batch 1284 batch loss 1.49765754 epoch total loss 1.54704428\n",
      "Trained batch 1285 batch loss 1.51623321 epoch total loss 1.5470202\n",
      "Trained batch 1286 batch loss 1.54329419 epoch total loss 1.54701734\n",
      "Trained batch 1287 batch loss 1.43540394 epoch total loss 1.54693067\n",
      "Trained batch 1288 batch loss 1.41525424 epoch total loss 1.54682851\n",
      "Trained batch 1289 batch loss 1.4779954 epoch total loss 1.5467751\n",
      "Trained batch 1290 batch loss 1.54240763 epoch total loss 1.54677165\n",
      "Trained batch 1291 batch loss 1.56582665 epoch total loss 1.54678643\n",
      "Trained batch 1292 batch loss 1.51372683 epoch total loss 1.5467608\n",
      "Trained batch 1293 batch loss 1.46467829 epoch total loss 1.54669738\n",
      "Trained batch 1294 batch loss 1.49302208 epoch total loss 1.54665589\n",
      "Trained batch 1295 batch loss 1.41043282 epoch total loss 1.54655063\n",
      "Trained batch 1296 batch loss 1.54682231 epoch total loss 1.54655087\n",
      "Trained batch 1297 batch loss 1.54230392 epoch total loss 1.54654765\n",
      "Trained batch 1298 batch loss 1.48772073 epoch total loss 1.54650235\n",
      "Trained batch 1299 batch loss 1.40934134 epoch total loss 1.54639673\n",
      "Trained batch 1300 batch loss 1.39655614 epoch total loss 1.54628146\n",
      "Trained batch 1301 batch loss 1.24670935 epoch total loss 1.54605114\n",
      "Trained batch 1302 batch loss 1.5445894 epoch total loss 1.54605007\n",
      "Trained batch 1303 batch loss 1.33041048 epoch total loss 1.54588461\n",
      "Trained batch 1304 batch loss 1.40525699 epoch total loss 1.54577672\n",
      "Trained batch 1305 batch loss 1.49489057 epoch total loss 1.54573774\n",
      "Trained batch 1306 batch loss 1.40491199 epoch total loss 1.54562986\n",
      "Trained batch 1307 batch loss 1.46005583 epoch total loss 1.54556441\n",
      "Trained batch 1308 batch loss 1.35642898 epoch total loss 1.54541981\n",
      "Trained batch 1309 batch loss 1.45336652 epoch total loss 1.5453496\n",
      "Trained batch 1310 batch loss 1.47597158 epoch total loss 1.54529655\n",
      "Trained batch 1311 batch loss 1.5635767 epoch total loss 1.5453105\n",
      "Trained batch 1312 batch loss 1.52314854 epoch total loss 1.54529369\n",
      "Trained batch 1313 batch loss 1.39363503 epoch total loss 1.54517817\n",
      "Trained batch 1314 batch loss 1.46311724 epoch total loss 1.54511571\n",
      "Trained batch 1315 batch loss 1.4456687 epoch total loss 1.54504013\n",
      "Trained batch 1316 batch loss 1.40471935 epoch total loss 1.54493344\n",
      "Trained batch 1317 batch loss 1.36970115 epoch total loss 1.5448004\n",
      "Trained batch 1318 batch loss 1.45219874 epoch total loss 1.54473019\n",
      "Trained batch 1319 batch loss 1.34551322 epoch total loss 1.54457903\n",
      "Trained batch 1320 batch loss 1.46856523 epoch total loss 1.54452145\n",
      "Trained batch 1321 batch loss 1.40531063 epoch total loss 1.54441607\n",
      "Trained batch 1322 batch loss 1.41677439 epoch total loss 1.54431951\n",
      "Trained batch 1323 batch loss 1.52814841 epoch total loss 1.54430723\n",
      "Trained batch 1324 batch loss 1.53563309 epoch total loss 1.54430079\n",
      "Trained batch 1325 batch loss 1.43566275 epoch total loss 1.54421878\n",
      "Trained batch 1326 batch loss 1.45879567 epoch total loss 1.54415429\n",
      "Trained batch 1327 batch loss 1.60002303 epoch total loss 1.54419649\n",
      "Trained batch 1328 batch loss 1.45066929 epoch total loss 1.54412603\n",
      "Trained batch 1329 batch loss 1.47844923 epoch total loss 1.54407668\n",
      "Trained batch 1330 batch loss 1.43730521 epoch total loss 1.54399633\n",
      "Trained batch 1331 batch loss 1.45139217 epoch total loss 1.54392684\n",
      "Trained batch 1332 batch loss 1.43596077 epoch total loss 1.54384577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1333 batch loss 1.42279124 epoch total loss 1.54375505\n",
      "Trained batch 1334 batch loss 1.45808744 epoch total loss 1.54369068\n",
      "Trained batch 1335 batch loss 1.45717204 epoch total loss 1.54362595\n",
      "Trained batch 1336 batch loss 1.39777792 epoch total loss 1.54351676\n",
      "Trained batch 1337 batch loss 1.38321376 epoch total loss 1.54339695\n",
      "Trained batch 1338 batch loss 1.35853088 epoch total loss 1.54325891\n",
      "Trained batch 1339 batch loss 1.33511341 epoch total loss 1.54310346\n",
      "Trained batch 1340 batch loss 1.32116115 epoch total loss 1.54293776\n",
      "Trained batch 1341 batch loss 1.33220458 epoch total loss 1.54278064\n",
      "Trained batch 1342 batch loss 1.33482802 epoch total loss 1.54262567\n",
      "Trained batch 1343 batch loss 1.31901717 epoch total loss 1.54245925\n",
      "Trained batch 1344 batch loss 1.38165689 epoch total loss 1.54233956\n",
      "Trained batch 1345 batch loss 1.28016281 epoch total loss 1.54214466\n",
      "Trained batch 1346 batch loss 1.35605097 epoch total loss 1.54200637\n",
      "Trained batch 1347 batch loss 1.31152964 epoch total loss 1.54183519\n",
      "Trained batch 1348 batch loss 1.31950974 epoch total loss 1.54167032\n",
      "Trained batch 1349 batch loss 1.53633058 epoch total loss 1.54166639\n",
      "Trained batch 1350 batch loss 1.42911232 epoch total loss 1.54158306\n",
      "Trained batch 1351 batch loss 1.4167726 epoch total loss 1.54149067\n",
      "Trained batch 1352 batch loss 1.46813512 epoch total loss 1.54143631\n",
      "Trained batch 1353 batch loss 1.53556645 epoch total loss 1.54143214\n",
      "Trained batch 1354 batch loss 1.42007375 epoch total loss 1.5413425\n",
      "Trained batch 1355 batch loss 1.48807693 epoch total loss 1.54130316\n",
      "Trained batch 1356 batch loss 1.47296 epoch total loss 1.54125273\n",
      "Trained batch 1357 batch loss 1.45089984 epoch total loss 1.54118621\n",
      "Trained batch 1358 batch loss 1.52444816 epoch total loss 1.54117382\n",
      "Trained batch 1359 batch loss 1.53079545 epoch total loss 1.54116619\n",
      "Trained batch 1360 batch loss 1.43391967 epoch total loss 1.54108727\n",
      "Trained batch 1361 batch loss 1.5265975 epoch total loss 1.54107666\n",
      "Trained batch 1362 batch loss 1.51756406 epoch total loss 1.54105937\n",
      "Trained batch 1363 batch loss 1.47260857 epoch total loss 1.54100919\n",
      "Trained batch 1364 batch loss 1.56518805 epoch total loss 1.54102695\n",
      "Trained batch 1365 batch loss 1.42280531 epoch total loss 1.54094028\n",
      "Trained batch 1366 batch loss 1.43103194 epoch total loss 1.54085994\n",
      "Trained batch 1367 batch loss 1.43182969 epoch total loss 1.54078019\n",
      "Trained batch 1368 batch loss 1.51521659 epoch total loss 1.54076147\n",
      "Trained batch 1369 batch loss 1.38621569 epoch total loss 1.54064858\n",
      "Trained batch 1370 batch loss 1.34458876 epoch total loss 1.54050541\n",
      "Trained batch 1371 batch loss 1.38469744 epoch total loss 1.5403918\n",
      "Trained batch 1372 batch loss 1.42832446 epoch total loss 1.54031\n",
      "Trained batch 1373 batch loss 1.41172934 epoch total loss 1.54021633\n",
      "Trained batch 1374 batch loss 1.41637588 epoch total loss 1.54012609\n",
      "Trained batch 1375 batch loss 1.37155211 epoch total loss 1.54000354\n",
      "Trained batch 1376 batch loss 1.49944949 epoch total loss 1.53997409\n",
      "Trained batch 1377 batch loss 1.45705295 epoch total loss 1.53991389\n",
      "Trained batch 1378 batch loss 1.48045468 epoch total loss 1.53987074\n",
      "Trained batch 1379 batch loss 1.49744689 epoch total loss 1.5398401\n",
      "Trained batch 1380 batch loss 1.53267384 epoch total loss 1.53983486\n",
      "Trained batch 1381 batch loss 1.43289018 epoch total loss 1.53975749\n",
      "Trained batch 1382 batch loss 1.4293 epoch total loss 1.5396775\n",
      "Trained batch 1383 batch loss 1.4317776 epoch total loss 1.53959954\n",
      "Trained batch 1384 batch loss 1.40034568 epoch total loss 1.53949893\n",
      "Trained batch 1385 batch loss 1.36270642 epoch total loss 1.53937137\n",
      "Trained batch 1386 batch loss 1.32322502 epoch total loss 1.53921545\n",
      "Trained batch 1387 batch loss 1.39643669 epoch total loss 1.53911245\n",
      "Trained batch 1388 batch loss 1.41422677 epoch total loss 1.53902256\n",
      "Epoch 1 train loss 1.5390225648880005\n",
      "Validated batch 1 batch loss 1.49790037\n",
      "Validated batch 2 batch loss 1.42651796\n",
      "Validated batch 3 batch loss 1.38979101\n",
      "Validated batch 4 batch loss 1.34659481\n",
      "Validated batch 5 batch loss 1.3947413\n",
      "Validated batch 6 batch loss 1.41142142\n",
      "Validated batch 7 batch loss 1.37661982\n",
      "Validated batch 8 batch loss 1.35729933\n",
      "Validated batch 9 batch loss 1.46579564\n",
      "Validated batch 10 batch loss 1.43512726\n",
      "Validated batch 11 batch loss 1.37649727\n",
      "Validated batch 12 batch loss 1.32441711\n",
      "Validated batch 13 batch loss 1.40277267\n",
      "Validated batch 14 batch loss 1.43489742\n",
      "Validated batch 15 batch loss 1.50451827\n",
      "Validated batch 16 batch loss 1.44455719\n",
      "Validated batch 17 batch loss 1.39741206\n",
      "Validated batch 18 batch loss 1.52992165\n",
      "Validated batch 19 batch loss 1.4440155\n",
      "Validated batch 20 batch loss 1.4261452\n",
      "Validated batch 21 batch loss 1.49628639\n",
      "Validated batch 22 batch loss 1.20904613\n",
      "Validated batch 23 batch loss 1.49549162\n",
      "Validated batch 24 batch loss 1.40586555\n",
      "Validated batch 25 batch loss 1.31222796\n",
      "Validated batch 26 batch loss 1.40439773\n",
      "Validated batch 27 batch loss 1.37764144\n",
      "Validated batch 28 batch loss 1.40257597\n",
      "Validated batch 29 batch loss 1.46306729\n",
      "Validated batch 30 batch loss 1.42310095\n",
      "Validated batch 31 batch loss 1.5380851\n",
      "Validated batch 32 batch loss 1.46358359\n",
      "Validated batch 33 batch loss 1.45535696\n",
      "Validated batch 34 batch loss 1.40969157\n",
      "Validated batch 35 batch loss 1.46050692\n",
      "Validated batch 36 batch loss 1.47688413\n",
      "Validated batch 37 batch loss 1.50932837\n",
      "Validated batch 38 batch loss 1.46468234\n",
      "Validated batch 39 batch loss 1.46348548\n",
      "Validated batch 40 batch loss 1.48605\n",
      "Validated batch 41 batch loss 1.2996968\n",
      "Validated batch 42 batch loss 1.405581\n",
      "Validated batch 43 batch loss 1.43400085\n",
      "Validated batch 44 batch loss 1.43271899\n",
      "Validated batch 45 batch loss 1.46546292\n",
      "Validated batch 46 batch loss 1.37923217\n",
      "Validated batch 47 batch loss 1.41318846\n",
      "Validated batch 48 batch loss 1.44354463\n",
      "Validated batch 49 batch loss 1.32269156\n",
      "Validated batch 50 batch loss 1.5067966\n",
      "Validated batch 51 batch loss 1.42219138\n",
      "Validated batch 52 batch loss 1.41348851\n",
      "Validated batch 53 batch loss 1.46574092\n",
      "Validated batch 54 batch loss 1.48039734\n",
      "Validated batch 55 batch loss 1.47058213\n",
      "Validated batch 56 batch loss 1.46095598\n",
      "Validated batch 57 batch loss 1.54774225\n",
      "Validated batch 58 batch loss 1.44435501\n",
      "Validated batch 59 batch loss 1.38803053\n",
      "Validated batch 60 batch loss 1.52222514\n",
      "Validated batch 61 batch loss 1.43284798\n",
      "Validated batch 62 batch loss 1.42415679\n",
      "Validated batch 63 batch loss 1.57533598\n",
      "Validated batch 64 batch loss 1.24299073\n",
      "Validated batch 65 batch loss 1.43888032\n",
      "Validated batch 66 batch loss 1.36850011\n",
      "Validated batch 67 batch loss 1.39509237\n",
      "Validated batch 68 batch loss 1.55382264\n",
      "Validated batch 69 batch loss 1.3653245\n",
      "Validated batch 70 batch loss 1.30725968\n",
      "Validated batch 71 batch loss 1.48857272\n",
      "Validated batch 72 batch loss 1.48488855\n",
      "Validated batch 73 batch loss 1.3968147\n",
      "Validated batch 74 batch loss 1.46304464\n",
      "Validated batch 75 batch loss 1.52167022\n",
      "Validated batch 76 batch loss 1.27492952\n",
      "Validated batch 77 batch loss 1.48499811\n",
      "Validated batch 78 batch loss 1.40348744\n",
      "Validated batch 79 batch loss 1.44432044\n",
      "Validated batch 80 batch loss 1.4739269\n",
      "Validated batch 81 batch loss 1.35691047\n",
      "Validated batch 82 batch loss 1.25603604\n",
      "Validated batch 83 batch loss 1.43989348\n",
      "Validated batch 84 batch loss 1.43021297\n",
      "Validated batch 85 batch loss 1.40388179\n",
      "Validated batch 86 batch loss 1.44345093\n",
      "Validated batch 87 batch loss 1.43255234\n",
      "Validated batch 88 batch loss 1.44000793\n",
      "Validated batch 89 batch loss 1.53172946\n",
      "Validated batch 90 batch loss 1.46241093\n",
      "Validated batch 91 batch loss 1.43873465\n",
      "Validated batch 92 batch loss 1.34755301\n",
      "Validated batch 93 batch loss 1.47291756\n",
      "Validated batch 94 batch loss 1.39505482\n",
      "Validated batch 95 batch loss 1.39181101\n",
      "Validated batch 96 batch loss 1.36058617\n",
      "Validated batch 97 batch loss 1.39422548\n",
      "Validated batch 98 batch loss 1.51594198\n",
      "Validated batch 99 batch loss 1.34562933\n",
      "Validated batch 100 batch loss 1.45097494\n",
      "Validated batch 101 batch loss 1.42641747\n",
      "Validated batch 102 batch loss 1.42651629\n",
      "Validated batch 103 batch loss 1.40156233\n",
      "Validated batch 104 batch loss 1.32218111\n",
      "Validated batch 105 batch loss 1.4723177\n",
      "Validated batch 106 batch loss 1.45816112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 107 batch loss 1.42789924\n",
      "Validated batch 108 batch loss 1.40575671\n",
      "Validated batch 109 batch loss 1.42358208\n",
      "Validated batch 110 batch loss 1.3100636\n",
      "Validated batch 111 batch loss 1.34455323\n",
      "Validated batch 112 batch loss 1.40811098\n",
      "Validated batch 113 batch loss 1.3063587\n",
      "Validated batch 114 batch loss 1.43204451\n",
      "Validated batch 115 batch loss 1.42167652\n",
      "Validated batch 116 batch loss 1.45346069\n",
      "Validated batch 117 batch loss 1.41049075\n",
      "Validated batch 118 batch loss 1.41118693\n",
      "Validated batch 119 batch loss 1.25380683\n",
      "Validated batch 120 batch loss 1.32562816\n",
      "Validated batch 121 batch loss 1.52022922\n",
      "Validated batch 122 batch loss 1.35496724\n",
      "Validated batch 123 batch loss 1.34950018\n",
      "Validated batch 124 batch loss 1.42152381\n",
      "Validated batch 125 batch loss 1.43498755\n",
      "Validated batch 126 batch loss 1.37623191\n",
      "Validated batch 127 batch loss 1.42040682\n",
      "Validated batch 128 batch loss 1.41494083\n",
      "Validated batch 129 batch loss 1.37629235\n",
      "Validated batch 130 batch loss 1.46370387\n",
      "Validated batch 131 batch loss 1.54491007\n",
      "Validated batch 132 batch loss 1.36498249\n",
      "Validated batch 133 batch loss 1.55188966\n",
      "Validated batch 134 batch loss 1.34149063\n",
      "Validated batch 135 batch loss 1.33978415\n",
      "Validated batch 136 batch loss 1.39305925\n",
      "Validated batch 137 batch loss 1.43859935\n",
      "Validated batch 138 batch loss 1.50872648\n",
      "Validated batch 139 batch loss 1.55812836\n",
      "Validated batch 140 batch loss 1.42821085\n",
      "Validated batch 141 batch loss 1.43669665\n",
      "Validated batch 142 batch loss 1.40734076\n",
      "Validated batch 143 batch loss 1.36880958\n",
      "Validated batch 144 batch loss 1.47431719\n",
      "Validated batch 145 batch loss 1.41856062\n",
      "Validated batch 146 batch loss 1.34141278\n",
      "Validated batch 147 batch loss 1.3699106\n",
      "Validated batch 148 batch loss 1.41877556\n",
      "Validated batch 149 batch loss 1.39931238\n",
      "Validated batch 150 batch loss 1.42684579\n",
      "Validated batch 151 batch loss 1.41962409\n",
      "Validated batch 152 batch loss 1.34047306\n",
      "Validated batch 153 batch loss 1.38250363\n",
      "Validated batch 154 batch loss 1.43231106\n",
      "Validated batch 155 batch loss 1.42353094\n",
      "Validated batch 156 batch loss 1.43187642\n",
      "Validated batch 157 batch loss 1.51157689\n",
      "Validated batch 158 batch loss 1.66093946\n",
      "Validated batch 159 batch loss 1.57512403\n",
      "Validated batch 160 batch loss 1.41589856\n",
      "Validated batch 161 batch loss 1.31295753\n",
      "Validated batch 162 batch loss 1.35507536\n",
      "Validated batch 163 batch loss 1.34707129\n",
      "Validated batch 164 batch loss 1.4142611\n",
      "Validated batch 165 batch loss 1.36864877\n",
      "Validated batch 166 batch loss 1.35939646\n",
      "Validated batch 167 batch loss 1.43386304\n",
      "Validated batch 168 batch loss 1.37005281\n",
      "Validated batch 169 batch loss 1.41278458\n",
      "Validated batch 170 batch loss 1.468575\n",
      "Validated batch 171 batch loss 1.34514976\n",
      "Validated batch 172 batch loss 1.51895428\n",
      "Validated batch 173 batch loss 1.50909567\n",
      "Validated batch 174 batch loss 1.33307815\n",
      "Validated batch 175 batch loss 1.38938797\n",
      "Validated batch 176 batch loss 1.44938588\n",
      "Validated batch 177 batch loss 1.39846683\n",
      "Validated batch 178 batch loss 1.50250399\n",
      "Validated batch 179 batch loss 1.39375436\n",
      "Validated batch 180 batch loss 1.477736\n",
      "Validated batch 181 batch loss 1.38225985\n",
      "Validated batch 182 batch loss 1.45429051\n",
      "Validated batch 183 batch loss 1.42244971\n",
      "Validated batch 184 batch loss 1.38239121\n",
      "Validated batch 185 batch loss 1.5130918\n",
      "Epoch 1 val loss 1.422103762626648\n",
      "Model /aiffel/aiffel/mpii/a/model-epoch-1-loss-1.4221.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.419065 epoch total loss 1.419065\n",
      "Trained batch 2 batch loss 1.27430367 epoch total loss 1.34668434\n",
      "Trained batch 3 batch loss 1.40009975 epoch total loss 1.36448956\n",
      "Trained batch 4 batch loss 1.37642467 epoch total loss 1.36747336\n",
      "Trained batch 5 batch loss 1.32485092 epoch total loss 1.35894895\n",
      "Trained batch 6 batch loss 1.37805593 epoch total loss 1.36213338\n",
      "Trained batch 7 batch loss 1.32258916 epoch total loss 1.35648417\n",
      "Trained batch 8 batch loss 1.35198653 epoch total loss 1.35592198\n",
      "Trained batch 9 batch loss 1.37763476 epoch total loss 1.35833454\n",
      "Trained batch 10 batch loss 1.40740156 epoch total loss 1.3632412\n",
      "Trained batch 11 batch loss 1.44357538 epoch total loss 1.37054431\n",
      "Trained batch 12 batch loss 1.29928076 epoch total loss 1.36460555\n",
      "Trained batch 13 batch loss 1.36300278 epoch total loss 1.36448228\n",
      "Trained batch 14 batch loss 1.4641341 epoch total loss 1.37160027\n",
      "Trained batch 15 batch loss 1.44379056 epoch total loss 1.37641299\n",
      "Trained batch 16 batch loss 1.37224054 epoch total loss 1.37615216\n",
      "Trained batch 17 batch loss 1.44885635 epoch total loss 1.38042891\n",
      "Trained batch 18 batch loss 1.34145856 epoch total loss 1.37826395\n",
      "Trained batch 19 batch loss 1.33432329 epoch total loss 1.37595129\n",
      "Trained batch 20 batch loss 1.38722992 epoch total loss 1.37651515\n",
      "Trained batch 21 batch loss 1.31668293 epoch total loss 1.37366605\n",
      "Trained batch 22 batch loss 1.33864367 epoch total loss 1.37207413\n",
      "Trained batch 23 batch loss 1.34016204 epoch total loss 1.37068665\n",
      "Trained batch 24 batch loss 1.26666117 epoch total loss 1.3663522\n",
      "Trained batch 25 batch loss 1.1809051 epoch total loss 1.35893428\n",
      "Trained batch 26 batch loss 1.29595685 epoch total loss 1.35651207\n",
      "Trained batch 27 batch loss 1.36228967 epoch total loss 1.35672605\n",
      "Trained batch 28 batch loss 1.38424301 epoch total loss 1.35770881\n",
      "Trained batch 29 batch loss 1.45008683 epoch total loss 1.36089432\n",
      "Trained batch 30 batch loss 1.30737031 epoch total loss 1.35911012\n",
      "Trained batch 31 batch loss 1.39824688 epoch total loss 1.36037266\n",
      "Trained batch 32 batch loss 1.4137888 epoch total loss 1.36204183\n",
      "Trained batch 33 batch loss 1.39508331 epoch total loss 1.36304307\n",
      "Trained batch 34 batch loss 1.27695632 epoch total loss 1.36051106\n",
      "Trained batch 35 batch loss 1.44731343 epoch total loss 1.36299121\n",
      "Trained batch 36 batch loss 1.36250687 epoch total loss 1.36297774\n",
      "Trained batch 37 batch loss 1.21617734 epoch total loss 1.35901022\n",
      "Trained batch 38 batch loss 1.13653231 epoch total loss 1.35315549\n",
      "Trained batch 39 batch loss 1.28413534 epoch total loss 1.35138571\n",
      "Trained batch 40 batch loss 1.42651832 epoch total loss 1.35326409\n",
      "Trained batch 41 batch loss 1.68515372 epoch total loss 1.36135888\n",
      "Trained batch 42 batch loss 1.56562448 epoch total loss 1.36622238\n",
      "Trained batch 43 batch loss 1.35808229 epoch total loss 1.36603308\n",
      "Trained batch 44 batch loss 1.37061584 epoch total loss 1.36613727\n",
      "Trained batch 45 batch loss 1.30020881 epoch total loss 1.36467218\n",
      "Trained batch 46 batch loss 1.33738303 epoch total loss 1.36407888\n",
      "Trained batch 47 batch loss 1.40516412 epoch total loss 1.36495304\n",
      "Trained batch 48 batch loss 1.45462692 epoch total loss 1.36682129\n",
      "Trained batch 49 batch loss 1.57515907 epoch total loss 1.37107301\n",
      "Trained batch 50 batch loss 1.44087875 epoch total loss 1.37246919\n",
      "Trained batch 51 batch loss 1.53135705 epoch total loss 1.3755846\n",
      "Trained batch 52 batch loss 1.5065608 epoch total loss 1.37810338\n",
      "Trained batch 53 batch loss 1.55480385 epoch total loss 1.3814373\n",
      "Trained batch 54 batch loss 1.41751277 epoch total loss 1.38210535\n",
      "Trained batch 55 batch loss 1.4082582 epoch total loss 1.38258088\n",
      "Trained batch 56 batch loss 1.447703 epoch total loss 1.38374364\n",
      "Trained batch 57 batch loss 1.52334297 epoch total loss 1.3861928\n",
      "Trained batch 58 batch loss 1.42606795 epoch total loss 1.3868804\n",
      "Trained batch 59 batch loss 1.39314091 epoch total loss 1.38698661\n",
      "Trained batch 60 batch loss 1.35776949 epoch total loss 1.38649964\n",
      "Trained batch 61 batch loss 1.33431196 epoch total loss 1.38564408\n",
      "Trained batch 62 batch loss 1.37172282 epoch total loss 1.38541949\n",
      "Trained batch 63 batch loss 1.42660654 epoch total loss 1.38607323\n",
      "Trained batch 64 batch loss 1.44205737 epoch total loss 1.38694799\n",
      "Trained batch 65 batch loss 1.41159177 epoch total loss 1.38732708\n",
      "Trained batch 66 batch loss 1.52026153 epoch total loss 1.38934135\n",
      "Trained batch 67 batch loss 1.42285442 epoch total loss 1.38984144\n",
      "Trained batch 68 batch loss 1.35747898 epoch total loss 1.38936543\n",
      "Trained batch 69 batch loss 1.293455 epoch total loss 1.38797545\n",
      "Trained batch 70 batch loss 1.34053087 epoch total loss 1.38729775\n",
      "Trained batch 71 batch loss 1.35645163 epoch total loss 1.38686323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 72 batch loss 1.32909107 epoch total loss 1.38606095\n",
      "Trained batch 73 batch loss 1.44886172 epoch total loss 1.38692117\n",
      "Trained batch 74 batch loss 1.47787023 epoch total loss 1.38815022\n",
      "Trained batch 75 batch loss 1.32791698 epoch total loss 1.3873471\n",
      "Trained batch 76 batch loss 1.51812541 epoch total loss 1.38906789\n",
      "Trained batch 77 batch loss 1.41818047 epoch total loss 1.38944602\n",
      "Trained batch 78 batch loss 1.49270916 epoch total loss 1.39076984\n",
      "Trained batch 79 batch loss 1.46551788 epoch total loss 1.391716\n",
      "Trained batch 80 batch loss 1.66929662 epoch total loss 1.39518571\n",
      "Trained batch 81 batch loss 1.54500127 epoch total loss 1.39703524\n",
      "Trained batch 82 batch loss 1.35842371 epoch total loss 1.39656436\n",
      "Trained batch 83 batch loss 1.27835035 epoch total loss 1.39514017\n",
      "Trained batch 84 batch loss 1.5200187 epoch total loss 1.39662683\n",
      "Trained batch 85 batch loss 1.56755185 epoch total loss 1.39863765\n",
      "Trained batch 86 batch loss 1.48757195 epoch total loss 1.39967179\n",
      "Trained batch 87 batch loss 1.42668211 epoch total loss 1.39998221\n",
      "Trained batch 88 batch loss 1.36778 epoch total loss 1.39961636\n",
      "Trained batch 89 batch loss 1.42496395 epoch total loss 1.39990115\n",
      "Trained batch 90 batch loss 1.42125797 epoch total loss 1.40013838\n",
      "Trained batch 91 batch loss 1.49167597 epoch total loss 1.40114439\n",
      "Trained batch 92 batch loss 1.47051549 epoch total loss 1.40189838\n",
      "Trained batch 93 batch loss 1.4941932 epoch total loss 1.4028908\n",
      "Trained batch 94 batch loss 1.42158377 epoch total loss 1.40308964\n",
      "Trained batch 95 batch loss 1.48599684 epoch total loss 1.40396225\n",
      "Trained batch 96 batch loss 1.43550193 epoch total loss 1.4042908\n",
      "Trained batch 97 batch loss 1.3714478 epoch total loss 1.40395224\n",
      "Trained batch 98 batch loss 1.44360447 epoch total loss 1.40435684\n",
      "Trained batch 99 batch loss 1.478544 epoch total loss 1.40510619\n",
      "Trained batch 100 batch loss 1.66988504 epoch total loss 1.40775406\n",
      "Trained batch 101 batch loss 1.50044608 epoch total loss 1.40867174\n",
      "Trained batch 102 batch loss 1.50263047 epoch total loss 1.40959287\n",
      "Trained batch 103 batch loss 1.51605582 epoch total loss 1.41062641\n",
      "Trained batch 104 batch loss 1.5721755 epoch total loss 1.41217983\n",
      "Trained batch 105 batch loss 1.62112594 epoch total loss 1.41416979\n",
      "Trained batch 106 batch loss 1.55614209 epoch total loss 1.4155091\n",
      "Trained batch 107 batch loss 1.46064293 epoch total loss 1.41593087\n",
      "Trained batch 108 batch loss 1.38499093 epoch total loss 1.41564453\n",
      "Trained batch 109 batch loss 1.36429048 epoch total loss 1.41517329\n",
      "Trained batch 110 batch loss 1.4468894 epoch total loss 1.41546154\n",
      "Trained batch 111 batch loss 1.40913737 epoch total loss 1.41540456\n",
      "Trained batch 112 batch loss 1.35610509 epoch total loss 1.41487515\n",
      "Trained batch 113 batch loss 1.44078112 epoch total loss 1.41510439\n",
      "Trained batch 114 batch loss 1.43626714 epoch total loss 1.41529\n",
      "Trained batch 115 batch loss 1.46197855 epoch total loss 1.41569602\n",
      "Trained batch 116 batch loss 1.37367296 epoch total loss 1.41533375\n",
      "Trained batch 117 batch loss 1.39261389 epoch total loss 1.41513944\n",
      "Trained batch 118 batch loss 1.51575756 epoch total loss 1.41599226\n",
      "Trained batch 119 batch loss 1.47783804 epoch total loss 1.41651201\n",
      "Trained batch 120 batch loss 1.45150959 epoch total loss 1.4168036\n",
      "Trained batch 121 batch loss 1.51082766 epoch total loss 1.41758072\n",
      "Trained batch 122 batch loss 1.47991252 epoch total loss 1.41809165\n",
      "Trained batch 123 batch loss 1.49342442 epoch total loss 1.41870415\n",
      "Trained batch 124 batch loss 1.43281209 epoch total loss 1.418818\n",
      "Trained batch 125 batch loss 1.50795293 epoch total loss 1.41953099\n",
      "Trained batch 126 batch loss 1.44054091 epoch total loss 1.41969776\n",
      "Trained batch 127 batch loss 1.41626239 epoch total loss 1.4196707\n",
      "Trained batch 128 batch loss 1.37442207 epoch total loss 1.41931713\n",
      "Trained batch 129 batch loss 1.35368788 epoch total loss 1.41880834\n",
      "Trained batch 130 batch loss 1.32454 epoch total loss 1.41808319\n",
      "Trained batch 131 batch loss 1.15552056 epoch total loss 1.41607893\n",
      "Trained batch 132 batch loss 1.49277735 epoch total loss 1.41666\n",
      "Trained batch 133 batch loss 1.47879064 epoch total loss 1.41712713\n",
      "Trained batch 134 batch loss 1.52531862 epoch total loss 1.41793442\n",
      "Trained batch 135 batch loss 1.46031535 epoch total loss 1.41824841\n",
      "Trained batch 136 batch loss 1.53329396 epoch total loss 1.41909432\n",
      "Trained batch 137 batch loss 1.46202374 epoch total loss 1.41940761\n",
      "Trained batch 138 batch loss 1.26640582 epoch total loss 1.41829896\n",
      "Trained batch 139 batch loss 1.36440659 epoch total loss 1.41791117\n",
      "Trained batch 140 batch loss 1.41253376 epoch total loss 1.41787279\n",
      "Trained batch 141 batch loss 1.45190597 epoch total loss 1.41811419\n",
      "Trained batch 142 batch loss 1.4096334 epoch total loss 1.41805446\n",
      "Trained batch 143 batch loss 1.46967626 epoch total loss 1.41841555\n",
      "Trained batch 144 batch loss 1.47820294 epoch total loss 1.41883075\n",
      "Trained batch 145 batch loss 1.44274974 epoch total loss 1.41899574\n",
      "Trained batch 146 batch loss 1.48474169 epoch total loss 1.41944599\n",
      "Trained batch 147 batch loss 1.48097849 epoch total loss 1.41986454\n",
      "Trained batch 148 batch loss 1.55031991 epoch total loss 1.42074609\n",
      "Trained batch 149 batch loss 1.43118024 epoch total loss 1.42081606\n",
      "Trained batch 150 batch loss 1.5019244 epoch total loss 1.4213568\n",
      "Trained batch 151 batch loss 1.46131599 epoch total loss 1.42162144\n",
      "Trained batch 152 batch loss 1.47730815 epoch total loss 1.42198789\n",
      "Trained batch 153 batch loss 1.4309727 epoch total loss 1.42204654\n",
      "Trained batch 154 batch loss 1.49475729 epoch total loss 1.42251861\n",
      "Trained batch 155 batch loss 1.45057082 epoch total loss 1.42269969\n",
      "Trained batch 156 batch loss 1.37923491 epoch total loss 1.4224211\n",
      "Trained batch 157 batch loss 1.31677544 epoch total loss 1.42174816\n",
      "Trained batch 158 batch loss 1.47307897 epoch total loss 1.42207313\n",
      "Trained batch 159 batch loss 1.3538475 epoch total loss 1.42164397\n",
      "Trained batch 160 batch loss 1.4467082 epoch total loss 1.42180061\n",
      "Trained batch 161 batch loss 1.40570152 epoch total loss 1.4217006\n",
      "Trained batch 162 batch loss 1.32930374 epoch total loss 1.42113018\n",
      "Trained batch 163 batch loss 1.42477858 epoch total loss 1.42115259\n",
      "Trained batch 164 batch loss 1.52092731 epoch total loss 1.42176092\n",
      "Trained batch 165 batch loss 1.43339562 epoch total loss 1.42183149\n",
      "Trained batch 166 batch loss 1.43722761 epoch total loss 1.42192423\n",
      "Trained batch 167 batch loss 1.3605659 epoch total loss 1.42155671\n",
      "Trained batch 168 batch loss 1.33297575 epoch total loss 1.42102945\n",
      "Trained batch 169 batch loss 1.31642854 epoch total loss 1.42041051\n",
      "Trained batch 170 batch loss 1.33122408 epoch total loss 1.41988587\n",
      "Trained batch 171 batch loss 1.40267408 epoch total loss 1.41978526\n",
      "Trained batch 172 batch loss 1.52863848 epoch total loss 1.42041814\n",
      "Trained batch 173 batch loss 1.39760876 epoch total loss 1.4202863\n",
      "Trained batch 174 batch loss 1.30293369 epoch total loss 1.41961193\n",
      "Trained batch 175 batch loss 1.30126488 epoch total loss 1.41893566\n",
      "Trained batch 176 batch loss 1.31912112 epoch total loss 1.41836846\n",
      "Trained batch 177 batch loss 1.44945478 epoch total loss 1.41854405\n",
      "Trained batch 178 batch loss 1.4660306 epoch total loss 1.41881084\n",
      "Trained batch 179 batch loss 1.47588384 epoch total loss 1.41912973\n",
      "Trained batch 180 batch loss 1.47985649 epoch total loss 1.41946721\n",
      "Trained batch 181 batch loss 1.30030251 epoch total loss 1.4188087\n",
      "Trained batch 182 batch loss 1.3277719 epoch total loss 1.4183085\n",
      "Trained batch 183 batch loss 1.4115026 epoch total loss 1.4182713\n",
      "Trained batch 184 batch loss 1.42211556 epoch total loss 1.41829216\n",
      "Trained batch 185 batch loss 1.33448827 epoch total loss 1.41783929\n",
      "Trained batch 186 batch loss 1.29489303 epoch total loss 1.41717827\n",
      "Trained batch 187 batch loss 1.36790788 epoch total loss 1.41691482\n",
      "Trained batch 188 batch loss 1.42866087 epoch total loss 1.41697729\n",
      "Trained batch 189 batch loss 1.4601512 epoch total loss 1.41720569\n",
      "Trained batch 190 batch loss 1.47056389 epoch total loss 1.41748643\n",
      "Trained batch 191 batch loss 1.32130873 epoch total loss 1.41698289\n",
      "Trained batch 192 batch loss 1.31008899 epoch total loss 1.41642618\n",
      "Trained batch 193 batch loss 1.27932107 epoch total loss 1.41571581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 194 batch loss 1.28205895 epoch total loss 1.4150269\n",
      "Trained batch 195 batch loss 1.34817302 epoch total loss 1.41468418\n",
      "Trained batch 196 batch loss 1.43668079 epoch total loss 1.41479635\n",
      "Trained batch 197 batch loss 1.39348388 epoch total loss 1.41468823\n",
      "Trained batch 198 batch loss 1.44575357 epoch total loss 1.41484499\n",
      "Trained batch 199 batch loss 1.49126101 epoch total loss 1.41522908\n",
      "Trained batch 200 batch loss 1.39054298 epoch total loss 1.41510558\n",
      "Trained batch 201 batch loss 1.30517161 epoch total loss 1.41455865\n",
      "Trained batch 202 batch loss 1.38344431 epoch total loss 1.41440463\n",
      "Trained batch 203 batch loss 1.41457784 epoch total loss 1.41440558\n",
      "Trained batch 204 batch loss 1.36763263 epoch total loss 1.41417634\n",
      "Trained batch 205 batch loss 1.29469347 epoch total loss 1.41359353\n",
      "Trained batch 206 batch loss 1.33555663 epoch total loss 1.4132148\n",
      "Trained batch 207 batch loss 1.45250583 epoch total loss 1.4134047\n",
      "Trained batch 208 batch loss 1.35927355 epoch total loss 1.41314447\n",
      "Trained batch 209 batch loss 1.36764169 epoch total loss 1.41292679\n",
      "Trained batch 210 batch loss 1.40775394 epoch total loss 1.41290212\n",
      "Trained batch 211 batch loss 1.49224126 epoch total loss 1.4132781\n",
      "Trained batch 212 batch loss 1.422732 epoch total loss 1.41332269\n",
      "Trained batch 213 batch loss 1.48484385 epoch total loss 1.4136585\n",
      "Trained batch 214 batch loss 1.31313443 epoch total loss 1.41318882\n",
      "Trained batch 215 batch loss 1.3680706 epoch total loss 1.41297889\n",
      "Trained batch 216 batch loss 1.33295226 epoch total loss 1.41260839\n",
      "Trained batch 217 batch loss 1.43558598 epoch total loss 1.41271424\n",
      "Trained batch 218 batch loss 1.40408885 epoch total loss 1.41267467\n",
      "Trained batch 219 batch loss 1.35592079 epoch total loss 1.4124155\n",
      "Trained batch 220 batch loss 1.38704073 epoch total loss 1.41230023\n",
      "Trained batch 221 batch loss 1.3948704 epoch total loss 1.41222131\n",
      "Trained batch 222 batch loss 1.26842427 epoch total loss 1.41157365\n",
      "Trained batch 223 batch loss 1.31980193 epoch total loss 1.41116214\n",
      "Trained batch 224 batch loss 1.38419652 epoch total loss 1.41104162\n",
      "Trained batch 225 batch loss 1.52120817 epoch total loss 1.41153133\n",
      "Trained batch 226 batch loss 1.42791247 epoch total loss 1.41160381\n",
      "Trained batch 227 batch loss 1.35099769 epoch total loss 1.41133678\n",
      "Trained batch 228 batch loss 1.31410146 epoch total loss 1.41091025\n",
      "Trained batch 229 batch loss 1.28375375 epoch total loss 1.41035497\n",
      "Trained batch 230 batch loss 1.27667344 epoch total loss 1.40977371\n",
      "Trained batch 231 batch loss 1.29991305 epoch total loss 1.40929818\n",
      "Trained batch 232 batch loss 1.25550687 epoch total loss 1.40863526\n",
      "Trained batch 233 batch loss 1.33308709 epoch total loss 1.40831101\n",
      "Trained batch 234 batch loss 1.3667562 epoch total loss 1.40813351\n",
      "Trained batch 235 batch loss 1.36860538 epoch total loss 1.40796518\n",
      "Trained batch 236 batch loss 1.43868351 epoch total loss 1.40809536\n",
      "Trained batch 237 batch loss 1.4818635 epoch total loss 1.40840673\n",
      "Trained batch 238 batch loss 1.40332389 epoch total loss 1.40838528\n",
      "Trained batch 239 batch loss 1.39117908 epoch total loss 1.40831327\n",
      "Trained batch 240 batch loss 1.44364309 epoch total loss 1.4084605\n",
      "Trained batch 241 batch loss 1.35240829 epoch total loss 1.40822792\n",
      "Trained batch 242 batch loss 1.36846852 epoch total loss 1.40806365\n",
      "Trained batch 243 batch loss 1.45590317 epoch total loss 1.40826046\n",
      "Trained batch 244 batch loss 1.46019506 epoch total loss 1.40847337\n",
      "Trained batch 245 batch loss 1.49508429 epoch total loss 1.40882695\n",
      "Trained batch 246 batch loss 1.47277474 epoch total loss 1.40908694\n",
      "Trained batch 247 batch loss 1.34947968 epoch total loss 1.40884554\n",
      "Trained batch 248 batch loss 1.30965137 epoch total loss 1.40844572\n",
      "Trained batch 249 batch loss 1.44068813 epoch total loss 1.40857506\n",
      "Trained batch 250 batch loss 1.44174123 epoch total loss 1.40870774\n",
      "Trained batch 251 batch loss 1.42730618 epoch total loss 1.40878189\n",
      "Trained batch 252 batch loss 1.49883747 epoch total loss 1.40913928\n",
      "Trained batch 253 batch loss 1.39218533 epoch total loss 1.40907216\n",
      "Trained batch 254 batch loss 1.4269042 epoch total loss 1.40914249\n",
      "Trained batch 255 batch loss 1.43026626 epoch total loss 1.40922523\n",
      "Trained batch 256 batch loss 1.28980303 epoch total loss 1.40875876\n",
      "Trained batch 257 batch loss 1.35048401 epoch total loss 1.40853202\n",
      "Trained batch 258 batch loss 1.45364535 epoch total loss 1.4087069\n",
      "Trained batch 259 batch loss 1.48422837 epoch total loss 1.40899849\n",
      "Trained batch 260 batch loss 1.45119703 epoch total loss 1.40916073\n",
      "Trained batch 261 batch loss 1.43064702 epoch total loss 1.40924311\n",
      "Trained batch 262 batch loss 1.36769462 epoch total loss 1.40908456\n",
      "Trained batch 263 batch loss 1.28508544 epoch total loss 1.40861309\n",
      "Trained batch 264 batch loss 1.22844601 epoch total loss 1.40793061\n",
      "Trained batch 265 batch loss 1.27005219 epoch total loss 1.40741038\n",
      "Trained batch 266 batch loss 1.34539866 epoch total loss 1.40717721\n",
      "Trained batch 267 batch loss 1.72414291 epoch total loss 1.40836442\n",
      "Trained batch 268 batch loss 1.51512265 epoch total loss 1.40876281\n",
      "Trained batch 269 batch loss 1.37272406 epoch total loss 1.40862882\n",
      "Trained batch 270 batch loss 1.42350292 epoch total loss 1.40868378\n",
      "Trained batch 271 batch loss 1.46297336 epoch total loss 1.40888417\n",
      "Trained batch 272 batch loss 1.44067764 epoch total loss 1.40900111\n",
      "Trained batch 273 batch loss 1.33934259 epoch total loss 1.408746\n",
      "Trained batch 274 batch loss 1.36585808 epoch total loss 1.40858936\n",
      "Trained batch 275 batch loss 1.39279079 epoch total loss 1.4085319\n",
      "Trained batch 276 batch loss 1.41007519 epoch total loss 1.40853751\n",
      "Trained batch 277 batch loss 1.39115167 epoch total loss 1.40847468\n",
      "Trained batch 278 batch loss 1.38567781 epoch total loss 1.40839267\n",
      "Trained batch 279 batch loss 1.40561533 epoch total loss 1.40838277\n",
      "Trained batch 280 batch loss 1.33364701 epoch total loss 1.40811586\n",
      "Trained batch 281 batch loss 1.30142951 epoch total loss 1.40773618\n",
      "Trained batch 282 batch loss 1.33227074 epoch total loss 1.40746856\n",
      "Trained batch 283 batch loss 1.35401976 epoch total loss 1.40727973\n",
      "Trained batch 284 batch loss 1.50743461 epoch total loss 1.40763247\n",
      "Trained batch 285 batch loss 1.44920063 epoch total loss 1.40777826\n",
      "Trained batch 286 batch loss 1.42352152 epoch total loss 1.40783334\n",
      "Trained batch 287 batch loss 1.32394159 epoch total loss 1.40754104\n",
      "Trained batch 288 batch loss 1.39276361 epoch total loss 1.40748966\n",
      "Trained batch 289 batch loss 1.437428 epoch total loss 1.40759325\n",
      "Trained batch 290 batch loss 1.45689905 epoch total loss 1.40776336\n",
      "Trained batch 291 batch loss 1.42965841 epoch total loss 1.40783858\n",
      "Trained batch 292 batch loss 1.4014163 epoch total loss 1.40781665\n",
      "Trained batch 293 batch loss 1.32542801 epoch total loss 1.40753543\n",
      "Trained batch 294 batch loss 1.40724468 epoch total loss 1.4075346\n",
      "Trained batch 295 batch loss 1.39387846 epoch total loss 1.40748835\n",
      "Trained batch 296 batch loss 1.48290658 epoch total loss 1.4077431\n",
      "Trained batch 297 batch loss 1.40153754 epoch total loss 1.40772223\n",
      "Trained batch 298 batch loss 1.37282312 epoch total loss 1.40760517\n",
      "Trained batch 299 batch loss 1.44839418 epoch total loss 1.40774155\n",
      "Trained batch 300 batch loss 1.34909511 epoch total loss 1.40754604\n",
      "Trained batch 301 batch loss 1.45899713 epoch total loss 1.40771699\n",
      "Trained batch 302 batch loss 1.39981437 epoch total loss 1.40769076\n",
      "Trained batch 303 batch loss 1.45227122 epoch total loss 1.40783787\n",
      "Trained batch 304 batch loss 1.49016213 epoch total loss 1.40810871\n",
      "Trained batch 305 batch loss 1.42806041 epoch total loss 1.40817416\n",
      "Trained batch 306 batch loss 1.31655157 epoch total loss 1.40787482\n",
      "Trained batch 307 batch loss 1.19897771 epoch total loss 1.40719438\n",
      "Trained batch 308 batch loss 1.34331346 epoch total loss 1.40698695\n",
      "Trained batch 309 batch loss 1.43146968 epoch total loss 1.40706623\n",
      "Trained batch 310 batch loss 1.41371787 epoch total loss 1.40708768\n",
      "Trained batch 311 batch loss 1.49805236 epoch total loss 1.4073801\n",
      "Trained batch 312 batch loss 1.31922686 epoch total loss 1.40709758\n",
      "Trained batch 313 batch loss 1.46771085 epoch total loss 1.40729117\n",
      "Trained batch 314 batch loss 1.38249123 epoch total loss 1.40721214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 315 batch loss 1.35498762 epoch total loss 1.40704632\n",
      "Trained batch 316 batch loss 1.37250221 epoch total loss 1.406937\n",
      "Trained batch 317 batch loss 1.3484453 epoch total loss 1.40675259\n",
      "Trained batch 318 batch loss 1.3323108 epoch total loss 1.40651846\n",
      "Trained batch 319 batch loss 1.4208014 epoch total loss 1.40656316\n",
      "Trained batch 320 batch loss 1.37520194 epoch total loss 1.40646529\n",
      "Trained batch 321 batch loss 1.40650415 epoch total loss 1.40646529\n",
      "Trained batch 322 batch loss 1.42675 epoch total loss 1.40652835\n",
      "Trained batch 323 batch loss 1.40021896 epoch total loss 1.4065088\n",
      "Trained batch 324 batch loss 1.37742138 epoch total loss 1.40641892\n",
      "Trained batch 325 batch loss 1.389503 epoch total loss 1.40636694\n",
      "Trained batch 326 batch loss 1.31797409 epoch total loss 1.40609574\n",
      "Trained batch 327 batch loss 1.3158896 epoch total loss 1.40581989\n",
      "Trained batch 328 batch loss 1.34215212 epoch total loss 1.40562582\n",
      "Trained batch 329 batch loss 1.450333 epoch total loss 1.40576172\n",
      "Trained batch 330 batch loss 1.37125266 epoch total loss 1.40565717\n",
      "Trained batch 331 batch loss 1.40260577 epoch total loss 1.40564799\n",
      "Trained batch 332 batch loss 1.41908956 epoch total loss 1.40568841\n",
      "Trained batch 333 batch loss 1.37358713 epoch total loss 1.40559208\n",
      "Trained batch 334 batch loss 1.53232026 epoch total loss 1.40597153\n",
      "Trained batch 335 batch loss 1.31413579 epoch total loss 1.40569735\n",
      "Trained batch 336 batch loss 1.43864751 epoch total loss 1.40579545\n",
      "Trained batch 337 batch loss 1.41993463 epoch total loss 1.40583742\n",
      "Trained batch 338 batch loss 1.30928802 epoch total loss 1.40555179\n",
      "Trained batch 339 batch loss 1.32070446 epoch total loss 1.40530157\n",
      "Trained batch 340 batch loss 1.41701961 epoch total loss 1.40533602\n",
      "Trained batch 341 batch loss 1.31938553 epoch total loss 1.40508401\n",
      "Trained batch 342 batch loss 1.47061968 epoch total loss 1.40527558\n",
      "Trained batch 343 batch loss 1.45496833 epoch total loss 1.40542042\n",
      "Trained batch 344 batch loss 1.38662541 epoch total loss 1.40536582\n",
      "Trained batch 345 batch loss 1.24923182 epoch total loss 1.40491319\n",
      "Trained batch 346 batch loss 1.45583975 epoch total loss 1.40506041\n",
      "Trained batch 347 batch loss 1.42898023 epoch total loss 1.40512943\n",
      "Trained batch 348 batch loss 1.35221148 epoch total loss 1.40497732\n",
      "Trained batch 349 batch loss 1.33379722 epoch total loss 1.40477335\n",
      "Trained batch 350 batch loss 1.47774315 epoch total loss 1.40498185\n",
      "Trained batch 351 batch loss 1.40171087 epoch total loss 1.40497255\n",
      "Trained batch 352 batch loss 1.35991955 epoch total loss 1.40484452\n",
      "Trained batch 353 batch loss 1.28711414 epoch total loss 1.40451097\n",
      "Trained batch 354 batch loss 1.26848793 epoch total loss 1.40412676\n",
      "Trained batch 355 batch loss 1.26461148 epoch total loss 1.40373385\n",
      "Trained batch 356 batch loss 1.30826473 epoch total loss 1.40346563\n",
      "Trained batch 357 batch loss 1.41425526 epoch total loss 1.40349579\n",
      "Trained batch 358 batch loss 1.46404457 epoch total loss 1.40366495\n",
      "Trained batch 359 batch loss 1.32304692 epoch total loss 1.40344048\n",
      "Trained batch 360 batch loss 1.2785368 epoch total loss 1.40309346\n",
      "Trained batch 361 batch loss 1.37328267 epoch total loss 1.40301096\n",
      "Trained batch 362 batch loss 1.45214033 epoch total loss 1.40314662\n",
      "Trained batch 363 batch loss 1.46088219 epoch total loss 1.40330565\n",
      "Trained batch 364 batch loss 1.3627708 epoch total loss 1.40319431\n",
      "Trained batch 365 batch loss 1.29854822 epoch total loss 1.40290761\n",
      "Trained batch 366 batch loss 1.2274183 epoch total loss 1.40242815\n",
      "Trained batch 367 batch loss 1.23432076 epoch total loss 1.40197\n",
      "Trained batch 368 batch loss 1.21651101 epoch total loss 1.40146601\n",
      "Trained batch 369 batch loss 1.31506705 epoch total loss 1.40123188\n",
      "Trained batch 370 batch loss 1.34227121 epoch total loss 1.40107262\n",
      "Trained batch 371 batch loss 1.42494535 epoch total loss 1.40113688\n",
      "Trained batch 372 batch loss 1.43060553 epoch total loss 1.40121603\n",
      "Trained batch 373 batch loss 1.37266922 epoch total loss 1.40113962\n",
      "Trained batch 374 batch loss 1.29116249 epoch total loss 1.40084541\n",
      "Trained batch 375 batch loss 1.20329392 epoch total loss 1.40031874\n",
      "Trained batch 376 batch loss 1.29270017 epoch total loss 1.40003252\n",
      "Trained batch 377 batch loss 1.44434214 epoch total loss 1.40015006\n",
      "Trained batch 378 batch loss 1.46724176 epoch total loss 1.40032744\n",
      "Trained batch 379 batch loss 1.46171212 epoch total loss 1.40048945\n",
      "Trained batch 380 batch loss 1.56866884 epoch total loss 1.40093207\n",
      "Trained batch 381 batch loss 1.48447263 epoch total loss 1.40115142\n",
      "Trained batch 382 batch loss 1.42958403 epoch total loss 1.40122581\n",
      "Trained batch 383 batch loss 1.44028556 epoch total loss 1.40132785\n",
      "Trained batch 384 batch loss 1.43048382 epoch total loss 1.40140378\n",
      "Trained batch 385 batch loss 1.49219966 epoch total loss 1.40163958\n",
      "Trained batch 386 batch loss 1.3948493 epoch total loss 1.40162194\n",
      "Trained batch 387 batch loss 1.52470565 epoch total loss 1.40194\n",
      "Trained batch 388 batch loss 1.47746944 epoch total loss 1.40213466\n",
      "Trained batch 389 batch loss 1.40223134 epoch total loss 1.4021349\n",
      "Trained batch 390 batch loss 1.34533429 epoch total loss 1.40198922\n",
      "Trained batch 391 batch loss 1.31296492 epoch total loss 1.40176165\n",
      "Trained batch 392 batch loss 1.38618016 epoch total loss 1.40172184\n",
      "Trained batch 393 batch loss 1.44905925 epoch total loss 1.40184224\n",
      "Trained batch 394 batch loss 1.46996069 epoch total loss 1.40201521\n",
      "Trained batch 395 batch loss 1.53640914 epoch total loss 1.40235555\n",
      "Trained batch 396 batch loss 1.37666297 epoch total loss 1.40229058\n",
      "Trained batch 397 batch loss 1.39766717 epoch total loss 1.4022789\n",
      "Trained batch 398 batch loss 1.32171679 epoch total loss 1.40207648\n",
      "Trained batch 399 batch loss 1.43202281 epoch total loss 1.40215147\n",
      "Trained batch 400 batch loss 1.44671452 epoch total loss 1.40226293\n",
      "Trained batch 401 batch loss 1.39615536 epoch total loss 1.40224767\n",
      "Trained batch 402 batch loss 1.45919442 epoch total loss 1.40238929\n",
      "Trained batch 403 batch loss 1.3227123 epoch total loss 1.40219152\n",
      "Trained batch 404 batch loss 1.24705791 epoch total loss 1.40180755\n",
      "Trained batch 405 batch loss 1.32309246 epoch total loss 1.40161324\n",
      "Trained batch 406 batch loss 1.25003767 epoch total loss 1.40124\n",
      "Trained batch 407 batch loss 1.18144059 epoch total loss 1.4007\n",
      "Trained batch 408 batch loss 1.35547614 epoch total loss 1.40058911\n",
      "Trained batch 409 batch loss 1.39072716 epoch total loss 1.40056503\n",
      "Trained batch 410 batch loss 1.53821313 epoch total loss 1.40090084\n",
      "Trained batch 411 batch loss 1.61257553 epoch total loss 1.40141571\n",
      "Trained batch 412 batch loss 1.54255414 epoch total loss 1.40175831\n",
      "Trained batch 413 batch loss 1.48614919 epoch total loss 1.40196264\n",
      "Trained batch 414 batch loss 1.55456614 epoch total loss 1.40233123\n",
      "Trained batch 415 batch loss 1.50398481 epoch total loss 1.40257609\n",
      "Trained batch 416 batch loss 1.38529396 epoch total loss 1.4025346\n",
      "Trained batch 417 batch loss 1.38882852 epoch total loss 1.40250182\n",
      "Trained batch 418 batch loss 1.36866713 epoch total loss 1.40242088\n",
      "Trained batch 419 batch loss 1.39216232 epoch total loss 1.40239632\n",
      "Trained batch 420 batch loss 1.34203148 epoch total loss 1.40225267\n",
      "Trained batch 421 batch loss 1.36096549 epoch total loss 1.40215456\n",
      "Trained batch 422 batch loss 1.51367092 epoch total loss 1.40241885\n",
      "Trained batch 423 batch loss 1.37165034 epoch total loss 1.40234601\n",
      "Trained batch 424 batch loss 1.38293028 epoch total loss 1.40230024\n",
      "Trained batch 425 batch loss 1.43309498 epoch total loss 1.40237272\n",
      "Trained batch 426 batch loss 1.39239693 epoch total loss 1.40234935\n",
      "Trained batch 427 batch loss 1.39003909 epoch total loss 1.4023205\n",
      "Trained batch 428 batch loss 1.37649846 epoch total loss 1.40226018\n",
      "Trained batch 429 batch loss 1.32535076 epoch total loss 1.40208101\n",
      "Trained batch 430 batch loss 1.33451438 epoch total loss 1.40192389\n",
      "Trained batch 431 batch loss 1.34306526 epoch total loss 1.4017874\n",
      "Trained batch 432 batch loss 1.31683159 epoch total loss 1.4015907\n",
      "Trained batch 433 batch loss 1.45023704 epoch total loss 1.40170312\n",
      "Trained batch 434 batch loss 1.5239017 epoch total loss 1.40198469\n",
      "Trained batch 435 batch loss 1.39375699 epoch total loss 1.40196574\n",
      "Trained batch 436 batch loss 1.46953726 epoch total loss 1.40212071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 437 batch loss 1.56127977 epoch total loss 1.40248501\n",
      "Trained batch 438 batch loss 1.52280712 epoch total loss 1.40275967\n",
      "Trained batch 439 batch loss 1.49286532 epoch total loss 1.40296495\n",
      "Trained batch 440 batch loss 1.42073119 epoch total loss 1.40300524\n",
      "Trained batch 441 batch loss 1.35328674 epoch total loss 1.40289247\n",
      "Trained batch 442 batch loss 1.38469326 epoch total loss 1.40285134\n",
      "Trained batch 443 batch loss 1.44873023 epoch total loss 1.40295494\n",
      "Trained batch 444 batch loss 1.41873 epoch total loss 1.40299034\n",
      "Trained batch 445 batch loss 1.35280895 epoch total loss 1.40287757\n",
      "Trained batch 446 batch loss 1.26733661 epoch total loss 1.4025737\n",
      "Trained batch 447 batch loss 1.2923975 epoch total loss 1.40232718\n",
      "Trained batch 448 batch loss 1.25762606 epoch total loss 1.40200424\n",
      "Trained batch 449 batch loss 1.37361372 epoch total loss 1.40194094\n",
      "Trained batch 450 batch loss 1.6407578 epoch total loss 1.40247166\n",
      "Trained batch 451 batch loss 1.61425924 epoch total loss 1.40294123\n",
      "Trained batch 452 batch loss 1.5311681 epoch total loss 1.40322495\n",
      "Trained batch 453 batch loss 1.53398323 epoch total loss 1.40351367\n",
      "Trained batch 454 batch loss 1.46643257 epoch total loss 1.40365219\n",
      "Trained batch 455 batch loss 1.64299393 epoch total loss 1.40417826\n",
      "Trained batch 456 batch loss 1.60293818 epoch total loss 1.40461421\n",
      "Trained batch 457 batch loss 1.4035362 epoch total loss 1.40461195\n",
      "Trained batch 458 batch loss 1.43233144 epoch total loss 1.40467238\n",
      "Trained batch 459 batch loss 1.44881141 epoch total loss 1.40476859\n",
      "Trained batch 460 batch loss 1.47485423 epoch total loss 1.40492094\n",
      "Trained batch 461 batch loss 1.36392 epoch total loss 1.40483189\n",
      "Trained batch 462 batch loss 1.4261657 epoch total loss 1.40487802\n",
      "Trained batch 463 batch loss 1.38052249 epoch total loss 1.40482533\n",
      "Trained batch 464 batch loss 1.48474765 epoch total loss 1.40499759\n",
      "Trained batch 465 batch loss 1.42279172 epoch total loss 1.40503585\n",
      "Trained batch 466 batch loss 1.34984589 epoch total loss 1.40491748\n",
      "Trained batch 467 batch loss 1.43873918 epoch total loss 1.40498984\n",
      "Trained batch 468 batch loss 1.37358224 epoch total loss 1.40492272\n",
      "Trained batch 469 batch loss 1.40278614 epoch total loss 1.40491819\n",
      "Trained batch 470 batch loss 1.27589083 epoch total loss 1.40464365\n",
      "Trained batch 471 batch loss 1.2869966 epoch total loss 1.40439379\n",
      "Trained batch 472 batch loss 1.24934411 epoch total loss 1.40406525\n",
      "Trained batch 473 batch loss 1.29069757 epoch total loss 1.40382564\n",
      "Trained batch 474 batch loss 1.34662712 epoch total loss 1.40370488\n",
      "Trained batch 475 batch loss 1.30876946 epoch total loss 1.40350509\n",
      "Trained batch 476 batch loss 1.26092243 epoch total loss 1.40320551\n",
      "Trained batch 477 batch loss 1.3307327 epoch total loss 1.40305364\n",
      "Trained batch 478 batch loss 1.32774282 epoch total loss 1.40289617\n",
      "Trained batch 479 batch loss 1.33914316 epoch total loss 1.40276313\n",
      "Trained batch 480 batch loss 1.30377841 epoch total loss 1.4025569\n",
      "Trained batch 481 batch loss 1.26755679 epoch total loss 1.40227628\n",
      "Trained batch 482 batch loss 1.26429439 epoch total loss 1.40198994\n",
      "Trained batch 483 batch loss 1.37404656 epoch total loss 1.401932\n",
      "Trained batch 484 batch loss 1.24740577 epoch total loss 1.40161276\n",
      "Trained batch 485 batch loss 1.26631272 epoch total loss 1.40133369\n",
      "Trained batch 486 batch loss 1.29706454 epoch total loss 1.40111911\n",
      "Trained batch 487 batch loss 1.33695018 epoch total loss 1.40098739\n",
      "Trained batch 488 batch loss 1.22362375 epoch total loss 1.40062404\n",
      "Trained batch 489 batch loss 1.22332537 epoch total loss 1.4002614\n",
      "Trained batch 490 batch loss 1.16520524 epoch total loss 1.39978182\n",
      "Trained batch 491 batch loss 1.27442634 epoch total loss 1.39952648\n",
      "Trained batch 492 batch loss 1.36100721 epoch total loss 1.39944816\n",
      "Trained batch 493 batch loss 1.32762265 epoch total loss 1.39930248\n",
      "Trained batch 494 batch loss 1.35771608 epoch total loss 1.39921832\n",
      "Trained batch 495 batch loss 1.3956356 epoch total loss 1.39921105\n",
      "Trained batch 496 batch loss 1.39162445 epoch total loss 1.39919579\n",
      "Trained batch 497 batch loss 1.46181512 epoch total loss 1.39932168\n",
      "Trained batch 498 batch loss 1.3318783 epoch total loss 1.39918625\n",
      "Trained batch 499 batch loss 1.38444054 epoch total loss 1.39915669\n",
      "Trained batch 500 batch loss 1.36095905 epoch total loss 1.39908028\n",
      "Trained batch 501 batch loss 1.39562321 epoch total loss 1.39907348\n",
      "Trained batch 502 batch loss 1.52823234 epoch total loss 1.39933074\n",
      "Trained batch 503 batch loss 1.46619332 epoch total loss 1.39946365\n",
      "Trained batch 504 batch loss 1.40997732 epoch total loss 1.39948452\n",
      "Trained batch 505 batch loss 1.40048647 epoch total loss 1.39948654\n",
      "Trained batch 506 batch loss 1.46070147 epoch total loss 1.39960754\n",
      "Trained batch 507 batch loss 1.38932014 epoch total loss 1.39958727\n",
      "Trained batch 508 batch loss 1.42839503 epoch total loss 1.39964402\n",
      "Trained batch 509 batch loss 1.44161427 epoch total loss 1.39972639\n",
      "Trained batch 510 batch loss 1.36636329 epoch total loss 1.39966094\n",
      "Trained batch 511 batch loss 1.3681953 epoch total loss 1.39959943\n",
      "Trained batch 512 batch loss 1.4453212 epoch total loss 1.39968872\n",
      "Trained batch 513 batch loss 1.41846967 epoch total loss 1.39972532\n",
      "Trained batch 514 batch loss 1.46573198 epoch total loss 1.39985383\n",
      "Trained batch 515 batch loss 1.31705093 epoch total loss 1.39969301\n",
      "Trained batch 516 batch loss 1.46534717 epoch total loss 1.39982021\n",
      "Trained batch 517 batch loss 1.35192466 epoch total loss 1.39972758\n",
      "Trained batch 518 batch loss 1.3402822 epoch total loss 1.39961278\n",
      "Trained batch 519 batch loss 1.3549031 epoch total loss 1.39952672\n",
      "Trained batch 520 batch loss 1.28324068 epoch total loss 1.3993032\n",
      "Trained batch 521 batch loss 1.37697875 epoch total loss 1.39926028\n",
      "Trained batch 522 batch loss 1.29001462 epoch total loss 1.39905095\n",
      "Trained batch 523 batch loss 1.51254666 epoch total loss 1.39926803\n",
      "Trained batch 524 batch loss 1.43197381 epoch total loss 1.39933038\n",
      "Trained batch 525 batch loss 1.47639275 epoch total loss 1.39947724\n",
      "Trained batch 526 batch loss 1.42208588 epoch total loss 1.39952016\n",
      "Trained batch 527 batch loss 1.4704752 epoch total loss 1.39965475\n",
      "Trained batch 528 batch loss 1.32451928 epoch total loss 1.39951241\n",
      "Trained batch 529 batch loss 1.33594084 epoch total loss 1.39939225\n",
      "Trained batch 530 batch loss 1.47666728 epoch total loss 1.39953804\n",
      "Trained batch 531 batch loss 1.55775714 epoch total loss 1.39983606\n",
      "Trained batch 532 batch loss 1.44315016 epoch total loss 1.39991748\n",
      "Trained batch 533 batch loss 1.37629366 epoch total loss 1.39987314\n",
      "Trained batch 534 batch loss 1.45202422 epoch total loss 1.39997077\n",
      "Trained batch 535 batch loss 1.45180607 epoch total loss 1.40006769\n",
      "Trained batch 536 batch loss 1.31816506 epoch total loss 1.39991486\n",
      "Trained batch 537 batch loss 1.52867234 epoch total loss 1.40015471\n",
      "Trained batch 538 batch loss 1.48789716 epoch total loss 1.40031779\n",
      "Trained batch 539 batch loss 1.54759884 epoch total loss 1.40059102\n",
      "Trained batch 540 batch loss 1.37597215 epoch total loss 1.40054548\n",
      "Trained batch 541 batch loss 1.21702933 epoch total loss 1.40020633\n",
      "Trained batch 542 batch loss 1.20525837 epoch total loss 1.39984655\n",
      "Trained batch 543 batch loss 1.38101888 epoch total loss 1.39981198\n",
      "Trained batch 544 batch loss 1.39800453 epoch total loss 1.39980865\n",
      "Trained batch 545 batch loss 1.45872962 epoch total loss 1.39991677\n",
      "Trained batch 546 batch loss 1.43657088 epoch total loss 1.399984\n",
      "Trained batch 547 batch loss 1.36928225 epoch total loss 1.39992774\n",
      "Trained batch 548 batch loss 1.39376557 epoch total loss 1.39991653\n",
      "Trained batch 549 batch loss 1.45489287 epoch total loss 1.40001667\n",
      "Trained batch 550 batch loss 1.45109892 epoch total loss 1.40010953\n",
      "Trained batch 551 batch loss 1.4187876 epoch total loss 1.40014338\n",
      "Trained batch 552 batch loss 1.36426735 epoch total loss 1.40007842\n",
      "Trained batch 553 batch loss 1.32999122 epoch total loss 1.3999517\n",
      "Trained batch 554 batch loss 1.41447985 epoch total loss 1.39997792\n",
      "Trained batch 555 batch loss 1.36637306 epoch total loss 1.39991736\n",
      "Trained batch 556 batch loss 1.32280779 epoch total loss 1.39977872\n",
      "Trained batch 557 batch loss 1.34256792 epoch total loss 1.39967608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 558 batch loss 1.28467858 epoch total loss 1.39947\n",
      "Trained batch 559 batch loss 1.40451276 epoch total loss 1.39947903\n",
      "Trained batch 560 batch loss 1.37502575 epoch total loss 1.39943528\n",
      "Trained batch 561 batch loss 1.44082546 epoch total loss 1.39950907\n",
      "Trained batch 562 batch loss 1.37395561 epoch total loss 1.39946353\n",
      "Trained batch 563 batch loss 1.49802291 epoch total loss 1.39963865\n",
      "Trained batch 564 batch loss 1.38056302 epoch total loss 1.3996048\n",
      "Trained batch 565 batch loss 1.38556516 epoch total loss 1.39958\n",
      "Trained batch 566 batch loss 1.41362953 epoch total loss 1.3996048\n",
      "Trained batch 567 batch loss 1.28867459 epoch total loss 1.39940917\n",
      "Trained batch 568 batch loss 1.33889663 epoch total loss 1.3993026\n",
      "Trained batch 569 batch loss 1.30944324 epoch total loss 1.39914477\n",
      "Trained batch 570 batch loss 1.35027337 epoch total loss 1.39905894\n",
      "Trained batch 571 batch loss 1.21451461 epoch total loss 1.39873588\n",
      "Trained batch 572 batch loss 1.3095454 epoch total loss 1.39858\n",
      "Trained batch 573 batch loss 1.31623769 epoch total loss 1.39843619\n",
      "Trained batch 574 batch loss 1.34126616 epoch total loss 1.39833665\n",
      "Trained batch 575 batch loss 1.28692746 epoch total loss 1.39814281\n",
      "Trained batch 576 batch loss 1.3213079 epoch total loss 1.39800942\n",
      "Trained batch 577 batch loss 1.38570726 epoch total loss 1.39798808\n",
      "Trained batch 578 batch loss 1.44888973 epoch total loss 1.39807618\n",
      "Trained batch 579 batch loss 1.47137976 epoch total loss 1.39820278\n",
      "Trained batch 580 batch loss 1.46302533 epoch total loss 1.39831448\n",
      "Trained batch 581 batch loss 1.43567801 epoch total loss 1.39837873\n",
      "Trained batch 582 batch loss 1.39025044 epoch total loss 1.39836478\n",
      "Trained batch 583 batch loss 1.35843611 epoch total loss 1.39829636\n",
      "Trained batch 584 batch loss 1.37706971 epoch total loss 1.39826\n",
      "Trained batch 585 batch loss 1.3303225 epoch total loss 1.39814389\n",
      "Trained batch 586 batch loss 1.19278204 epoch total loss 1.39779353\n",
      "Trained batch 587 batch loss 1.1642462 epoch total loss 1.39739561\n",
      "Trained batch 588 batch loss 1.42303705 epoch total loss 1.39743924\n",
      "Trained batch 589 batch loss 1.41303098 epoch total loss 1.39746571\n",
      "Trained batch 590 batch loss 1.47335291 epoch total loss 1.39759433\n",
      "Trained batch 591 batch loss 1.39081573 epoch total loss 1.39758277\n",
      "Trained batch 592 batch loss 1.40850663 epoch total loss 1.39760125\n",
      "Trained batch 593 batch loss 1.47030902 epoch total loss 1.39772391\n",
      "Trained batch 594 batch loss 1.3703692 epoch total loss 1.3976779\n",
      "Trained batch 595 batch loss 1.40865338 epoch total loss 1.39769626\n",
      "Trained batch 596 batch loss 1.54812765 epoch total loss 1.39794874\n",
      "Trained batch 597 batch loss 1.48928893 epoch total loss 1.39810181\n",
      "Trained batch 598 batch loss 1.30911064 epoch total loss 1.39795291\n",
      "Trained batch 599 batch loss 1.28907442 epoch total loss 1.39777112\n",
      "Trained batch 600 batch loss 1.24259543 epoch total loss 1.39751256\n",
      "Trained batch 601 batch loss 1.24734926 epoch total loss 1.39726269\n",
      "Trained batch 602 batch loss 1.32831502 epoch total loss 1.39714813\n",
      "Trained batch 603 batch loss 1.4643923 epoch total loss 1.39725971\n",
      "Trained batch 604 batch loss 1.40021181 epoch total loss 1.3972646\n",
      "Trained batch 605 batch loss 1.43419695 epoch total loss 1.39732563\n",
      "Trained batch 606 batch loss 1.47423482 epoch total loss 1.39745259\n",
      "Trained batch 607 batch loss 1.39181852 epoch total loss 1.39744329\n",
      "Trained batch 608 batch loss 1.21714818 epoch total loss 1.39714682\n",
      "Trained batch 609 batch loss 1.3708775 epoch total loss 1.39710367\n",
      "Trained batch 610 batch loss 1.26744556 epoch total loss 1.39689112\n",
      "Trained batch 611 batch loss 1.35848808 epoch total loss 1.39682817\n",
      "Trained batch 612 batch loss 1.29941487 epoch total loss 1.39666903\n",
      "Trained batch 613 batch loss 1.32127094 epoch total loss 1.39654613\n",
      "Trained batch 614 batch loss 1.37593186 epoch total loss 1.39651251\n",
      "Trained batch 615 batch loss 1.22974145 epoch total loss 1.39624131\n",
      "Trained batch 616 batch loss 1.14199913 epoch total loss 1.3958286\n",
      "Trained batch 617 batch loss 1.01768148 epoch total loss 1.39521575\n",
      "Trained batch 618 batch loss 1.18346024 epoch total loss 1.39487314\n",
      "Trained batch 619 batch loss 1.39252543 epoch total loss 1.39486933\n",
      "Trained batch 620 batch loss 1.38775122 epoch total loss 1.39485788\n",
      "Trained batch 621 batch loss 1.33716559 epoch total loss 1.39476502\n",
      "Trained batch 622 batch loss 1.40205812 epoch total loss 1.3947767\n",
      "Trained batch 623 batch loss 1.44903386 epoch total loss 1.39486372\n",
      "Trained batch 624 batch loss 1.33340025 epoch total loss 1.39476526\n",
      "Trained batch 625 batch loss 1.24354947 epoch total loss 1.39452326\n",
      "Trained batch 626 batch loss 1.27895916 epoch total loss 1.39433861\n",
      "Trained batch 627 batch loss 1.37506831 epoch total loss 1.39430785\n",
      "Trained batch 628 batch loss 1.32162201 epoch total loss 1.3941921\n",
      "Trained batch 629 batch loss 1.27716565 epoch total loss 1.39400601\n",
      "Trained batch 630 batch loss 1.32081866 epoch total loss 1.39388978\n",
      "Trained batch 631 batch loss 1.42283869 epoch total loss 1.39393568\n",
      "Trained batch 632 batch loss 1.54193294 epoch total loss 1.39416993\n",
      "Trained batch 633 batch loss 1.74635696 epoch total loss 1.39472616\n",
      "Trained batch 634 batch loss 1.57222152 epoch total loss 1.39500618\n",
      "Trained batch 635 batch loss 1.51471364 epoch total loss 1.39519465\n",
      "Trained batch 636 batch loss 1.45619404 epoch total loss 1.39529049\n",
      "Trained batch 637 batch loss 1.48295665 epoch total loss 1.39542818\n",
      "Trained batch 638 batch loss 1.31205654 epoch total loss 1.39529753\n",
      "Trained batch 639 batch loss 1.31445956 epoch total loss 1.39517105\n",
      "Trained batch 640 batch loss 1.39390182 epoch total loss 1.39516902\n",
      "Trained batch 641 batch loss 1.51242065 epoch total loss 1.39535201\n",
      "Trained batch 642 batch loss 1.36811686 epoch total loss 1.39530957\n",
      "Trained batch 643 batch loss 1.35958886 epoch total loss 1.39525414\n",
      "Trained batch 644 batch loss 1.26317239 epoch total loss 1.39504898\n",
      "Trained batch 645 batch loss 1.28980589 epoch total loss 1.39488578\n",
      "Trained batch 646 batch loss 1.24096036 epoch total loss 1.3946476\n",
      "Trained batch 647 batch loss 1.35496855 epoch total loss 1.39458621\n",
      "Trained batch 648 batch loss 1.33926749 epoch total loss 1.39450097\n",
      "Trained batch 649 batch loss 1.32172108 epoch total loss 1.39438879\n",
      "Trained batch 650 batch loss 1.27045941 epoch total loss 1.39419806\n",
      "Trained batch 651 batch loss 1.2072196 epoch total loss 1.39391088\n",
      "Trained batch 652 batch loss 1.23300755 epoch total loss 1.39366412\n",
      "Trained batch 653 batch loss 1.17886519 epoch total loss 1.39333522\n",
      "Trained batch 654 batch loss 1.32125914 epoch total loss 1.39322507\n",
      "Trained batch 655 batch loss 1.34721863 epoch total loss 1.39315486\n",
      "Trained batch 656 batch loss 1.32094312 epoch total loss 1.39304471\n",
      "Trained batch 657 batch loss 1.29383278 epoch total loss 1.39289367\n",
      "Trained batch 658 batch loss 1.3221159 epoch total loss 1.39278615\n",
      "Trained batch 659 batch loss 1.40110815 epoch total loss 1.39279878\n",
      "Trained batch 660 batch loss 1.27379394 epoch total loss 1.39261854\n",
      "Trained batch 661 batch loss 1.46100628 epoch total loss 1.39272201\n",
      "Trained batch 662 batch loss 1.27668953 epoch total loss 1.39254665\n",
      "Trained batch 663 batch loss 1.27620256 epoch total loss 1.39237118\n",
      "Trained batch 664 batch loss 1.23651171 epoch total loss 1.39213645\n",
      "Trained batch 665 batch loss 1.34460449 epoch total loss 1.39206493\n",
      "Trained batch 666 batch loss 1.39834905 epoch total loss 1.39207447\n",
      "Trained batch 667 batch loss 1.58656025 epoch total loss 1.39236605\n",
      "Trained batch 668 batch loss 1.63957691 epoch total loss 1.39273608\n",
      "Trained batch 669 batch loss 1.4670186 epoch total loss 1.39284718\n",
      "Trained batch 670 batch loss 1.28303349 epoch total loss 1.39268327\n",
      "Trained batch 671 batch loss 1.22162151 epoch total loss 1.39242828\n",
      "Trained batch 672 batch loss 1.3221457 epoch total loss 1.39232373\n",
      "Trained batch 673 batch loss 1.39076519 epoch total loss 1.39232135\n",
      "Trained batch 674 batch loss 1.33184433 epoch total loss 1.3922317\n",
      "Trained batch 675 batch loss 1.26947069 epoch total loss 1.39204979\n",
      "Trained batch 676 batch loss 1.40572977 epoch total loss 1.39206994\n",
      "Trained batch 677 batch loss 1.4573164 epoch total loss 1.39216638\n",
      "Trained batch 678 batch loss 1.4689523 epoch total loss 1.39227962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 679 batch loss 1.42856061 epoch total loss 1.39233303\n",
      "Trained batch 680 batch loss 1.38049173 epoch total loss 1.39231563\n",
      "Trained batch 681 batch loss 1.39332986 epoch total loss 1.39231718\n",
      "Trained batch 682 batch loss 1.37631989 epoch total loss 1.39229369\n",
      "Trained batch 683 batch loss 1.45585942 epoch total loss 1.39238679\n",
      "Trained batch 684 batch loss 1.39416969 epoch total loss 1.39238942\n",
      "Trained batch 685 batch loss 1.37527776 epoch total loss 1.3923645\n",
      "Trained batch 686 batch loss 1.40211368 epoch total loss 1.39237869\n",
      "Trained batch 687 batch loss 1.3168062 epoch total loss 1.39226866\n",
      "Trained batch 688 batch loss 1.2927475 epoch total loss 1.39212406\n",
      "Trained batch 689 batch loss 1.2766273 epoch total loss 1.39195633\n",
      "Trained batch 690 batch loss 1.43647826 epoch total loss 1.39202082\n",
      "Trained batch 691 batch loss 1.35919642 epoch total loss 1.39197338\n",
      "Trained batch 692 batch loss 1.29595053 epoch total loss 1.39183462\n",
      "Trained batch 693 batch loss 1.25076091 epoch total loss 1.39163101\n",
      "Trained batch 694 batch loss 1.28257692 epoch total loss 1.39147389\n",
      "Trained batch 695 batch loss 1.36323881 epoch total loss 1.39143324\n",
      "Trained batch 696 batch loss 1.41860771 epoch total loss 1.39147222\n",
      "Trained batch 697 batch loss 1.44289315 epoch total loss 1.39154601\n",
      "Trained batch 698 batch loss 1.31200671 epoch total loss 1.39143205\n",
      "Trained batch 699 batch loss 1.32624388 epoch total loss 1.39133871\n",
      "Trained batch 700 batch loss 1.35289013 epoch total loss 1.39128387\n",
      "Trained batch 701 batch loss 1.32805789 epoch total loss 1.39119363\n",
      "Trained batch 702 batch loss 1.30041182 epoch total loss 1.39106429\n",
      "Trained batch 703 batch loss 1.34516025 epoch total loss 1.39099908\n",
      "Trained batch 704 batch loss 1.28981662 epoch total loss 1.39085531\n",
      "Trained batch 705 batch loss 1.37706447 epoch total loss 1.39083576\n",
      "Trained batch 706 batch loss 1.34332299 epoch total loss 1.39076841\n",
      "Trained batch 707 batch loss 1.31990361 epoch total loss 1.39066815\n",
      "Trained batch 708 batch loss 1.38073039 epoch total loss 1.39065409\n",
      "Trained batch 709 batch loss 1.27528894 epoch total loss 1.39049137\n",
      "Trained batch 710 batch loss 1.3409375 epoch total loss 1.39042163\n",
      "Trained batch 711 batch loss 1.20197988 epoch total loss 1.39015651\n",
      "Trained batch 712 batch loss 1.32552123 epoch total loss 1.39006579\n",
      "Trained batch 713 batch loss 1.3329711 epoch total loss 1.38998568\n",
      "Trained batch 714 batch loss 1.36313415 epoch total loss 1.38994801\n",
      "Trained batch 715 batch loss 1.37345695 epoch total loss 1.389925\n",
      "Trained batch 716 batch loss 1.31144989 epoch total loss 1.38981545\n",
      "Trained batch 717 batch loss 1.36416733 epoch total loss 1.38977969\n",
      "Trained batch 718 batch loss 1.36694312 epoch total loss 1.38974786\n",
      "Trained batch 719 batch loss 1.27787948 epoch total loss 1.38959229\n",
      "Trained batch 720 batch loss 1.21940303 epoch total loss 1.38935602\n",
      "Trained batch 721 batch loss 1.25386691 epoch total loss 1.38916802\n",
      "Trained batch 722 batch loss 1.22957385 epoch total loss 1.38894701\n",
      "Trained batch 723 batch loss 1.31133246 epoch total loss 1.3888396\n",
      "Trained batch 724 batch loss 1.404984 epoch total loss 1.38886189\n",
      "Trained batch 725 batch loss 1.3187083 epoch total loss 1.3887651\n",
      "Trained batch 726 batch loss 1.34458184 epoch total loss 1.3887043\n",
      "Trained batch 727 batch loss 1.38334727 epoch total loss 1.38869703\n",
      "Trained batch 728 batch loss 1.37902796 epoch total loss 1.38868368\n",
      "Trained batch 729 batch loss 1.40320849 epoch total loss 1.38870358\n",
      "Trained batch 730 batch loss 1.3700372 epoch total loss 1.38867807\n",
      "Trained batch 731 batch loss 1.33267128 epoch total loss 1.38860142\n",
      "Trained batch 732 batch loss 1.33782458 epoch total loss 1.38853204\n",
      "Trained batch 733 batch loss 1.24009478 epoch total loss 1.38832951\n",
      "Trained batch 734 batch loss 1.28595567 epoch total loss 1.38819\n",
      "Trained batch 735 batch loss 1.3609302 epoch total loss 1.38815296\n",
      "Trained batch 736 batch loss 1.2731607 epoch total loss 1.38799667\n",
      "Trained batch 737 batch loss 1.31803119 epoch total loss 1.38790178\n",
      "Trained batch 738 batch loss 1.38319945 epoch total loss 1.38789546\n",
      "Trained batch 739 batch loss 1.41140962 epoch total loss 1.38792717\n",
      "Trained batch 740 batch loss 1.31626046 epoch total loss 1.38783038\n",
      "Trained batch 741 batch loss 1.44261372 epoch total loss 1.38790441\n",
      "Trained batch 742 batch loss 1.47637022 epoch total loss 1.3880235\n",
      "Trained batch 743 batch loss 1.25309432 epoch total loss 1.38784182\n",
      "Trained batch 744 batch loss 1.35328984 epoch total loss 1.38779545\n",
      "Trained batch 745 batch loss 1.38334894 epoch total loss 1.38778937\n",
      "Trained batch 746 batch loss 1.29369164 epoch total loss 1.38766325\n",
      "Trained batch 747 batch loss 1.31806827 epoch total loss 1.38757014\n",
      "Trained batch 748 batch loss 1.4002105 epoch total loss 1.38758707\n",
      "Trained batch 749 batch loss 1.43301749 epoch total loss 1.38764775\n",
      "Trained batch 750 batch loss 1.43654 epoch total loss 1.38771284\n",
      "Trained batch 751 batch loss 1.44528985 epoch total loss 1.38778961\n",
      "Trained batch 752 batch loss 1.41726661 epoch total loss 1.38782871\n",
      "Trained batch 753 batch loss 1.40880442 epoch total loss 1.3878566\n",
      "Trained batch 754 batch loss 1.34870934 epoch total loss 1.38780475\n",
      "Trained batch 755 batch loss 1.40818954 epoch total loss 1.38783181\n",
      "Trained batch 756 batch loss 1.37630022 epoch total loss 1.38781655\n",
      "Trained batch 757 batch loss 1.40567017 epoch total loss 1.38784015\n",
      "Trained batch 758 batch loss 1.25298285 epoch total loss 1.38766217\n",
      "Trained batch 759 batch loss 1.46436787 epoch total loss 1.38776314\n",
      "Trained batch 760 batch loss 1.46423364 epoch total loss 1.38786376\n",
      "Trained batch 761 batch loss 1.39622545 epoch total loss 1.38787484\n",
      "Trained batch 762 batch loss 1.48029387 epoch total loss 1.3879962\n",
      "Trained batch 763 batch loss 1.28902125 epoch total loss 1.3878665\n",
      "Trained batch 764 batch loss 1.34811509 epoch total loss 1.38781452\n",
      "Trained batch 765 batch loss 1.33080637 epoch total loss 1.38774\n",
      "Trained batch 766 batch loss 1.23145938 epoch total loss 1.38753593\n",
      "Trained batch 767 batch loss 1.22319555 epoch total loss 1.38732159\n",
      "Trained batch 768 batch loss 1.14506 epoch total loss 1.38700616\n",
      "Trained batch 769 batch loss 1.2530005 epoch total loss 1.38683188\n",
      "Trained batch 770 batch loss 1.27363062 epoch total loss 1.38668501\n",
      "Trained batch 771 batch loss 1.12207031 epoch total loss 1.38634181\n",
      "Trained batch 772 batch loss 1.28118455 epoch total loss 1.38620543\n",
      "Trained batch 773 batch loss 1.35642564 epoch total loss 1.38616693\n",
      "Trained batch 774 batch loss 1.36608672 epoch total loss 1.38614106\n",
      "Trained batch 775 batch loss 1.32506025 epoch total loss 1.38606226\n",
      "Trained batch 776 batch loss 1.27032781 epoch total loss 1.38591313\n",
      "Trained batch 777 batch loss 1.22993 epoch total loss 1.3857125\n",
      "Trained batch 778 batch loss 1.40952766 epoch total loss 1.38574314\n",
      "Trained batch 779 batch loss 1.40003371 epoch total loss 1.3857615\n",
      "Trained batch 780 batch loss 1.42235637 epoch total loss 1.38580835\n",
      "Trained batch 781 batch loss 1.31854033 epoch total loss 1.38572216\n",
      "Trained batch 782 batch loss 1.41672039 epoch total loss 1.38576186\n",
      "Trained batch 783 batch loss 1.36664331 epoch total loss 1.38573754\n",
      "Trained batch 784 batch loss 1.31175411 epoch total loss 1.38564312\n",
      "Trained batch 785 batch loss 1.27118742 epoch total loss 1.38549745\n",
      "Trained batch 786 batch loss 1.35466623 epoch total loss 1.38545811\n",
      "Trained batch 787 batch loss 1.37370038 epoch total loss 1.38544309\n",
      "Trained batch 788 batch loss 1.3533597 epoch total loss 1.38540244\n",
      "Trained batch 789 batch loss 1.40934098 epoch total loss 1.38543272\n",
      "Trained batch 790 batch loss 1.33334541 epoch total loss 1.38536692\n",
      "Trained batch 791 batch loss 1.36511278 epoch total loss 1.38534129\n",
      "Trained batch 792 batch loss 1.41971993 epoch total loss 1.38538456\n",
      "Trained batch 793 batch loss 1.45194018 epoch total loss 1.38546848\n",
      "Trained batch 794 batch loss 1.40910339 epoch total loss 1.38549817\n",
      "Trained batch 795 batch loss 1.44043314 epoch total loss 1.38556731\n",
      "Trained batch 796 batch loss 1.40728331 epoch total loss 1.38559449\n",
      "Trained batch 797 batch loss 1.24592376 epoch total loss 1.38541937\n",
      "Trained batch 798 batch loss 1.35258341 epoch total loss 1.38537812\n",
      "Trained batch 799 batch loss 1.26597905 epoch total loss 1.38522875\n",
      "Trained batch 800 batch loss 1.25889444 epoch total loss 1.3850708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 801 batch loss 1.26526356 epoch total loss 1.38492119\n",
      "Trained batch 802 batch loss 1.36728454 epoch total loss 1.38489926\n",
      "Trained batch 803 batch loss 1.45675588 epoch total loss 1.38498878\n",
      "Trained batch 804 batch loss 1.30479336 epoch total loss 1.38488901\n",
      "Trained batch 805 batch loss 1.20110536 epoch total loss 1.38466072\n",
      "Trained batch 806 batch loss 1.21767569 epoch total loss 1.38445354\n",
      "Trained batch 807 batch loss 1.20439601 epoch total loss 1.38423026\n",
      "Trained batch 808 batch loss 1.28555167 epoch total loss 1.38410819\n",
      "Trained batch 809 batch loss 1.27535677 epoch total loss 1.38397372\n",
      "Trained batch 810 batch loss 1.3375057 epoch total loss 1.38391638\n",
      "Trained batch 811 batch loss 1.3086381 epoch total loss 1.38382351\n",
      "Trained batch 812 batch loss 1.30588818 epoch total loss 1.38372755\n",
      "Trained batch 813 batch loss 1.31661415 epoch total loss 1.38364506\n",
      "Trained batch 814 batch loss 1.43463528 epoch total loss 1.38370776\n",
      "Trained batch 815 batch loss 1.44502711 epoch total loss 1.3837831\n",
      "Trained batch 816 batch loss 1.31772673 epoch total loss 1.38370216\n",
      "Trained batch 817 batch loss 1.24121892 epoch total loss 1.38352776\n",
      "Trained batch 818 batch loss 1.30108976 epoch total loss 1.38342702\n",
      "Trained batch 819 batch loss 1.32588315 epoch total loss 1.38335681\n",
      "Trained batch 820 batch loss 1.44327784 epoch total loss 1.38342988\n",
      "Trained batch 821 batch loss 1.48684716 epoch total loss 1.38355577\n",
      "Trained batch 822 batch loss 1.34992266 epoch total loss 1.38351488\n",
      "Trained batch 823 batch loss 1.28630483 epoch total loss 1.38339674\n",
      "Trained batch 824 batch loss 1.31672621 epoch total loss 1.38331592\n",
      "Trained batch 825 batch loss 1.38180435 epoch total loss 1.38331413\n",
      "Trained batch 826 batch loss 1.26037908 epoch total loss 1.38316524\n",
      "Trained batch 827 batch loss 1.32286692 epoch total loss 1.3830924\n",
      "Trained batch 828 batch loss 1.29876244 epoch total loss 1.38299048\n",
      "Trained batch 829 batch loss 1.29022181 epoch total loss 1.38287842\n",
      "Trained batch 830 batch loss 1.47498047 epoch total loss 1.38298941\n",
      "Trained batch 831 batch loss 1.42647505 epoch total loss 1.38304186\n",
      "Trained batch 832 batch loss 1.32494521 epoch total loss 1.382972\n",
      "Trained batch 833 batch loss 1.26606202 epoch total loss 1.38283169\n",
      "Trained batch 834 batch loss 1.3646208 epoch total loss 1.38280988\n",
      "Trained batch 835 batch loss 1.3105551 epoch total loss 1.38272333\n",
      "Trained batch 836 batch loss 1.29992437 epoch total loss 1.38262427\n",
      "Trained batch 837 batch loss 1.28420687 epoch total loss 1.38250661\n",
      "Trained batch 838 batch loss 1.37490499 epoch total loss 1.38249755\n",
      "Trained batch 839 batch loss 1.353688 epoch total loss 1.38246322\n",
      "Trained batch 840 batch loss 1.40122747 epoch total loss 1.38248551\n",
      "Trained batch 841 batch loss 1.37395895 epoch total loss 1.38247538\n",
      "Trained batch 842 batch loss 1.35840571 epoch total loss 1.38244677\n",
      "Trained batch 843 batch loss 1.42665195 epoch total loss 1.3824991\n",
      "Trained batch 844 batch loss 1.37569571 epoch total loss 1.38249111\n",
      "Trained batch 845 batch loss 1.42192268 epoch total loss 1.38253772\n",
      "Trained batch 846 batch loss 1.4546206 epoch total loss 1.38262296\n",
      "Trained batch 847 batch loss 1.39452302 epoch total loss 1.3826369\n",
      "Trained batch 848 batch loss 1.303 epoch total loss 1.38254297\n",
      "Trained batch 849 batch loss 1.37503433 epoch total loss 1.38253415\n",
      "Trained batch 850 batch loss 1.27416575 epoch total loss 1.38240671\n",
      "Trained batch 851 batch loss 1.39461195 epoch total loss 1.38242102\n",
      "Trained batch 852 batch loss 1.37943912 epoch total loss 1.38241744\n",
      "Trained batch 853 batch loss 1.34487343 epoch total loss 1.38237345\n",
      "Trained batch 854 batch loss 1.34276831 epoch total loss 1.38232708\n",
      "Trained batch 855 batch loss 1.43983805 epoch total loss 1.38239431\n",
      "Trained batch 856 batch loss 1.50200176 epoch total loss 1.38253403\n",
      "Trained batch 857 batch loss 1.39711344 epoch total loss 1.38255095\n",
      "Trained batch 858 batch loss 1.39738786 epoch total loss 1.38256824\n",
      "Trained batch 859 batch loss 1.32066703 epoch total loss 1.38249612\n",
      "Trained batch 860 batch loss 1.23237157 epoch total loss 1.38232172\n",
      "Trained batch 861 batch loss 1.35619032 epoch total loss 1.38229132\n",
      "Trained batch 862 batch loss 1.31829596 epoch total loss 1.38221705\n",
      "Trained batch 863 batch loss 1.34082294 epoch total loss 1.38216901\n",
      "Trained batch 864 batch loss 1.31331897 epoch total loss 1.38208938\n",
      "Trained batch 865 batch loss 1.28117263 epoch total loss 1.38197267\n",
      "Trained batch 866 batch loss 1.41211808 epoch total loss 1.38200748\n",
      "Trained batch 867 batch loss 1.35081553 epoch total loss 1.38197148\n",
      "Trained batch 868 batch loss 1.55665684 epoch total loss 1.3821727\n",
      "Trained batch 869 batch loss 1.6157316 epoch total loss 1.38244152\n",
      "Trained batch 870 batch loss 1.41512036 epoch total loss 1.38247907\n",
      "Trained batch 871 batch loss 1.29667664 epoch total loss 1.3823806\n",
      "Trained batch 872 batch loss 1.25278 epoch total loss 1.38223195\n",
      "Trained batch 873 batch loss 1.28231823 epoch total loss 1.38211751\n",
      "Trained batch 874 batch loss 1.36887 epoch total loss 1.38210237\n",
      "Trained batch 875 batch loss 1.34928524 epoch total loss 1.38206482\n",
      "Trained batch 876 batch loss 1.3119638 epoch total loss 1.38198495\n",
      "Trained batch 877 batch loss 1.40069926 epoch total loss 1.38200629\n",
      "Trained batch 878 batch loss 1.48115504 epoch total loss 1.3821193\n",
      "Trained batch 879 batch loss 1.41591048 epoch total loss 1.38215768\n",
      "Trained batch 880 batch loss 1.43047392 epoch total loss 1.38221252\n",
      "Trained batch 881 batch loss 1.41653848 epoch total loss 1.3822515\n",
      "Trained batch 882 batch loss 1.273103 epoch total loss 1.38212764\n",
      "Trained batch 883 batch loss 1.35181427 epoch total loss 1.38209331\n",
      "Trained batch 884 batch loss 1.38687944 epoch total loss 1.38209867\n",
      "Trained batch 885 batch loss 1.3457092 epoch total loss 1.38205755\n",
      "Trained batch 886 batch loss 1.31072283 epoch total loss 1.38197696\n",
      "Trained batch 887 batch loss 1.30328512 epoch total loss 1.38188839\n",
      "Trained batch 888 batch loss 1.27888966 epoch total loss 1.3817724\n",
      "Trained batch 889 batch loss 1.21159697 epoch total loss 1.38158095\n",
      "Trained batch 890 batch loss 1.20469546 epoch total loss 1.38138223\n",
      "Trained batch 891 batch loss 1.26665473 epoch total loss 1.38125336\n",
      "Trained batch 892 batch loss 1.29083669 epoch total loss 1.38115215\n",
      "Trained batch 893 batch loss 1.22033918 epoch total loss 1.38097203\n",
      "Trained batch 894 batch loss 1.2949177 epoch total loss 1.38087571\n",
      "Trained batch 895 batch loss 1.34480357 epoch total loss 1.38083553\n",
      "Trained batch 896 batch loss 1.39536667 epoch total loss 1.38085175\n",
      "Trained batch 897 batch loss 1.35440302 epoch total loss 1.38082218\n",
      "Trained batch 898 batch loss 1.43014443 epoch total loss 1.38087714\n",
      "Trained batch 899 batch loss 1.29543495 epoch total loss 1.38078213\n",
      "Trained batch 900 batch loss 1.35172462 epoch total loss 1.38074982\n",
      "Trained batch 901 batch loss 1.26426029 epoch total loss 1.38062048\n",
      "Trained batch 902 batch loss 1.26880491 epoch total loss 1.3804965\n",
      "Trained batch 903 batch loss 1.33654499 epoch total loss 1.38044786\n",
      "Trained batch 904 batch loss 1.31889868 epoch total loss 1.38037968\n",
      "Trained batch 905 batch loss 1.36588907 epoch total loss 1.3803637\n",
      "Trained batch 906 batch loss 1.39406514 epoch total loss 1.38037872\n",
      "Trained batch 907 batch loss 1.3125689 epoch total loss 1.3803041\n",
      "Trained batch 908 batch loss 1.26159692 epoch total loss 1.38017333\n",
      "Trained batch 909 batch loss 1.36135399 epoch total loss 1.38015258\n",
      "Trained batch 910 batch loss 1.18045247 epoch total loss 1.37993312\n",
      "Trained batch 911 batch loss 1.44008195 epoch total loss 1.37999916\n",
      "Trained batch 912 batch loss 1.50283515 epoch total loss 1.38013375\n",
      "Trained batch 913 batch loss 1.4342854 epoch total loss 1.38019311\n",
      "Trained batch 914 batch loss 1.38523149 epoch total loss 1.38019872\n",
      "Trained batch 915 batch loss 1.48917711 epoch total loss 1.38031769\n",
      "Trained batch 916 batch loss 1.45008624 epoch total loss 1.38039386\n",
      "Trained batch 917 batch loss 1.38708782 epoch total loss 1.38040113\n",
      "Trained batch 918 batch loss 1.32005942 epoch total loss 1.38033545\n",
      "Trained batch 919 batch loss 1.33487666 epoch total loss 1.38028598\n",
      "Trained batch 920 batch loss 1.24472165 epoch total loss 1.38013864\n",
      "Trained batch 921 batch loss 1.43038976 epoch total loss 1.38019323\n",
      "Trained batch 922 batch loss 1.30063915 epoch total loss 1.38010693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 923 batch loss 1.45548201 epoch total loss 1.38018858\n",
      "Trained batch 924 batch loss 1.48957276 epoch total loss 1.38030708\n",
      "Trained batch 925 batch loss 1.36001706 epoch total loss 1.38028502\n",
      "Trained batch 926 batch loss 1.44592166 epoch total loss 1.38035595\n",
      "Trained batch 927 batch loss 1.35868955 epoch total loss 1.38033247\n",
      "Trained batch 928 batch loss 1.29061496 epoch total loss 1.38023591\n",
      "Trained batch 929 batch loss 1.28788698 epoch total loss 1.38013637\n",
      "Trained batch 930 batch loss 1.33700037 epoch total loss 1.38009012\n",
      "Trained batch 931 batch loss 1.29717898 epoch total loss 1.38000095\n",
      "Trained batch 932 batch loss 1.30435395 epoch total loss 1.37991977\n",
      "Trained batch 933 batch loss 1.20037532 epoch total loss 1.37972724\n",
      "Trained batch 934 batch loss 1.44257474 epoch total loss 1.3797946\n",
      "Trained batch 935 batch loss 1.52776146 epoch total loss 1.37995279\n",
      "Trained batch 936 batch loss 1.42111254 epoch total loss 1.37999678\n",
      "Trained batch 937 batch loss 1.27679527 epoch total loss 1.37988675\n",
      "Trained batch 938 batch loss 1.20014668 epoch total loss 1.37969518\n",
      "Trained batch 939 batch loss 1.23333359 epoch total loss 1.37953925\n",
      "Trained batch 940 batch loss 1.32107544 epoch total loss 1.37947702\n",
      "Trained batch 941 batch loss 1.41477025 epoch total loss 1.37951458\n",
      "Trained batch 942 batch loss 1.3850174 epoch total loss 1.37952042\n",
      "Trained batch 943 batch loss 1.42498529 epoch total loss 1.37956846\n",
      "Trained batch 944 batch loss 1.38344169 epoch total loss 1.37957263\n",
      "Trained batch 945 batch loss 1.31182909 epoch total loss 1.37950099\n",
      "Trained batch 946 batch loss 1.2864629 epoch total loss 1.37940264\n",
      "Trained batch 947 batch loss 1.3517077 epoch total loss 1.37937343\n",
      "Trained batch 948 batch loss 1.31254053 epoch total loss 1.37930286\n",
      "Trained batch 949 batch loss 1.59555101 epoch total loss 1.37953079\n",
      "Trained batch 950 batch loss 1.43652248 epoch total loss 1.37959075\n",
      "Trained batch 951 batch loss 1.27469611 epoch total loss 1.37948036\n",
      "Trained batch 952 batch loss 1.32229829 epoch total loss 1.37942028\n",
      "Trained batch 953 batch loss 1.23811364 epoch total loss 1.3792721\n",
      "Trained batch 954 batch loss 1.45747292 epoch total loss 1.37935412\n",
      "Trained batch 955 batch loss 1.29661083 epoch total loss 1.37926745\n",
      "Trained batch 956 batch loss 1.330356 epoch total loss 1.37921631\n",
      "Trained batch 957 batch loss 1.38448417 epoch total loss 1.3792218\n",
      "Trained batch 958 batch loss 1.35422993 epoch total loss 1.37919581\n",
      "Trained batch 959 batch loss 1.3096137 epoch total loss 1.37912309\n",
      "Trained batch 960 batch loss 1.28386891 epoch total loss 1.37902391\n",
      "Trained batch 961 batch loss 1.28217769 epoch total loss 1.37892318\n",
      "Trained batch 962 batch loss 1.38024843 epoch total loss 1.37892449\n",
      "Trained batch 963 batch loss 1.32286143 epoch total loss 1.37886631\n",
      "Trained batch 964 batch loss 1.35927725 epoch total loss 1.37884593\n",
      "Trained batch 965 batch loss 1.39443278 epoch total loss 1.37886214\n",
      "Trained batch 966 batch loss 1.37327409 epoch total loss 1.3788563\n",
      "Trained batch 967 batch loss 1.39534163 epoch total loss 1.37887347\n",
      "Trained batch 968 batch loss 1.36580455 epoch total loss 1.37886\n",
      "Trained batch 969 batch loss 1.32147849 epoch total loss 1.37880075\n",
      "Trained batch 970 batch loss 1.42479217 epoch total loss 1.3788482\n",
      "Trained batch 971 batch loss 1.41810691 epoch total loss 1.37888861\n",
      "Trained batch 972 batch loss 1.30676687 epoch total loss 1.37881446\n",
      "Trained batch 973 batch loss 1.40383077 epoch total loss 1.37884009\n",
      "Trained batch 974 batch loss 1.30838466 epoch total loss 1.37876773\n",
      "Trained batch 975 batch loss 1.37265253 epoch total loss 1.37876153\n",
      "Trained batch 976 batch loss 1.29957283 epoch total loss 1.37868035\n",
      "Trained batch 977 batch loss 1.29953325 epoch total loss 1.37859941\n",
      "Trained batch 978 batch loss 1.39887428 epoch total loss 1.37862015\n",
      "Trained batch 979 batch loss 1.33557069 epoch total loss 1.37857616\n",
      "Trained batch 980 batch loss 1.33871233 epoch total loss 1.37853551\n",
      "Trained batch 981 batch loss 1.28552318 epoch total loss 1.37844074\n",
      "Trained batch 982 batch loss 1.28282797 epoch total loss 1.37834334\n",
      "Trained batch 983 batch loss 1.26916599 epoch total loss 1.37823236\n",
      "Trained batch 984 batch loss 1.38079834 epoch total loss 1.37823486\n",
      "Trained batch 985 batch loss 1.40204167 epoch total loss 1.37825906\n",
      "Trained batch 986 batch loss 1.36221707 epoch total loss 1.37824273\n",
      "Trained batch 987 batch loss 1.39638007 epoch total loss 1.37826109\n",
      "Trained batch 988 batch loss 1.22304189 epoch total loss 1.37810397\n",
      "Trained batch 989 batch loss 1.28862047 epoch total loss 1.37801349\n",
      "Trained batch 990 batch loss 1.38154364 epoch total loss 1.37801707\n",
      "Trained batch 991 batch loss 1.42037797 epoch total loss 1.37805986\n",
      "Trained batch 992 batch loss 1.35669255 epoch total loss 1.37803829\n",
      "Trained batch 993 batch loss 1.45901799 epoch total loss 1.37811983\n",
      "Trained batch 994 batch loss 1.43644512 epoch total loss 1.37817848\n",
      "Trained batch 995 batch loss 1.45314693 epoch total loss 1.37825382\n",
      "Trained batch 996 batch loss 1.44523251 epoch total loss 1.37832105\n",
      "Trained batch 997 batch loss 1.26925313 epoch total loss 1.37821162\n",
      "Trained batch 998 batch loss 1.24907422 epoch total loss 1.37808216\n",
      "Trained batch 999 batch loss 1.30024719 epoch total loss 1.37800431\n",
      "Trained batch 1000 batch loss 1.25840163 epoch total loss 1.37788475\n",
      "Trained batch 1001 batch loss 1.25603974 epoch total loss 1.37776303\n",
      "Trained batch 1002 batch loss 1.33638525 epoch total loss 1.37772179\n",
      "Trained batch 1003 batch loss 1.37465429 epoch total loss 1.37771869\n",
      "Trained batch 1004 batch loss 1.50062633 epoch total loss 1.377841\n",
      "Trained batch 1005 batch loss 1.35597825 epoch total loss 1.3778193\n",
      "Trained batch 1006 batch loss 1.17020226 epoch total loss 1.37761283\n",
      "Trained batch 1007 batch loss 1.30325747 epoch total loss 1.37753904\n",
      "Trained batch 1008 batch loss 1.22199297 epoch total loss 1.37738478\n",
      "Trained batch 1009 batch loss 1.31117034 epoch total loss 1.3773191\n",
      "Trained batch 1010 batch loss 1.39114797 epoch total loss 1.37733281\n",
      "Trained batch 1011 batch loss 1.35390079 epoch total loss 1.37730956\n",
      "Trained batch 1012 batch loss 1.38313222 epoch total loss 1.3773154\n",
      "Trained batch 1013 batch loss 1.29283214 epoch total loss 1.37723196\n",
      "Trained batch 1014 batch loss 1.32049179 epoch total loss 1.37717593\n",
      "Trained batch 1015 batch loss 1.42509294 epoch total loss 1.37722313\n",
      "Trained batch 1016 batch loss 1.33527756 epoch total loss 1.37718189\n",
      "Trained batch 1017 batch loss 1.24899483 epoch total loss 1.37705588\n",
      "Trained batch 1018 batch loss 1.34663284 epoch total loss 1.37702608\n",
      "Trained batch 1019 batch loss 1.46009517 epoch total loss 1.3771075\n",
      "Trained batch 1020 batch loss 1.34419477 epoch total loss 1.37707531\n",
      "Trained batch 1021 batch loss 1.30699909 epoch total loss 1.37700665\n",
      "Trained batch 1022 batch loss 1.39450252 epoch total loss 1.37702382\n",
      "Trained batch 1023 batch loss 1.29533267 epoch total loss 1.37694395\n",
      "Trained batch 1024 batch loss 1.49274683 epoch total loss 1.37705708\n",
      "Trained batch 1025 batch loss 1.46451831 epoch total loss 1.37714231\n",
      "Trained batch 1026 batch loss 1.30539191 epoch total loss 1.37707245\n",
      "Trained batch 1027 batch loss 1.43421721 epoch total loss 1.37712812\n",
      "Trained batch 1028 batch loss 1.4388653 epoch total loss 1.37718809\n",
      "Trained batch 1029 batch loss 1.39734209 epoch total loss 1.37720776\n",
      "Trained batch 1030 batch loss 1.40785086 epoch total loss 1.37723744\n",
      "Trained batch 1031 batch loss 1.30544043 epoch total loss 1.37716782\n",
      "Trained batch 1032 batch loss 1.26034582 epoch total loss 1.37705457\n",
      "Trained batch 1033 batch loss 1.3213923 epoch total loss 1.37700069\n",
      "Trained batch 1034 batch loss 1.31451285 epoch total loss 1.37694025\n",
      "Trained batch 1035 batch loss 1.38745213 epoch total loss 1.37695038\n",
      "Trained batch 1036 batch loss 1.37047291 epoch total loss 1.37694418\n",
      "Trained batch 1037 batch loss 1.20348477 epoch total loss 1.37677693\n",
      "Trained batch 1038 batch loss 1.28100109 epoch total loss 1.37668467\n",
      "Trained batch 1039 batch loss 1.29021525 epoch total loss 1.37660134\n",
      "Trained batch 1040 batch loss 1.28405774 epoch total loss 1.37651241\n",
      "Trained batch 1041 batch loss 1.25929511 epoch total loss 1.37639976\n",
      "Trained batch 1042 batch loss 1.43608761 epoch total loss 1.37645698\n",
      "Trained batch 1043 batch loss 1.40480971 epoch total loss 1.37648416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1044 batch loss 1.31731892 epoch total loss 1.37642741\n",
      "Trained batch 1045 batch loss 1.31617641 epoch total loss 1.37636971\n",
      "Trained batch 1046 batch loss 1.19876528 epoch total loss 1.3762\n",
      "Trained batch 1047 batch loss 1.35711432 epoch total loss 1.3761816\n",
      "Trained batch 1048 batch loss 1.18186283 epoch total loss 1.37599623\n",
      "Trained batch 1049 batch loss 1.40104544 epoch total loss 1.37602007\n",
      "Trained batch 1050 batch loss 1.39310575 epoch total loss 1.37603629\n",
      "Trained batch 1051 batch loss 1.37678957 epoch total loss 1.37603712\n",
      "Trained batch 1052 batch loss 1.31003654 epoch total loss 1.37597442\n",
      "Trained batch 1053 batch loss 1.27032745 epoch total loss 1.37587404\n",
      "Trained batch 1054 batch loss 1.27388108 epoch total loss 1.37577736\n",
      "Trained batch 1055 batch loss 1.30559325 epoch total loss 1.37571073\n",
      "Trained batch 1056 batch loss 1.29658091 epoch total loss 1.37563586\n",
      "Trained batch 1057 batch loss 1.34612012 epoch total loss 1.37560797\n",
      "Trained batch 1058 batch loss 1.35312533 epoch total loss 1.37558675\n",
      "Trained batch 1059 batch loss 1.37197089 epoch total loss 1.37558329\n",
      "Trained batch 1060 batch loss 1.343979 epoch total loss 1.37555349\n",
      "Trained batch 1061 batch loss 1.28233218 epoch total loss 1.37546563\n",
      "Trained batch 1062 batch loss 1.22479868 epoch total loss 1.37532377\n",
      "Trained batch 1063 batch loss 1.28117037 epoch total loss 1.3752352\n",
      "Trained batch 1064 batch loss 1.30891991 epoch total loss 1.37517285\n",
      "Trained batch 1065 batch loss 1.2939961 epoch total loss 1.37509668\n",
      "Trained batch 1066 batch loss 1.22347641 epoch total loss 1.37495446\n",
      "Trained batch 1067 batch loss 1.38451362 epoch total loss 1.3749634\n",
      "Trained batch 1068 batch loss 1.42085862 epoch total loss 1.37500644\n",
      "Trained batch 1069 batch loss 1.53672576 epoch total loss 1.37515771\n",
      "Trained batch 1070 batch loss 1.51093006 epoch total loss 1.37528467\n",
      "Trained batch 1071 batch loss 1.49311757 epoch total loss 1.3753947\n",
      "Trained batch 1072 batch loss 1.51187944 epoch total loss 1.37552202\n",
      "Trained batch 1073 batch loss 1.52127635 epoch total loss 1.3756578\n",
      "Trained batch 1074 batch loss 1.33155656 epoch total loss 1.37561667\n",
      "Trained batch 1075 batch loss 1.39084387 epoch total loss 1.37563086\n",
      "Trained batch 1076 batch loss 1.4018259 epoch total loss 1.37565529\n",
      "Trained batch 1077 batch loss 1.28369749 epoch total loss 1.37556994\n",
      "Trained batch 1078 batch loss 1.35145056 epoch total loss 1.37554753\n",
      "Trained batch 1079 batch loss 1.38190055 epoch total loss 1.37555349\n",
      "Trained batch 1080 batch loss 1.40794635 epoch total loss 1.37558341\n",
      "Trained batch 1081 batch loss 1.40227115 epoch total loss 1.37560809\n",
      "Trained batch 1082 batch loss 1.30504596 epoch total loss 1.37554288\n",
      "Trained batch 1083 batch loss 1.28727758 epoch total loss 1.37546134\n",
      "Trained batch 1084 batch loss 1.35051084 epoch total loss 1.37543833\n",
      "Trained batch 1085 batch loss 1.35027361 epoch total loss 1.37541509\n",
      "Trained batch 1086 batch loss 1.29328179 epoch total loss 1.37533951\n",
      "Trained batch 1087 batch loss 1.33946109 epoch total loss 1.37530649\n",
      "Trained batch 1088 batch loss 1.44520235 epoch total loss 1.37537074\n",
      "Trained batch 1089 batch loss 1.44107604 epoch total loss 1.37543106\n",
      "Trained batch 1090 batch loss 1.5479784 epoch total loss 1.37558925\n",
      "Trained batch 1091 batch loss 1.56462133 epoch total loss 1.37576246\n",
      "Trained batch 1092 batch loss 1.45026779 epoch total loss 1.37583077\n",
      "Trained batch 1093 batch loss 1.19770408 epoch total loss 1.37566781\n",
      "Trained batch 1094 batch loss 1.34026682 epoch total loss 1.3756355\n",
      "Trained batch 1095 batch loss 1.24179149 epoch total loss 1.3755132\n",
      "Trained batch 1096 batch loss 1.30237627 epoch total loss 1.37544656\n",
      "Trained batch 1097 batch loss 1.24617529 epoch total loss 1.37532866\n",
      "Trained batch 1098 batch loss 1.26656818 epoch total loss 1.37522972\n",
      "Trained batch 1099 batch loss 1.33213377 epoch total loss 1.3751905\n",
      "Trained batch 1100 batch loss 1.17782032 epoch total loss 1.37501109\n",
      "Trained batch 1101 batch loss 1.17980945 epoch total loss 1.37483382\n",
      "Trained batch 1102 batch loss 1.23465478 epoch total loss 1.37470651\n",
      "Trained batch 1103 batch loss 1.40389073 epoch total loss 1.37473309\n",
      "Trained batch 1104 batch loss 1.24754012 epoch total loss 1.37461782\n",
      "Trained batch 1105 batch loss 1.18639755 epoch total loss 1.37444758\n",
      "Trained batch 1106 batch loss 1.23906589 epoch total loss 1.37432504\n",
      "Trained batch 1107 batch loss 1.33478177 epoch total loss 1.37428939\n",
      "Trained batch 1108 batch loss 1.30960965 epoch total loss 1.37423098\n",
      "Trained batch 1109 batch loss 1.279562 epoch total loss 1.37414563\n",
      "Trained batch 1110 batch loss 1.1482563 epoch total loss 1.37394214\n",
      "Trained batch 1111 batch loss 1.19253612 epoch total loss 1.37377882\n",
      "Trained batch 1112 batch loss 1.31232738 epoch total loss 1.37372363\n",
      "Trained batch 1113 batch loss 1.34924638 epoch total loss 1.37370169\n",
      "Trained batch 1114 batch loss 1.51925826 epoch total loss 1.37383235\n",
      "Trained batch 1115 batch loss 1.58841181 epoch total loss 1.37402475\n",
      "Trained batch 1116 batch loss 1.22552538 epoch total loss 1.37389171\n",
      "Trained batch 1117 batch loss 1.25021243 epoch total loss 1.37378109\n",
      "Trained batch 1118 batch loss 1.46490693 epoch total loss 1.37386262\n",
      "Trained batch 1119 batch loss 1.32904434 epoch total loss 1.37382257\n",
      "Trained batch 1120 batch loss 1.49353337 epoch total loss 1.3739295\n",
      "Trained batch 1121 batch loss 1.39311981 epoch total loss 1.37394655\n",
      "Trained batch 1122 batch loss 1.44492137 epoch total loss 1.37400985\n",
      "Trained batch 1123 batch loss 1.55594862 epoch total loss 1.37417185\n",
      "Trained batch 1124 batch loss 1.41758168 epoch total loss 1.37421048\n",
      "Trained batch 1125 batch loss 1.41900015 epoch total loss 1.37425017\n",
      "Trained batch 1126 batch loss 1.30490065 epoch total loss 1.37418866\n",
      "Trained batch 1127 batch loss 1.20481205 epoch total loss 1.37403834\n",
      "Trained batch 1128 batch loss 1.2502265 epoch total loss 1.37392867\n",
      "Trained batch 1129 batch loss 1.27312684 epoch total loss 1.37383926\n",
      "Trained batch 1130 batch loss 1.23689568 epoch total loss 1.37371814\n",
      "Trained batch 1131 batch loss 1.26027942 epoch total loss 1.37361789\n",
      "Trained batch 1132 batch loss 1.33467698 epoch total loss 1.37358344\n",
      "Trained batch 1133 batch loss 1.29205143 epoch total loss 1.37351143\n",
      "Trained batch 1134 batch loss 1.38871253 epoch total loss 1.37352479\n",
      "Trained batch 1135 batch loss 1.44214249 epoch total loss 1.37358522\n",
      "Trained batch 1136 batch loss 1.34916234 epoch total loss 1.37356377\n",
      "Trained batch 1137 batch loss 1.40739226 epoch total loss 1.37359345\n",
      "Trained batch 1138 batch loss 1.36226583 epoch total loss 1.37358356\n",
      "Trained batch 1139 batch loss 1.36387384 epoch total loss 1.37357497\n",
      "Trained batch 1140 batch loss 1.34438789 epoch total loss 1.37354934\n",
      "Trained batch 1141 batch loss 1.43477094 epoch total loss 1.37360311\n",
      "Trained batch 1142 batch loss 1.47385097 epoch total loss 1.37369084\n",
      "Trained batch 1143 batch loss 1.41719532 epoch total loss 1.37372899\n",
      "Trained batch 1144 batch loss 1.30495608 epoch total loss 1.37366891\n",
      "Trained batch 1145 batch loss 1.3937062 epoch total loss 1.37368631\n",
      "Trained batch 1146 batch loss 1.4247179 epoch total loss 1.37373078\n",
      "Trained batch 1147 batch loss 1.46878731 epoch total loss 1.37381363\n",
      "Trained batch 1148 batch loss 1.40847242 epoch total loss 1.37384379\n",
      "Trained batch 1149 batch loss 1.30190849 epoch total loss 1.3737812\n",
      "Trained batch 1150 batch loss 1.34876394 epoch total loss 1.37375939\n",
      "Trained batch 1151 batch loss 1.41599596 epoch total loss 1.37379611\n",
      "Trained batch 1152 batch loss 1.32157493 epoch total loss 1.37375081\n",
      "Trained batch 1153 batch loss 1.2809087 epoch total loss 1.37367022\n",
      "Trained batch 1154 batch loss 1.4478755 epoch total loss 1.37373459\n",
      "Trained batch 1155 batch loss 1.3878659 epoch total loss 1.37374675\n",
      "Trained batch 1156 batch loss 1.29298306 epoch total loss 1.3736769\n",
      "Trained batch 1157 batch loss 1.32628489 epoch total loss 1.37363589\n",
      "Trained batch 1158 batch loss 1.23832583 epoch total loss 1.37351906\n",
      "Trained batch 1159 batch loss 1.31459761 epoch total loss 1.37346816\n",
      "Trained batch 1160 batch loss 1.25921965 epoch total loss 1.37336969\n",
      "Trained batch 1161 batch loss 1.26268232 epoch total loss 1.37327445\n",
      "Trained batch 1162 batch loss 1.33204579 epoch total loss 1.37323892\n",
      "Trained batch 1163 batch loss 1.49300337 epoch total loss 1.37334192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1164 batch loss 1.52361894 epoch total loss 1.37347102\n",
      "Trained batch 1165 batch loss 1.55509257 epoch total loss 1.37362683\n",
      "Trained batch 1166 batch loss 1.43113232 epoch total loss 1.37367618\n",
      "Trained batch 1167 batch loss 1.23890185 epoch total loss 1.37356067\n",
      "Trained batch 1168 batch loss 1.19383347 epoch total loss 1.37340677\n",
      "Trained batch 1169 batch loss 1.13678455 epoch total loss 1.37320447\n",
      "Trained batch 1170 batch loss 1.0866468 epoch total loss 1.37295949\n",
      "Trained batch 1171 batch loss 1.2476747 epoch total loss 1.37285256\n",
      "Trained batch 1172 batch loss 1.23800325 epoch total loss 1.37273753\n",
      "Trained batch 1173 batch loss 1.34991813 epoch total loss 1.3727181\n",
      "Trained batch 1174 batch loss 1.3611877 epoch total loss 1.37270832\n",
      "Trained batch 1175 batch loss 1.35368323 epoch total loss 1.37269211\n",
      "Trained batch 1176 batch loss 1.52576101 epoch total loss 1.37282228\n",
      "Trained batch 1177 batch loss 1.50522673 epoch total loss 1.37293482\n",
      "Trained batch 1178 batch loss 1.41039288 epoch total loss 1.37296653\n",
      "Trained batch 1179 batch loss 1.30565834 epoch total loss 1.37290943\n",
      "Trained batch 1180 batch loss 1.34468007 epoch total loss 1.37288558\n",
      "Trained batch 1181 batch loss 1.33480549 epoch total loss 1.3728534\n",
      "Trained batch 1182 batch loss 1.344468 epoch total loss 1.37282944\n",
      "Trained batch 1183 batch loss 1.33985615 epoch total loss 1.37280154\n",
      "Trained batch 1184 batch loss 1.46435273 epoch total loss 1.37287879\n",
      "Trained batch 1185 batch loss 1.46019673 epoch total loss 1.37295246\n",
      "Trained batch 1186 batch loss 1.44060016 epoch total loss 1.37300956\n",
      "Trained batch 1187 batch loss 1.45618463 epoch total loss 1.37307954\n",
      "Trained batch 1188 batch loss 1.57497978 epoch total loss 1.37324953\n",
      "Trained batch 1189 batch loss 1.46495223 epoch total loss 1.37332666\n",
      "Trained batch 1190 batch loss 1.44529438 epoch total loss 1.3733871\n",
      "Trained batch 1191 batch loss 1.4669838 epoch total loss 1.37346578\n",
      "Trained batch 1192 batch loss 1.41405106 epoch total loss 1.37349987\n",
      "Trained batch 1193 batch loss 1.26861238 epoch total loss 1.37341189\n",
      "Trained batch 1194 batch loss 1.1837852 epoch total loss 1.37325311\n",
      "Trained batch 1195 batch loss 1.13819194 epoch total loss 1.37305641\n",
      "Trained batch 1196 batch loss 1.2661581 epoch total loss 1.372967\n",
      "Trained batch 1197 batch loss 1.42916703 epoch total loss 1.37301397\n",
      "Trained batch 1198 batch loss 1.61828983 epoch total loss 1.37321866\n",
      "Trained batch 1199 batch loss 1.43891346 epoch total loss 1.37327349\n",
      "Trained batch 1200 batch loss 1.38501668 epoch total loss 1.37328327\n",
      "Trained batch 1201 batch loss 1.32138777 epoch total loss 1.37324011\n",
      "Trained batch 1202 batch loss 1.44446802 epoch total loss 1.37329936\n",
      "Trained batch 1203 batch loss 1.34881711 epoch total loss 1.37327909\n",
      "Trained batch 1204 batch loss 1.44090521 epoch total loss 1.37333524\n",
      "Trained batch 1205 batch loss 1.35076404 epoch total loss 1.37331641\n",
      "Trained batch 1206 batch loss 1.39110208 epoch total loss 1.37333119\n",
      "Trained batch 1207 batch loss 1.40186286 epoch total loss 1.37335479\n",
      "Trained batch 1208 batch loss 1.34002042 epoch total loss 1.37332714\n",
      "Trained batch 1209 batch loss 1.27706766 epoch total loss 1.37324762\n",
      "Trained batch 1210 batch loss 1.30459356 epoch total loss 1.37319088\n",
      "Trained batch 1211 batch loss 1.26319635 epoch total loss 1.3731\n",
      "Trained batch 1212 batch loss 1.42340851 epoch total loss 1.37314153\n",
      "Trained batch 1213 batch loss 1.24287188 epoch total loss 1.37303424\n",
      "Trained batch 1214 batch loss 1.19561255 epoch total loss 1.37288797\n",
      "Trained batch 1215 batch loss 1.43840897 epoch total loss 1.37294185\n",
      "Trained batch 1216 batch loss 1.41131973 epoch total loss 1.37297344\n",
      "Trained batch 1217 batch loss 1.37974846 epoch total loss 1.37297904\n",
      "Trained batch 1218 batch loss 1.45165277 epoch total loss 1.37304366\n",
      "Trained batch 1219 batch loss 1.33072591 epoch total loss 1.37300897\n",
      "Trained batch 1220 batch loss 1.41934133 epoch total loss 1.37304688\n",
      "Trained batch 1221 batch loss 1.31658208 epoch total loss 1.37300062\n",
      "Trained batch 1222 batch loss 1.3588748 epoch total loss 1.37298906\n",
      "Trained batch 1223 batch loss 1.21193862 epoch total loss 1.37285733\n",
      "Trained batch 1224 batch loss 1.12888467 epoch total loss 1.37265801\n",
      "Trained batch 1225 batch loss 1.13056338 epoch total loss 1.37246048\n",
      "Trained batch 1226 batch loss 1.21524584 epoch total loss 1.37233222\n",
      "Trained batch 1227 batch loss 1.23421633 epoch total loss 1.37221968\n",
      "Trained batch 1228 batch loss 1.07531238 epoch total loss 1.37197781\n",
      "Trained batch 1229 batch loss 1.00991488 epoch total loss 1.37168324\n",
      "Trained batch 1230 batch loss 1.04695559 epoch total loss 1.37141931\n",
      "Trained batch 1231 batch loss 1.07652032 epoch total loss 1.3711797\n",
      "Trained batch 1232 batch loss 1.42491317 epoch total loss 1.37122333\n",
      "Trained batch 1233 batch loss 1.28153896 epoch total loss 1.37115061\n",
      "Trained batch 1234 batch loss 1.32499588 epoch total loss 1.37111318\n",
      "Trained batch 1235 batch loss 1.29929256 epoch total loss 1.37105501\n",
      "Trained batch 1236 batch loss 1.30307364 epoch total loss 1.371\n",
      "Trained batch 1237 batch loss 1.41661048 epoch total loss 1.37103689\n",
      "Trained batch 1238 batch loss 1.24390531 epoch total loss 1.37093425\n",
      "Trained batch 1239 batch loss 1.31931639 epoch total loss 1.37089252\n",
      "Trained batch 1240 batch loss 1.37605369 epoch total loss 1.37089682\n",
      "Trained batch 1241 batch loss 1.28289676 epoch total loss 1.37082577\n",
      "Trained batch 1242 batch loss 1.26907134 epoch total loss 1.37074387\n",
      "Trained batch 1243 batch loss 1.33646369 epoch total loss 1.37071621\n",
      "Trained batch 1244 batch loss 1.30305481 epoch total loss 1.37066185\n",
      "Trained batch 1245 batch loss 1.21508121 epoch total loss 1.37053692\n",
      "Trained batch 1246 batch loss 1.15191948 epoch total loss 1.37036157\n",
      "Trained batch 1247 batch loss 1.12428236 epoch total loss 1.37016416\n",
      "Trained batch 1248 batch loss 1.35376418 epoch total loss 1.37015104\n",
      "Trained batch 1249 batch loss 1.31118703 epoch total loss 1.37010384\n",
      "Trained batch 1250 batch loss 1.21063185 epoch total loss 1.36997616\n",
      "Trained batch 1251 batch loss 1.22833753 epoch total loss 1.36986303\n",
      "Trained batch 1252 batch loss 1.2809031 epoch total loss 1.36979198\n",
      "Trained batch 1253 batch loss 1.42072845 epoch total loss 1.36983263\n",
      "Trained batch 1254 batch loss 1.41663575 epoch total loss 1.36987\n",
      "Trained batch 1255 batch loss 1.42284381 epoch total loss 1.36991215\n",
      "Trained batch 1256 batch loss 1.35072958 epoch total loss 1.36989689\n",
      "Trained batch 1257 batch loss 1.37960076 epoch total loss 1.36990464\n",
      "Trained batch 1258 batch loss 1.34434664 epoch total loss 1.36988425\n",
      "Trained batch 1259 batch loss 1.33780026 epoch total loss 1.36985874\n",
      "Trained batch 1260 batch loss 1.18853104 epoch total loss 1.36971486\n",
      "Trained batch 1261 batch loss 1.46419322 epoch total loss 1.36978984\n",
      "Trained batch 1262 batch loss 1.31659675 epoch total loss 1.36974764\n",
      "Trained batch 1263 batch loss 1.28544807 epoch total loss 1.36968088\n",
      "Trained batch 1264 batch loss 1.29812551 epoch total loss 1.36962426\n",
      "Trained batch 1265 batch loss 1.34710419 epoch total loss 1.36960638\n",
      "Trained batch 1266 batch loss 1.4205215 epoch total loss 1.36964667\n",
      "Trained batch 1267 batch loss 1.27627361 epoch total loss 1.36957288\n",
      "Trained batch 1268 batch loss 1.10139406 epoch total loss 1.36936152\n",
      "Trained batch 1269 batch loss 1.09457231 epoch total loss 1.36914492\n",
      "Trained batch 1270 batch loss 1.28744709 epoch total loss 1.36908066\n",
      "Trained batch 1271 batch loss 1.28356171 epoch total loss 1.36901331\n",
      "Trained batch 1272 batch loss 1.30137897 epoch total loss 1.36896026\n",
      "Trained batch 1273 batch loss 1.21383727 epoch total loss 1.36883843\n",
      "Trained batch 1274 batch loss 1.34394169 epoch total loss 1.36881888\n",
      "Trained batch 1275 batch loss 1.40383255 epoch total loss 1.3688463\n",
      "Trained batch 1276 batch loss 1.38653827 epoch total loss 1.36886024\n",
      "Trained batch 1277 batch loss 1.39806509 epoch total loss 1.36888313\n",
      "Trained batch 1278 batch loss 1.50820112 epoch total loss 1.36899209\n",
      "Trained batch 1279 batch loss 1.55506933 epoch total loss 1.36913753\n",
      "Trained batch 1280 batch loss 1.36636496 epoch total loss 1.36913538\n",
      "Trained batch 1281 batch loss 1.32279611 epoch total loss 1.36909914\n",
      "Trained batch 1282 batch loss 1.33965087 epoch total loss 1.36907613\n",
      "Trained batch 1283 batch loss 1.42179537 epoch total loss 1.36911726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1284 batch loss 1.4075613 epoch total loss 1.36914718\n",
      "Trained batch 1285 batch loss 1.37301815 epoch total loss 1.36915016\n",
      "Trained batch 1286 batch loss 1.25636 epoch total loss 1.36906254\n",
      "Trained batch 1287 batch loss 1.23184788 epoch total loss 1.36895585\n",
      "Trained batch 1288 batch loss 1.25464189 epoch total loss 1.36886716\n",
      "Trained batch 1289 batch loss 1.18888736 epoch total loss 1.36872745\n",
      "Trained batch 1290 batch loss 1.25214946 epoch total loss 1.36863708\n",
      "Trained batch 1291 batch loss 1.28465414 epoch total loss 1.36857212\n",
      "Trained batch 1292 batch loss 1.22005057 epoch total loss 1.3684572\n",
      "Trained batch 1293 batch loss 1.30643892 epoch total loss 1.36840916\n",
      "Trained batch 1294 batch loss 1.20002985 epoch total loss 1.3682791\n",
      "Trained batch 1295 batch loss 1.36239266 epoch total loss 1.36827457\n",
      "Trained batch 1296 batch loss 1.27021718 epoch total loss 1.36819887\n",
      "Trained batch 1297 batch loss 1.19994378 epoch total loss 1.36806917\n",
      "Trained batch 1298 batch loss 1.3409586 epoch total loss 1.36804831\n",
      "Trained batch 1299 batch loss 1.43658292 epoch total loss 1.368101\n",
      "Trained batch 1300 batch loss 1.3540554 epoch total loss 1.36809015\n",
      "Trained batch 1301 batch loss 1.31683755 epoch total loss 1.36805081\n",
      "Trained batch 1302 batch loss 1.28514802 epoch total loss 1.36798716\n",
      "Trained batch 1303 batch loss 1.31471622 epoch total loss 1.36794627\n",
      "Trained batch 1304 batch loss 1.35194802 epoch total loss 1.36793399\n",
      "Trained batch 1305 batch loss 1.33542573 epoch total loss 1.36790907\n",
      "Trained batch 1306 batch loss 1.53509784 epoch total loss 1.3680371\n",
      "Trained batch 1307 batch loss 1.4854207 epoch total loss 1.36812699\n",
      "Trained batch 1308 batch loss 1.4148922 epoch total loss 1.36816275\n",
      "Trained batch 1309 batch loss 1.29254043 epoch total loss 1.36810493\n",
      "Trained batch 1310 batch loss 1.29560149 epoch total loss 1.36804962\n",
      "Trained batch 1311 batch loss 1.11572814 epoch total loss 1.36785722\n",
      "Trained batch 1312 batch loss 1.12211609 epoch total loss 1.36766982\n",
      "Trained batch 1313 batch loss 1.26029587 epoch total loss 1.36758804\n",
      "Trained batch 1314 batch loss 1.07826686 epoch total loss 1.36736786\n",
      "Trained batch 1315 batch loss 1.11926019 epoch total loss 1.36717916\n",
      "Trained batch 1316 batch loss 1.12553406 epoch total loss 1.36699545\n",
      "Trained batch 1317 batch loss 1.08219123 epoch total loss 1.36677921\n",
      "Trained batch 1318 batch loss 1.3278898 epoch total loss 1.36674964\n",
      "Trained batch 1319 batch loss 1.3026371 epoch total loss 1.36670113\n",
      "Trained batch 1320 batch loss 1.36422825 epoch total loss 1.36669922\n",
      "Trained batch 1321 batch loss 1.3796196 epoch total loss 1.36670899\n",
      "Trained batch 1322 batch loss 1.42194915 epoch total loss 1.36675084\n",
      "Trained batch 1323 batch loss 1.39743221 epoch total loss 1.36677408\n",
      "Trained batch 1324 batch loss 1.27872872 epoch total loss 1.36670756\n",
      "Trained batch 1325 batch loss 1.24106526 epoch total loss 1.36661267\n",
      "Trained batch 1326 batch loss 1.21718144 epoch total loss 1.3665\n",
      "Trained batch 1327 batch loss 1.25848651 epoch total loss 1.3664186\n",
      "Trained batch 1328 batch loss 1.24658108 epoch total loss 1.36632836\n",
      "Trained batch 1329 batch loss 1.24082208 epoch total loss 1.36623394\n",
      "Trained batch 1330 batch loss 1.28017473 epoch total loss 1.36616921\n",
      "Trained batch 1331 batch loss 1.2194953 epoch total loss 1.36605906\n",
      "Trained batch 1332 batch loss 1.22366595 epoch total loss 1.36595213\n",
      "Trained batch 1333 batch loss 1.21029961 epoch total loss 1.36583543\n",
      "Trained batch 1334 batch loss 1.1978 epoch total loss 1.36570942\n",
      "Trained batch 1335 batch loss 1.25594056 epoch total loss 1.36562717\n",
      "Trained batch 1336 batch loss 1.34027553 epoch total loss 1.36560822\n",
      "Trained batch 1337 batch loss 1.50393462 epoch total loss 1.36571169\n",
      "Trained batch 1338 batch loss 1.45645142 epoch total loss 1.36577952\n",
      "Trained batch 1339 batch loss 1.41852021 epoch total loss 1.36581898\n",
      "Trained batch 1340 batch loss 1.52278018 epoch total loss 1.36593604\n",
      "Trained batch 1341 batch loss 1.32851171 epoch total loss 1.36590815\n",
      "Trained batch 1342 batch loss 1.39676046 epoch total loss 1.36593115\n",
      "Trained batch 1343 batch loss 1.40774155 epoch total loss 1.36596227\n",
      "Trained batch 1344 batch loss 1.21651435 epoch total loss 1.36585104\n",
      "Trained batch 1345 batch loss 1.27631795 epoch total loss 1.36578453\n",
      "Trained batch 1346 batch loss 1.36557734 epoch total loss 1.36578441\n",
      "Trained batch 1347 batch loss 1.34753096 epoch total loss 1.36577082\n",
      "Trained batch 1348 batch loss 1.2497896 epoch total loss 1.36568475\n",
      "Trained batch 1349 batch loss 1.19920897 epoch total loss 1.36556137\n",
      "Trained batch 1350 batch loss 1.1740557 epoch total loss 1.36541951\n",
      "Trained batch 1351 batch loss 1.33351624 epoch total loss 1.3653959\n",
      "Trained batch 1352 batch loss 1.33841157 epoch total loss 1.365376\n",
      "Trained batch 1353 batch loss 1.41485322 epoch total loss 1.36541247\n",
      "Trained batch 1354 batch loss 1.58766103 epoch total loss 1.36557662\n",
      "Trained batch 1355 batch loss 1.48096824 epoch total loss 1.36566174\n",
      "Trained batch 1356 batch loss 1.435179 epoch total loss 1.365713\n",
      "Trained batch 1357 batch loss 1.41390061 epoch total loss 1.36574852\n",
      "Trained batch 1358 batch loss 1.22348189 epoch total loss 1.36564386\n",
      "Trained batch 1359 batch loss 1.28836095 epoch total loss 1.36558688\n",
      "Trained batch 1360 batch loss 1.21408141 epoch total loss 1.36547554\n",
      "Trained batch 1361 batch loss 1.3169471 epoch total loss 1.36543989\n",
      "Trained batch 1362 batch loss 1.33983135 epoch total loss 1.36542106\n",
      "Trained batch 1363 batch loss 1.37462819 epoch total loss 1.36542785\n",
      "Trained batch 1364 batch loss 1.314206 epoch total loss 1.3653903\n",
      "Trained batch 1365 batch loss 1.35800219 epoch total loss 1.36538494\n",
      "Trained batch 1366 batch loss 1.3177886 epoch total loss 1.36535\n",
      "Trained batch 1367 batch loss 1.34237289 epoch total loss 1.3653332\n",
      "Trained batch 1368 batch loss 1.43887925 epoch total loss 1.36538696\n",
      "Trained batch 1369 batch loss 1.23072147 epoch total loss 1.36528862\n",
      "Trained batch 1370 batch loss 1.28778958 epoch total loss 1.36523211\n",
      "Trained batch 1371 batch loss 1.28262699 epoch total loss 1.36517179\n",
      "Trained batch 1372 batch loss 1.14576435 epoch total loss 1.36501181\n",
      "Trained batch 1373 batch loss 1.17713344 epoch total loss 1.36487496\n",
      "Trained batch 1374 batch loss 1.24700177 epoch total loss 1.36478913\n",
      "Trained batch 1375 batch loss 1.19886291 epoch total loss 1.36466849\n",
      "Trained batch 1376 batch loss 1.1473434 epoch total loss 1.36451054\n",
      "Trained batch 1377 batch loss 1.24160337 epoch total loss 1.36442125\n",
      "Trained batch 1378 batch loss 1.17969334 epoch total loss 1.36428726\n",
      "Trained batch 1379 batch loss 1.31283844 epoch total loss 1.36425\n",
      "Trained batch 1380 batch loss 1.26995647 epoch total loss 1.36418152\n",
      "Trained batch 1381 batch loss 1.3919189 epoch total loss 1.36420166\n",
      "Trained batch 1382 batch loss 1.28388619 epoch total loss 1.36414361\n",
      "Trained batch 1383 batch loss 1.31862593 epoch total loss 1.36411071\n",
      "Trained batch 1384 batch loss 1.33481145 epoch total loss 1.36408949\n",
      "Trained batch 1385 batch loss 1.23567092 epoch total loss 1.36399686\n",
      "Trained batch 1386 batch loss 1.33917224 epoch total loss 1.36397886\n",
      "Trained batch 1387 batch loss 1.32640648 epoch total loss 1.3639518\n",
      "Trained batch 1388 batch loss 1.50111735 epoch total loss 1.36405063\n",
      "Epoch 2 train loss 1.3640506267547607\n",
      "Validated batch 1 batch loss 1.30860066\n",
      "Validated batch 2 batch loss 1.28135824\n",
      "Validated batch 3 batch loss 1.18924224\n",
      "Validated batch 4 batch loss 1.36081457\n",
      "Validated batch 5 batch loss 1.26585078\n",
      "Validated batch 6 batch loss 1.32403386\n",
      "Validated batch 7 batch loss 1.3979156\n",
      "Validated batch 8 batch loss 1.34756756\n",
      "Validated batch 9 batch loss 1.29625595\n",
      "Validated batch 10 batch loss 1.27965212\n",
      "Validated batch 11 batch loss 1.34921515\n",
      "Validated batch 12 batch loss 1.27351868\n",
      "Validated batch 13 batch loss 1.3068409\n",
      "Validated batch 14 batch loss 1.41009569\n",
      "Validated batch 15 batch loss 1.35945761\n",
      "Validated batch 16 batch loss 1.28501594\n",
      "Validated batch 17 batch loss 1.4208076\n",
      "Validated batch 18 batch loss 1.17833805\n",
      "Validated batch 19 batch loss 1.37052429\n",
      "Validated batch 20 batch loss 1.11777449\n",
      "Validated batch 21 batch loss 1.32012582\n",
      "Validated batch 22 batch loss 1.38352311\n",
      "Validated batch 23 batch loss 1.2105875\n",
      "Validated batch 24 batch loss 1.21178722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 25 batch loss 1.30847669\n",
      "Validated batch 26 batch loss 1.32126689\n",
      "Validated batch 27 batch loss 1.26650727\n",
      "Validated batch 28 batch loss 1.36566448\n",
      "Validated batch 29 batch loss 1.44410539\n",
      "Validated batch 30 batch loss 1.17437923\n",
      "Validated batch 31 batch loss 1.3189404\n",
      "Validated batch 32 batch loss 1.28463674\n",
      "Validated batch 33 batch loss 1.3815186\n",
      "Validated batch 34 batch loss 1.32813954\n",
      "Validated batch 35 batch loss 1.19180453\n",
      "Validated batch 36 batch loss 1.17334747\n",
      "Validated batch 37 batch loss 1.28914332\n",
      "Validated batch 38 batch loss 1.24897277\n",
      "Validated batch 39 batch loss 1.25175428\n",
      "Validated batch 40 batch loss 1.28400242\n",
      "Validated batch 41 batch loss 1.21958113\n",
      "Validated batch 42 batch loss 1.35407126\n",
      "Validated batch 43 batch loss 1.34245455\n",
      "Validated batch 44 batch loss 1.3550154\n",
      "Validated batch 45 batch loss 1.32175767\n",
      "Validated batch 46 batch loss 1.20989823\n",
      "Validated batch 47 batch loss 1.32395935\n",
      "Validated batch 48 batch loss 1.31331706\n",
      "Validated batch 49 batch loss 1.22545838\n",
      "Validated batch 50 batch loss 1.22633362\n",
      "Validated batch 51 batch loss 1.29660249\n",
      "Validated batch 52 batch loss 1.31973386\n",
      "Validated batch 53 batch loss 1.27683878\n",
      "Validated batch 54 batch loss 1.27413297\n",
      "Validated batch 55 batch loss 1.27755511\n",
      "Validated batch 56 batch loss 1.34549642\n",
      "Validated batch 57 batch loss 1.19641566\n",
      "Validated batch 58 batch loss 1.15935254\n",
      "Validated batch 59 batch loss 1.34384775\n",
      "Validated batch 60 batch loss 1.36736131\n",
      "Validated batch 61 batch loss 1.41985059\n",
      "Validated batch 62 batch loss 1.38516235\n",
      "Validated batch 63 batch loss 1.24448228\n",
      "Validated batch 64 batch loss 1.46473551\n",
      "Validated batch 65 batch loss 1.25598538\n",
      "Validated batch 66 batch loss 1.35691726\n",
      "Validated batch 67 batch loss 1.33191061\n",
      "Validated batch 68 batch loss 1.06822848\n",
      "Validated batch 69 batch loss 1.2866354\n",
      "Validated batch 70 batch loss 1.28583479\n",
      "Validated batch 71 batch loss 1.25431347\n",
      "Validated batch 72 batch loss 1.28896952\n",
      "Validated batch 73 batch loss 1.17422807\n",
      "Validated batch 74 batch loss 1.27464807\n",
      "Validated batch 75 batch loss 1.35501218\n",
      "Validated batch 76 batch loss 1.30360866\n",
      "Validated batch 77 batch loss 1.4208709\n",
      "Validated batch 78 batch loss 1.36563158\n",
      "Validated batch 79 batch loss 1.31310773\n",
      "Validated batch 80 batch loss 1.29857373\n",
      "Validated batch 81 batch loss 1.4198935\n",
      "Validated batch 82 batch loss 1.34446669\n",
      "Validated batch 83 batch loss 1.39413595\n",
      "Validated batch 84 batch loss 1.3764565\n",
      "Validated batch 85 batch loss 1.38538718\n",
      "Validated batch 86 batch loss 1.35901248\n",
      "Validated batch 87 batch loss 1.21759701\n",
      "Validated batch 88 batch loss 1.24795771\n",
      "Validated batch 89 batch loss 1.35558069\n",
      "Validated batch 90 batch loss 1.36933684\n",
      "Validated batch 91 batch loss 1.27773941\n",
      "Validated batch 92 batch loss 1.31181216\n",
      "Validated batch 93 batch loss 1.35122323\n",
      "Validated batch 94 batch loss 1.29220057\n",
      "Validated batch 95 batch loss 1.27150059\n",
      "Validated batch 96 batch loss 1.26742089\n",
      "Validated batch 97 batch loss 1.3113879\n",
      "Validated batch 98 batch loss 1.45581758\n",
      "Validated batch 99 batch loss 1.244717\n",
      "Validated batch 100 batch loss 1.34406841\n",
      "Validated batch 101 batch loss 1.27894771\n",
      "Validated batch 102 batch loss 1.33190155\n",
      "Validated batch 103 batch loss 1.29674947\n",
      "Validated batch 104 batch loss 1.21083474\n",
      "Validated batch 105 batch loss 1.37908649\n",
      "Validated batch 106 batch loss 1.34716392\n",
      "Validated batch 107 batch loss 1.33098054\n",
      "Validated batch 108 batch loss 1.31728709\n",
      "Validated batch 109 batch loss 1.34301269\n",
      "Validated batch 110 batch loss 1.23169708\n",
      "Validated batch 111 batch loss 1.28541517\n",
      "Validated batch 112 batch loss 1.28734374\n",
      "Validated batch 113 batch loss 1.23583531\n",
      "Validated batch 114 batch loss 1.33973861\n",
      "Validated batch 115 batch loss 1.28963411\n",
      "Validated batch 116 batch loss 1.46758807\n",
      "Validated batch 117 batch loss 1.28536367\n",
      "Validated batch 118 batch loss 1.2593317\n",
      "Validated batch 119 batch loss 1.27936459\n",
      "Validated batch 120 batch loss 1.22020698\n",
      "Validated batch 121 batch loss 1.31256247\n",
      "Validated batch 122 batch loss 1.30554616\n",
      "Validated batch 123 batch loss 1.27711391\n",
      "Validated batch 124 batch loss 1.24247718\n",
      "Validated batch 125 batch loss 1.31108284\n",
      "Validated batch 126 batch loss 1.31311238\n",
      "Validated batch 127 batch loss 1.36085272\n",
      "Validated batch 128 batch loss 1.32172489\n",
      "Validated batch 129 batch loss 1.20732903\n",
      "Validated batch 130 batch loss 1.24742293\n",
      "Validated batch 131 batch loss 1.30722964\n",
      "Validated batch 132 batch loss 1.26879752\n",
      "Validated batch 133 batch loss 1.34152484\n",
      "Validated batch 134 batch loss 1.37029445\n",
      "Validated batch 135 batch loss 1.51680207\n",
      "Validated batch 136 batch loss 1.45211911\n",
      "Validated batch 137 batch loss 1.30202544\n",
      "Validated batch 138 batch loss 1.22312546\n",
      "Validated batch 139 batch loss 1.22494888\n",
      "Validated batch 140 batch loss 1.16581392\n",
      "Validated batch 141 batch loss 1.26670337\n",
      "Validated batch 142 batch loss 1.24078107\n",
      "Validated batch 143 batch loss 1.22662282\n",
      "Validated batch 144 batch loss 1.27393293\n",
      "Validated batch 145 batch loss 1.26564538\n",
      "Validated batch 146 batch loss 1.29318476\n",
      "Validated batch 147 batch loss 1.39347124\n",
      "Validated batch 148 batch loss 1.17235768\n",
      "Validated batch 149 batch loss 1.37829411\n",
      "Validated batch 150 batch loss 1.29307127\n",
      "Validated batch 151 batch loss 1.17392159\n",
      "Validated batch 152 batch loss 1.268731\n",
      "Validated batch 153 batch loss 1.27694142\n",
      "Validated batch 154 batch loss 1.23883212\n",
      "Validated batch 155 batch loss 1.43421042\n",
      "Validated batch 156 batch loss 1.31876075\n",
      "Validated batch 157 batch loss 1.34734118\n",
      "Validated batch 158 batch loss 1.25853467\n",
      "Validated batch 159 batch loss 1.29955876\n",
      "Validated batch 160 batch loss 1.2840184\n",
      "Validated batch 161 batch loss 1.24003267\n",
      "Validated batch 162 batch loss 1.26992321\n",
      "Validated batch 163 batch loss 1.32830977\n",
      "Validated batch 164 batch loss 1.29469872\n",
      "Validated batch 165 batch loss 1.1689\n",
      "Validated batch 166 batch loss 1.25378025\n",
      "Validated batch 167 batch loss 1.37513411\n",
      "Validated batch 168 batch loss 1.18252468\n",
      "Validated batch 169 batch loss 1.18721557\n",
      "Validated batch 170 batch loss 1.22851729\n",
      "Validated batch 171 batch loss 1.28391755\n",
      "Validated batch 172 batch loss 1.19126248\n",
      "Validated batch 173 batch loss 1.27860379\n",
      "Validated batch 174 batch loss 1.16356063\n",
      "Validated batch 175 batch loss 1.30535936\n",
      "Validated batch 176 batch loss 1.34589577\n",
      "Validated batch 177 batch loss 1.3918879\n",
      "Validated batch 178 batch loss 1.22698939\n",
      "Validated batch 179 batch loss 1.40479898\n",
      "Validated batch 180 batch loss 1.13279378\n",
      "Validated batch 181 batch loss 1.11125052\n",
      "Validated batch 182 batch loss 1.29621625\n",
      "Validated batch 183 batch loss 1.23780107\n",
      "Validated batch 184 batch loss 1.40787768\n",
      "Validated batch 185 batch loss 1.44411898\n",
      "Epoch 2 val loss 1.2968807220458984\n",
      "Model /aiffel/aiffel/mpii/a/model-epoch-2-loss-1.2969.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.47604632 epoch total loss 1.47604632\n",
      "Trained batch 2 batch loss 1.46211672 epoch total loss 1.46908152\n",
      "Trained batch 3 batch loss 1.43069291 epoch total loss 1.45628536\n",
      "Trained batch 4 batch loss 1.30293941 epoch total loss 1.41794884\n",
      "Trained batch 5 batch loss 1.28160858 epoch total loss 1.39068079\n",
      "Trained batch 6 batch loss 1.33487582 epoch total loss 1.38138\n",
      "Trained batch 7 batch loss 1.328565 epoch total loss 1.37383485\n",
      "Trained batch 8 batch loss 1.15254152 epoch total loss 1.34617317\n",
      "Trained batch 9 batch loss 1.3108772 epoch total loss 1.3422513\n",
      "Trained batch 10 batch loss 1.38822746 epoch total loss 1.34684896\n",
      "Trained batch 11 batch loss 1.27897203 epoch total loss 1.34067833\n",
      "Trained batch 12 batch loss 1.31715596 epoch total loss 1.33871806\n",
      "Trained batch 13 batch loss 1.37576413 epoch total loss 1.34156787\n",
      "Trained batch 14 batch loss 1.28973913 epoch total loss 1.33786583\n",
      "Trained batch 15 batch loss 1.2564261 epoch total loss 1.33243644\n",
      "Trained batch 16 batch loss 1.25672293 epoch total loss 1.32770443\n",
      "Trained batch 17 batch loss 1.24152136 epoch total loss 1.32263482\n",
      "Trained batch 18 batch loss 1.22089326 epoch total loss 1.31698263\n",
      "Trained batch 19 batch loss 1.21942639 epoch total loss 1.31184804\n",
      "Trained batch 20 batch loss 1.24255431 epoch total loss 1.30838335\n",
      "Trained batch 21 batch loss 1.29046547 epoch total loss 1.30753016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 22 batch loss 1.28877473 epoch total loss 1.3066777\n",
      "Trained batch 23 batch loss 1.23461556 epoch total loss 1.30354452\n",
      "Trained batch 24 batch loss 1.21300912 epoch total loss 1.29977214\n",
      "Trained batch 25 batch loss 1.2121309 epoch total loss 1.29626644\n",
      "Trained batch 26 batch loss 1.27001858 epoch total loss 1.29525697\n",
      "Trained batch 27 batch loss 1.49062812 epoch total loss 1.30249298\n",
      "Trained batch 28 batch loss 1.38587427 epoch total loss 1.30547082\n",
      "Trained batch 29 batch loss 1.24288058 epoch total loss 1.30331266\n",
      "Trained batch 30 batch loss 1.1592654 epoch total loss 1.29851103\n",
      "Trained batch 31 batch loss 1.17284358 epoch total loss 1.2944572\n",
      "Trained batch 32 batch loss 1.17931509 epoch total loss 1.29085898\n",
      "Trained batch 33 batch loss 1.2893554 epoch total loss 1.29081345\n",
      "Trained batch 34 batch loss 1.32624578 epoch total loss 1.29185557\n",
      "Trained batch 35 batch loss 1.38360023 epoch total loss 1.29447675\n",
      "Trained batch 36 batch loss 1.39647162 epoch total loss 1.29731\n",
      "Trained batch 37 batch loss 1.28725135 epoch total loss 1.29703808\n",
      "Trained batch 38 batch loss 1.30497169 epoch total loss 1.29724681\n",
      "Trained batch 39 batch loss 1.25612116 epoch total loss 1.29619241\n",
      "Trained batch 40 batch loss 1.25258625 epoch total loss 1.29510224\n",
      "Trained batch 41 batch loss 1.26276863 epoch total loss 1.29431355\n",
      "Trained batch 42 batch loss 1.30000079 epoch total loss 1.29444897\n",
      "Trained batch 43 batch loss 1.32318282 epoch total loss 1.29511714\n",
      "Trained batch 44 batch loss 1.38602686 epoch total loss 1.29718328\n",
      "Trained batch 45 batch loss 1.34493291 epoch total loss 1.29824436\n",
      "Trained batch 46 batch loss 1.55562401 epoch total loss 1.30383968\n",
      "Trained batch 47 batch loss 1.36712408 epoch total loss 1.30518603\n",
      "Trained batch 48 batch loss 1.30474973 epoch total loss 1.30517697\n",
      "Trained batch 49 batch loss 1.35284102 epoch total loss 1.30614972\n",
      "Trained batch 50 batch loss 1.34146452 epoch total loss 1.30685592\n",
      "Trained batch 51 batch loss 1.29152727 epoch total loss 1.30655539\n",
      "Trained batch 52 batch loss 1.3053987 epoch total loss 1.3065331\n",
      "Trained batch 53 batch loss 1.49381173 epoch total loss 1.3100667\n",
      "Trained batch 54 batch loss 1.39947939 epoch total loss 1.31172252\n",
      "Trained batch 55 batch loss 1.3999114 epoch total loss 1.31332588\n",
      "Trained batch 56 batch loss 1.3364079 epoch total loss 1.31373811\n",
      "Trained batch 57 batch loss 1.34045076 epoch total loss 1.31420684\n",
      "Trained batch 58 batch loss 1.32188702 epoch total loss 1.31433916\n",
      "Trained batch 59 batch loss 1.27129078 epoch total loss 1.3136096\n",
      "Trained batch 60 batch loss 1.44935989 epoch total loss 1.31587219\n",
      "Trained batch 61 batch loss 1.42738259 epoch total loss 1.31770027\n",
      "Trained batch 62 batch loss 1.49668717 epoch total loss 1.32058716\n",
      "Trained batch 63 batch loss 1.22022486 epoch total loss 1.31899405\n",
      "Trained batch 64 batch loss 1.13742578 epoch total loss 1.3161571\n",
      "Trained batch 65 batch loss 1.33940196 epoch total loss 1.31651473\n",
      "Trained batch 66 batch loss 1.30697596 epoch total loss 1.31637013\n",
      "Trained batch 67 batch loss 1.28472245 epoch total loss 1.31589782\n",
      "Trained batch 68 batch loss 1.3052206 epoch total loss 1.31574082\n",
      "Trained batch 69 batch loss 1.40411162 epoch total loss 1.31702161\n",
      "Trained batch 70 batch loss 1.37350261 epoch total loss 1.31782854\n",
      "Trained batch 71 batch loss 1.44161499 epoch total loss 1.31957185\n",
      "Trained batch 72 batch loss 1.29336655 epoch total loss 1.31920791\n",
      "Trained batch 73 batch loss 1.30481553 epoch total loss 1.31901085\n",
      "Trained batch 74 batch loss 1.36555517 epoch total loss 1.3196398\n",
      "Trained batch 75 batch loss 1.31876659 epoch total loss 1.31962812\n",
      "Trained batch 76 batch loss 1.30004048 epoch total loss 1.31937039\n",
      "Trained batch 77 batch loss 1.29187703 epoch total loss 1.31901336\n",
      "Trained batch 78 batch loss 1.33623016 epoch total loss 1.31923401\n",
      "Trained batch 79 batch loss 1.32855344 epoch total loss 1.31935191\n",
      "Trained batch 80 batch loss 1.17631924 epoch total loss 1.31756413\n",
      "Trained batch 81 batch loss 1.16608691 epoch total loss 1.31569397\n",
      "Trained batch 82 batch loss 1.11138391 epoch total loss 1.31320238\n",
      "Trained batch 83 batch loss 1.24695182 epoch total loss 1.31240416\n",
      "Trained batch 84 batch loss 1.26178229 epoch total loss 1.31180143\n",
      "Trained batch 85 batch loss 1.22197592 epoch total loss 1.31074464\n",
      "Trained batch 86 batch loss 1.13094509 epoch total loss 1.30865395\n",
      "Trained batch 87 batch loss 1.23978567 epoch total loss 1.3078624\n",
      "Trained batch 88 batch loss 1.32510614 epoch total loss 1.30805826\n",
      "Trained batch 89 batch loss 1.25330758 epoch total loss 1.30744314\n",
      "Trained batch 90 batch loss 1.24862623 epoch total loss 1.30678964\n",
      "Trained batch 91 batch loss 1.26429558 epoch total loss 1.30632269\n",
      "Trained batch 92 batch loss 1.27374518 epoch total loss 1.30596852\n",
      "Trained batch 93 batch loss 1.30254054 epoch total loss 1.30593181\n",
      "Trained batch 94 batch loss 1.31896329 epoch total loss 1.30607033\n",
      "Trained batch 95 batch loss 1.33986366 epoch total loss 1.30642617\n",
      "Trained batch 96 batch loss 1.37113762 epoch total loss 1.30710018\n",
      "Trained batch 97 batch loss 1.34109402 epoch total loss 1.30745065\n",
      "Trained batch 98 batch loss 1.33858228 epoch total loss 1.30776834\n",
      "Trained batch 99 batch loss 1.34191537 epoch total loss 1.30811334\n",
      "Trained batch 100 batch loss 1.45668745 epoch total loss 1.30959904\n",
      "Trained batch 101 batch loss 1.4064554 epoch total loss 1.31055796\n",
      "Trained batch 102 batch loss 1.22619748 epoch total loss 1.30973089\n",
      "Trained batch 103 batch loss 1.22718835 epoch total loss 1.30892944\n",
      "Trained batch 104 batch loss 1.36041617 epoch total loss 1.30942452\n",
      "Trained batch 105 batch loss 1.16438925 epoch total loss 1.30804312\n",
      "Trained batch 106 batch loss 1.14733493 epoch total loss 1.30652702\n",
      "Trained batch 107 batch loss 1.33210826 epoch total loss 1.30676615\n",
      "Trained batch 108 batch loss 1.20379329 epoch total loss 1.30581272\n",
      "Trained batch 109 batch loss 1.27961826 epoch total loss 1.30557239\n",
      "Trained batch 110 batch loss 1.18954587 epoch total loss 1.30451763\n",
      "Trained batch 111 batch loss 1.21683657 epoch total loss 1.30372775\n",
      "Trained batch 112 batch loss 1.29092717 epoch total loss 1.30361342\n",
      "Trained batch 113 batch loss 1.30124581 epoch total loss 1.30359232\n",
      "Trained batch 114 batch loss 1.40397477 epoch total loss 1.30447292\n",
      "Trained batch 115 batch loss 1.35602725 epoch total loss 1.30492127\n",
      "Trained batch 116 batch loss 1.38937366 epoch total loss 1.30564928\n",
      "Trained batch 117 batch loss 1.46677411 epoch total loss 1.30702651\n",
      "Trained batch 118 batch loss 1.37932444 epoch total loss 1.30763912\n",
      "Trained batch 119 batch loss 1.39201307 epoch total loss 1.30834818\n",
      "Trained batch 120 batch loss 1.20711803 epoch total loss 1.30750465\n",
      "Trained batch 121 batch loss 1.32646537 epoch total loss 1.30766129\n",
      "Trained batch 122 batch loss 1.32644045 epoch total loss 1.30781531\n",
      "Trained batch 123 batch loss 1.24427795 epoch total loss 1.30729878\n",
      "Trained batch 124 batch loss 1.33420324 epoch total loss 1.30751562\n",
      "Trained batch 125 batch loss 1.27356219 epoch total loss 1.30724406\n",
      "Trained batch 126 batch loss 1.21259189 epoch total loss 1.30649281\n",
      "Trained batch 127 batch loss 1.23884225 epoch total loss 1.30596\n",
      "Trained batch 128 batch loss 1.21817017 epoch total loss 1.30527425\n",
      "Trained batch 129 batch loss 1.26231289 epoch total loss 1.30494118\n",
      "Trained batch 130 batch loss 1.25386524 epoch total loss 1.30454826\n",
      "Trained batch 131 batch loss 1.18361163 epoch total loss 1.30362511\n",
      "Trained batch 132 batch loss 1.14276385 epoch total loss 1.30240643\n",
      "Trained batch 133 batch loss 1.30978298 epoch total loss 1.30246186\n",
      "Trained batch 134 batch loss 1.28481841 epoch total loss 1.30233026\n",
      "Trained batch 135 batch loss 1.35820484 epoch total loss 1.30274415\n",
      "Trained batch 136 batch loss 1.28686965 epoch total loss 1.30262733\n",
      "Trained batch 137 batch loss 1.43067765 epoch total loss 1.30356205\n",
      "Trained batch 138 batch loss 1.400738 epoch total loss 1.30426621\n",
      "Trained batch 139 batch loss 1.31132281 epoch total loss 1.304317\n",
      "Trained batch 140 batch loss 1.28202081 epoch total loss 1.30415785\n",
      "Trained batch 141 batch loss 1.41424 epoch total loss 1.30493855\n",
      "Trained batch 142 batch loss 1.27872908 epoch total loss 1.30475402\n",
      "Trained batch 143 batch loss 1.27584195 epoch total loss 1.30455184\n",
      "Trained batch 144 batch loss 1.40052032 epoch total loss 1.30521834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 145 batch loss 1.42806292 epoch total loss 1.30606568\n",
      "Trained batch 146 batch loss 1.24621356 epoch total loss 1.30565572\n",
      "Trained batch 147 batch loss 1.37142682 epoch total loss 1.30610311\n",
      "Trained batch 148 batch loss 1.44992387 epoch total loss 1.3070749\n",
      "Trained batch 149 batch loss 1.51044941 epoch total loss 1.30843985\n",
      "Trained batch 150 batch loss 1.32575941 epoch total loss 1.30855525\n",
      "Trained batch 151 batch loss 1.29711604 epoch total loss 1.30847955\n",
      "Trained batch 152 batch loss 1.31117189 epoch total loss 1.30849731\n",
      "Trained batch 153 batch loss 1.17598 epoch total loss 1.30763113\n",
      "Trained batch 154 batch loss 1.20349383 epoch total loss 1.30695486\n",
      "Trained batch 155 batch loss 1.25089359 epoch total loss 1.3065933\n",
      "Trained batch 156 batch loss 1.25560951 epoch total loss 1.30626655\n",
      "Trained batch 157 batch loss 1.21273839 epoch total loss 1.30567074\n",
      "Trained batch 158 batch loss 1.28609014 epoch total loss 1.30554688\n",
      "Trained batch 159 batch loss 1.43250489 epoch total loss 1.30634534\n",
      "Trained batch 160 batch loss 1.29325223 epoch total loss 1.30626357\n",
      "Trained batch 161 batch loss 1.36237991 epoch total loss 1.30661213\n",
      "Trained batch 162 batch loss 1.3194716 epoch total loss 1.30669153\n",
      "Trained batch 163 batch loss 1.28203392 epoch total loss 1.30654013\n",
      "Trained batch 164 batch loss 1.16180313 epoch total loss 1.30565763\n",
      "Trained batch 165 batch loss 1.20163524 epoch total loss 1.30502713\n",
      "Trained batch 166 batch loss 1.2032479 epoch total loss 1.30441403\n",
      "Trained batch 167 batch loss 1.36260033 epoch total loss 1.30476248\n",
      "Trained batch 168 batch loss 1.3656565 epoch total loss 1.30512488\n",
      "Trained batch 169 batch loss 1.38098729 epoch total loss 1.30557382\n",
      "Trained batch 170 batch loss 1.28004456 epoch total loss 1.30542362\n",
      "Trained batch 171 batch loss 1.22771835 epoch total loss 1.30496919\n",
      "Trained batch 172 batch loss 1.29373634 epoch total loss 1.30490386\n",
      "Trained batch 173 batch loss 1.12208056 epoch total loss 1.30384707\n",
      "Trained batch 174 batch loss 1.23986101 epoch total loss 1.30347943\n",
      "Trained batch 175 batch loss 1.264817 epoch total loss 1.30325854\n",
      "Trained batch 176 batch loss 1.12698984 epoch total loss 1.30225694\n",
      "Trained batch 177 batch loss 1.15138066 epoch total loss 1.30140448\n",
      "Trained batch 178 batch loss 1.07486737 epoch total loss 1.30013192\n",
      "Trained batch 179 batch loss 1.22093749 epoch total loss 1.29968941\n",
      "Trained batch 180 batch loss 1.31751823 epoch total loss 1.29978848\n",
      "Trained batch 181 batch loss 1.40986192 epoch total loss 1.30039668\n",
      "Trained batch 182 batch loss 1.36639476 epoch total loss 1.30075932\n",
      "Trained batch 183 batch loss 1.31716216 epoch total loss 1.30084896\n",
      "Trained batch 184 batch loss 1.28782177 epoch total loss 1.30077815\n",
      "Trained batch 185 batch loss 1.21231914 epoch total loss 1.30030012\n",
      "Trained batch 186 batch loss 1.32346261 epoch total loss 1.30042458\n",
      "Trained batch 187 batch loss 1.27561629 epoch total loss 1.3002919\n",
      "Trained batch 188 batch loss 1.36643076 epoch total loss 1.30064368\n",
      "Trained batch 189 batch loss 1.43968153 epoch total loss 1.30137932\n",
      "Trained batch 190 batch loss 1.47730947 epoch total loss 1.30230534\n",
      "Trained batch 191 batch loss 1.38363087 epoch total loss 1.30273116\n",
      "Trained batch 192 batch loss 1.34661579 epoch total loss 1.30295968\n",
      "Trained batch 193 batch loss 1.33003211 epoch total loss 1.3031\n",
      "Trained batch 194 batch loss 1.25278664 epoch total loss 1.30284059\n",
      "Trained batch 195 batch loss 1.15310669 epoch total loss 1.30207276\n",
      "Trained batch 196 batch loss 1.21433401 epoch total loss 1.30162513\n",
      "Trained batch 197 batch loss 1.41918051 epoch total loss 1.30222189\n",
      "Trained batch 198 batch loss 1.48980224 epoch total loss 1.30316937\n",
      "Trained batch 199 batch loss 1.38270247 epoch total loss 1.30356896\n",
      "Trained batch 200 batch loss 1.42086482 epoch total loss 1.30415547\n",
      "Trained batch 201 batch loss 1.34809077 epoch total loss 1.30437398\n",
      "Trained batch 202 batch loss 1.30987871 epoch total loss 1.30440116\n",
      "Trained batch 203 batch loss 1.23700058 epoch total loss 1.30406916\n",
      "Trained batch 204 batch loss 1.3019377 epoch total loss 1.30405879\n",
      "Trained batch 205 batch loss 1.31695223 epoch total loss 1.30412161\n",
      "Trained batch 206 batch loss 1.3093313 epoch total loss 1.30414689\n",
      "Trained batch 207 batch loss 1.25871587 epoch total loss 1.30392754\n",
      "Trained batch 208 batch loss 1.14945555 epoch total loss 1.30318475\n",
      "Trained batch 209 batch loss 1.14673805 epoch total loss 1.30243623\n",
      "Trained batch 210 batch loss 1.16159964 epoch total loss 1.30176556\n",
      "Trained batch 211 batch loss 1.44075394 epoch total loss 1.30242431\n",
      "Trained batch 212 batch loss 1.6208241 epoch total loss 1.30392611\n",
      "Trained batch 213 batch loss 1.57433975 epoch total loss 1.30519569\n",
      "Trained batch 214 batch loss 1.44005227 epoch total loss 1.30582595\n",
      "Trained batch 215 batch loss 1.42504644 epoch total loss 1.30638039\n",
      "Trained batch 216 batch loss 1.44993174 epoch total loss 1.30704498\n",
      "Trained batch 217 batch loss 1.3897897 epoch total loss 1.30742633\n",
      "Trained batch 218 batch loss 1.35534608 epoch total loss 1.30764616\n",
      "Trained batch 219 batch loss 1.3989222 epoch total loss 1.30806291\n",
      "Trained batch 220 batch loss 1.26106977 epoch total loss 1.30784941\n",
      "Trained batch 221 batch loss 1.35381806 epoch total loss 1.30805743\n",
      "Trained batch 222 batch loss 1.40123582 epoch total loss 1.30847716\n",
      "Trained batch 223 batch loss 1.31089735 epoch total loss 1.30848801\n",
      "Trained batch 224 batch loss 1.30144334 epoch total loss 1.30845654\n",
      "Trained batch 225 batch loss 1.24759221 epoch total loss 1.30818605\n",
      "Trained batch 226 batch loss 1.31104136 epoch total loss 1.30819869\n",
      "Trained batch 227 batch loss 1.21061122 epoch total loss 1.3077687\n",
      "Trained batch 228 batch loss 1.24826992 epoch total loss 1.30750775\n",
      "Trained batch 229 batch loss 1.25661635 epoch total loss 1.30728555\n",
      "Trained batch 230 batch loss 1.45938778 epoch total loss 1.3079468\n",
      "Trained batch 231 batch loss 1.4927578 epoch total loss 1.30874681\n",
      "Trained batch 232 batch loss 1.38695741 epoch total loss 1.30908406\n",
      "Trained batch 233 batch loss 1.46482897 epoch total loss 1.30975246\n",
      "Trained batch 234 batch loss 1.364609 epoch total loss 1.30998695\n",
      "Trained batch 235 batch loss 1.16564941 epoch total loss 1.30937278\n",
      "Trained batch 236 batch loss 1.11739373 epoch total loss 1.3085593\n",
      "Trained batch 237 batch loss 1.04567611 epoch total loss 1.30745018\n",
      "Trained batch 238 batch loss 1.06383932 epoch total loss 1.30642664\n",
      "Trained batch 239 batch loss 1.17265081 epoch total loss 1.30586684\n",
      "Trained batch 240 batch loss 1.19981992 epoch total loss 1.30542505\n",
      "Trained batch 241 batch loss 1.2157681 epoch total loss 1.305053\n",
      "Trained batch 242 batch loss 1.13171089 epoch total loss 1.30433667\n",
      "Trained batch 243 batch loss 1.48019266 epoch total loss 1.30506039\n",
      "Trained batch 244 batch loss 1.30640852 epoch total loss 1.30506587\n",
      "Trained batch 245 batch loss 1.36470938 epoch total loss 1.3053093\n",
      "Trained batch 246 batch loss 1.3923316 epoch total loss 1.30566311\n",
      "Trained batch 247 batch loss 1.23618674 epoch total loss 1.30538177\n",
      "Trained batch 248 batch loss 1.39607716 epoch total loss 1.30574751\n",
      "Trained batch 249 batch loss 1.35562396 epoch total loss 1.30594778\n",
      "Trained batch 250 batch loss 1.29932165 epoch total loss 1.30592132\n",
      "Trained batch 251 batch loss 1.27635837 epoch total loss 1.30580354\n",
      "Trained batch 252 batch loss 1.34098077 epoch total loss 1.30594313\n",
      "Trained batch 253 batch loss 1.20536768 epoch total loss 1.30554545\n",
      "Trained batch 254 batch loss 1.19377494 epoch total loss 1.30510545\n",
      "Trained batch 255 batch loss 1.34205413 epoch total loss 1.30525029\n",
      "Trained batch 256 batch loss 1.28981853 epoch total loss 1.30519009\n",
      "Trained batch 257 batch loss 1.30203211 epoch total loss 1.30517781\n",
      "Trained batch 258 batch loss 1.19443059 epoch total loss 1.30474854\n",
      "Trained batch 259 batch loss 1.26118457 epoch total loss 1.30458033\n",
      "Trained batch 260 batch loss 1.25411248 epoch total loss 1.30438614\n",
      "Trained batch 261 batch loss 1.23769546 epoch total loss 1.30413067\n",
      "Trained batch 262 batch loss 1.31082869 epoch total loss 1.30415618\n",
      "Trained batch 263 batch loss 1.3676281 epoch total loss 1.30439758\n",
      "Trained batch 264 batch loss 1.23829544 epoch total loss 1.30414712\n",
      "Trained batch 265 batch loss 1.33737922 epoch total loss 1.30427241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 266 batch loss 1.35415757 epoch total loss 1.30446\n",
      "Trained batch 267 batch loss 1.3360492 epoch total loss 1.3045783\n",
      "Trained batch 268 batch loss 1.31102371 epoch total loss 1.30460238\n",
      "Trained batch 269 batch loss 1.23344314 epoch total loss 1.30433786\n",
      "Trained batch 270 batch loss 1.15775323 epoch total loss 1.30379486\n",
      "Trained batch 271 batch loss 1.39731216 epoch total loss 1.30414\n",
      "Trained batch 272 batch loss 1.40525937 epoch total loss 1.30451179\n",
      "Trained batch 273 batch loss 1.36441505 epoch total loss 1.30473125\n",
      "Trained batch 274 batch loss 1.29287219 epoch total loss 1.30468798\n",
      "Trained batch 275 batch loss 1.28885221 epoch total loss 1.3046304\n",
      "Trained batch 276 batch loss 1.31019092 epoch total loss 1.30465043\n",
      "Trained batch 277 batch loss 1.18520594 epoch total loss 1.30421925\n",
      "Trained batch 278 batch loss 1.13955402 epoch total loss 1.30362701\n",
      "Trained batch 279 batch loss 1.32814956 epoch total loss 1.30371487\n",
      "Trained batch 280 batch loss 1.37467182 epoch total loss 1.30396831\n",
      "Trained batch 281 batch loss 1.33709252 epoch total loss 1.30408621\n",
      "Trained batch 282 batch loss 1.28403199 epoch total loss 1.30401504\n",
      "Trained batch 283 batch loss 1.36062431 epoch total loss 1.30421507\n",
      "Trained batch 284 batch loss 1.35241008 epoch total loss 1.30438483\n",
      "Trained batch 285 batch loss 1.27001119 epoch total loss 1.30426419\n",
      "Trained batch 286 batch loss 1.30665791 epoch total loss 1.30427265\n",
      "Trained batch 287 batch loss 1.64452481 epoch total loss 1.30545819\n",
      "Trained batch 288 batch loss 1.43251348 epoch total loss 1.30589938\n",
      "Trained batch 289 batch loss 1.29462183 epoch total loss 1.3058604\n",
      "Trained batch 290 batch loss 1.29478478 epoch total loss 1.30582225\n",
      "Trained batch 291 batch loss 1.15794981 epoch total loss 1.30531406\n",
      "Trained batch 292 batch loss 1.05763912 epoch total loss 1.30446589\n",
      "Trained batch 293 batch loss 1.19075096 epoch total loss 1.30407786\n",
      "Trained batch 294 batch loss 1.20296431 epoch total loss 1.30373394\n",
      "Trained batch 295 batch loss 1.0271641 epoch total loss 1.30279648\n",
      "Trained batch 296 batch loss 0.984056711 epoch total loss 1.30171967\n",
      "Trained batch 297 batch loss 1.15164912 epoch total loss 1.30121434\n",
      "Trained batch 298 batch loss 1.17117119 epoch total loss 1.30077803\n",
      "Trained batch 299 batch loss 1.13218808 epoch total loss 1.30021417\n",
      "Trained batch 300 batch loss 1.28483701 epoch total loss 1.30016291\n",
      "Trained batch 301 batch loss 1.29060006 epoch total loss 1.3001312\n",
      "Trained batch 302 batch loss 1.34750319 epoch total loss 1.30028796\n",
      "Trained batch 303 batch loss 1.3238163 epoch total loss 1.30036569\n",
      "Trained batch 304 batch loss 1.2483536 epoch total loss 1.30019462\n",
      "Trained batch 305 batch loss 1.32843673 epoch total loss 1.30028713\n",
      "Trained batch 306 batch loss 1.3047092 epoch total loss 1.30030167\n",
      "Trained batch 307 batch loss 1.57529747 epoch total loss 1.30119741\n",
      "Trained batch 308 batch loss 1.43729198 epoch total loss 1.3016392\n",
      "Trained batch 309 batch loss 1.45422387 epoch total loss 1.30213296\n",
      "Trained batch 310 batch loss 1.47832584 epoch total loss 1.30270135\n",
      "Trained batch 311 batch loss 1.31823504 epoch total loss 1.3027513\n",
      "Trained batch 312 batch loss 1.29190898 epoch total loss 1.30271661\n",
      "Trained batch 313 batch loss 1.23568439 epoch total loss 1.30250239\n",
      "Trained batch 314 batch loss 1.23662674 epoch total loss 1.3022927\n",
      "Trained batch 315 batch loss 1.34834909 epoch total loss 1.30243886\n",
      "Trained batch 316 batch loss 1.37344837 epoch total loss 1.30266356\n",
      "Trained batch 317 batch loss 1.31408584 epoch total loss 1.30269957\n",
      "Trained batch 318 batch loss 1.2632612 epoch total loss 1.30257559\n",
      "Trained batch 319 batch loss 1.25238085 epoch total loss 1.30241835\n",
      "Trained batch 320 batch loss 1.18342626 epoch total loss 1.30204654\n",
      "Trained batch 321 batch loss 1.2354151 epoch total loss 1.30183887\n",
      "Trained batch 322 batch loss 1.2941308 epoch total loss 1.30181491\n",
      "Trained batch 323 batch loss 1.26338518 epoch total loss 1.30169606\n",
      "Trained batch 324 batch loss 1.19712675 epoch total loss 1.30137324\n",
      "Trained batch 325 batch loss 1.29098082 epoch total loss 1.3013413\n",
      "Trained batch 326 batch loss 1.33772969 epoch total loss 1.30145288\n",
      "Trained batch 327 batch loss 1.28419852 epoch total loss 1.30140018\n",
      "Trained batch 328 batch loss 1.34153485 epoch total loss 1.30152249\n",
      "Trained batch 329 batch loss 1.32273972 epoch total loss 1.30158699\n",
      "Trained batch 330 batch loss 1.37606335 epoch total loss 1.30181277\n",
      "Trained batch 331 batch loss 1.265733 epoch total loss 1.30170381\n",
      "Trained batch 332 batch loss 1.21765339 epoch total loss 1.30145061\n",
      "Trained batch 333 batch loss 1.25276518 epoch total loss 1.30130446\n",
      "Trained batch 334 batch loss 1.18652892 epoch total loss 1.30096078\n",
      "Trained batch 335 batch loss 1.14632225 epoch total loss 1.3004992\n",
      "Trained batch 336 batch loss 1.20083976 epoch total loss 1.30020261\n",
      "Trained batch 337 batch loss 1.22060966 epoch total loss 1.29996645\n",
      "Trained batch 338 batch loss 1.18857801 epoch total loss 1.29963684\n",
      "Trained batch 339 batch loss 1.24484253 epoch total loss 1.29947519\n",
      "Trained batch 340 batch loss 1.18707263 epoch total loss 1.29914463\n",
      "Trained batch 341 batch loss 1.30765152 epoch total loss 1.29916954\n",
      "Trained batch 342 batch loss 1.1073277 epoch total loss 1.29860866\n",
      "Trained batch 343 batch loss 1.23698413 epoch total loss 1.29842889\n",
      "Trained batch 344 batch loss 1.39080501 epoch total loss 1.29869747\n",
      "Trained batch 345 batch loss 1.30825424 epoch total loss 1.29872513\n",
      "Trained batch 346 batch loss 1.28517413 epoch total loss 1.29868603\n",
      "Trained batch 347 batch loss 1.25714695 epoch total loss 1.29856634\n",
      "Trained batch 348 batch loss 1.32036543 epoch total loss 1.29862893\n",
      "Trained batch 349 batch loss 1.32678676 epoch total loss 1.29870963\n",
      "Trained batch 350 batch loss 1.33922601 epoch total loss 1.29882538\n",
      "Trained batch 351 batch loss 1.31332541 epoch total loss 1.29886675\n",
      "Trained batch 352 batch loss 1.30356622 epoch total loss 1.2988801\n",
      "Trained batch 353 batch loss 1.28033531 epoch total loss 1.29882753\n",
      "Trained batch 354 batch loss 1.20526028 epoch total loss 1.29856324\n",
      "Trained batch 355 batch loss 1.17212629 epoch total loss 1.29820704\n",
      "Trained batch 356 batch loss 1.36119795 epoch total loss 1.29838395\n",
      "Trained batch 357 batch loss 1.23712206 epoch total loss 1.29821241\n",
      "Trained batch 358 batch loss 1.30117774 epoch total loss 1.29822063\n",
      "Trained batch 359 batch loss 1.24224305 epoch total loss 1.29806471\n",
      "Trained batch 360 batch loss 1.22265124 epoch total loss 1.29785526\n",
      "Trained batch 361 batch loss 1.25516808 epoch total loss 1.297737\n",
      "Trained batch 362 batch loss 1.29128325 epoch total loss 1.29771924\n",
      "Trained batch 363 batch loss 1.25629508 epoch total loss 1.29760504\n",
      "Trained batch 364 batch loss 1.10791624 epoch total loss 1.29708397\n",
      "Trained batch 365 batch loss 1.36031961 epoch total loss 1.29725718\n",
      "Trained batch 366 batch loss 1.21498609 epoch total loss 1.29703248\n",
      "Trained batch 367 batch loss 1.30821216 epoch total loss 1.29706287\n",
      "Trained batch 368 batch loss 1.40001 epoch total loss 1.29734266\n",
      "Trained batch 369 batch loss 1.35681033 epoch total loss 1.29750383\n",
      "Trained batch 370 batch loss 1.34335935 epoch total loss 1.29762769\n",
      "Trained batch 371 batch loss 1.28236532 epoch total loss 1.29758656\n",
      "Trained batch 372 batch loss 1.27946591 epoch total loss 1.29753792\n",
      "Trained batch 373 batch loss 1.2830143 epoch total loss 1.29749906\n",
      "Trained batch 374 batch loss 1.31308985 epoch total loss 1.29754066\n",
      "Trained batch 375 batch loss 1.3053875 epoch total loss 1.29756165\n",
      "Trained batch 376 batch loss 1.34408975 epoch total loss 1.29768538\n",
      "Trained batch 377 batch loss 1.34781826 epoch total loss 1.2978183\n",
      "Trained batch 378 batch loss 1.34579563 epoch total loss 1.29794526\n",
      "Trained batch 379 batch loss 1.43977845 epoch total loss 1.29831946\n",
      "Trained batch 380 batch loss 1.43317556 epoch total loss 1.29867435\n",
      "Trained batch 381 batch loss 1.34286308 epoch total loss 1.29879034\n",
      "Trained batch 382 batch loss 1.19097543 epoch total loss 1.29850805\n",
      "Trained batch 383 batch loss 1.30364645 epoch total loss 1.29852152\n",
      "Trained batch 384 batch loss 1.27754581 epoch total loss 1.29846692\n",
      "Trained batch 385 batch loss 1.2759192 epoch total loss 1.29840839\n",
      "Trained batch 386 batch loss 1.30882072 epoch total loss 1.29843521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 387 batch loss 1.27260137 epoch total loss 1.29836857\n",
      "Trained batch 388 batch loss 1.31262255 epoch total loss 1.29840529\n",
      "Trained batch 389 batch loss 1.31324494 epoch total loss 1.29844344\n",
      "Trained batch 390 batch loss 1.30235314 epoch total loss 1.29845345\n",
      "Trained batch 391 batch loss 1.27150178 epoch total loss 1.29838455\n",
      "Trained batch 392 batch loss 1.24027848 epoch total loss 1.29823625\n",
      "Trained batch 393 batch loss 1.31717443 epoch total loss 1.29828453\n",
      "Trained batch 394 batch loss 1.09754443 epoch total loss 1.29777491\n",
      "Trained batch 395 batch loss 1.04266882 epoch total loss 1.29712915\n",
      "Trained batch 396 batch loss 1.25432754 epoch total loss 1.29702115\n",
      "Trained batch 397 batch loss 1.77813148 epoch total loss 1.29823303\n",
      "Trained batch 398 batch loss 1.31257665 epoch total loss 1.29826903\n",
      "Trained batch 399 batch loss 1.2713778 epoch total loss 1.29820156\n",
      "Trained batch 400 batch loss 1.23412311 epoch total loss 1.29804134\n",
      "Trained batch 401 batch loss 1.33307528 epoch total loss 1.29812872\n",
      "Trained batch 402 batch loss 1.24885106 epoch total loss 1.29800618\n",
      "Trained batch 403 batch loss 1.24616754 epoch total loss 1.29787743\n",
      "Trained batch 404 batch loss 1.21187806 epoch total loss 1.29766452\n",
      "Trained batch 405 batch loss 1.25892985 epoch total loss 1.2975688\n",
      "Trained batch 406 batch loss 1.3374517 epoch total loss 1.29766715\n",
      "Trained batch 407 batch loss 1.33836246 epoch total loss 1.29776716\n",
      "Trained batch 408 batch loss 1.32503653 epoch total loss 1.29783392\n",
      "Trained batch 409 batch loss 1.21821475 epoch total loss 1.29763925\n",
      "Trained batch 410 batch loss 1.24209547 epoch total loss 1.29750371\n",
      "Trained batch 411 batch loss 1.18684161 epoch total loss 1.29723442\n",
      "Trained batch 412 batch loss 1.28118277 epoch total loss 1.29719543\n",
      "Trained batch 413 batch loss 1.29224801 epoch total loss 1.29718339\n",
      "Trained batch 414 batch loss 1.3122077 epoch total loss 1.29721963\n",
      "Trained batch 415 batch loss 1.3309232 epoch total loss 1.29730093\n",
      "Trained batch 416 batch loss 1.26812708 epoch total loss 1.29723084\n",
      "Trained batch 417 batch loss 1.42359257 epoch total loss 1.29753375\n",
      "Trained batch 418 batch loss 1.41219759 epoch total loss 1.29780805\n",
      "Trained batch 419 batch loss 1.3123157 epoch total loss 1.29784262\n",
      "Trained batch 420 batch loss 1.32552218 epoch total loss 1.29790854\n",
      "Trained batch 421 batch loss 1.45861816 epoch total loss 1.29829025\n",
      "Trained batch 422 batch loss 1.42208755 epoch total loss 1.29858351\n",
      "Trained batch 423 batch loss 1.27018929 epoch total loss 1.29851651\n",
      "Trained batch 424 batch loss 1.26911712 epoch total loss 1.29844713\n",
      "Trained batch 425 batch loss 1.29118109 epoch total loss 1.29843009\n",
      "Trained batch 426 batch loss 1.23286462 epoch total loss 1.29827607\n",
      "Trained batch 427 batch loss 1.28865921 epoch total loss 1.29825354\n",
      "Trained batch 428 batch loss 1.3357023 epoch total loss 1.29834104\n",
      "Trained batch 429 batch loss 1.34672964 epoch total loss 1.29845381\n",
      "Trained batch 430 batch loss 1.19597411 epoch total loss 1.29821551\n",
      "Trained batch 431 batch loss 1.30514479 epoch total loss 1.29823148\n",
      "Trained batch 432 batch loss 1.22921324 epoch total loss 1.29807174\n",
      "Trained batch 433 batch loss 1.24766612 epoch total loss 1.29795527\n",
      "Trained batch 434 batch loss 1.2388643 epoch total loss 1.29781926\n",
      "Trained batch 435 batch loss 1.33540702 epoch total loss 1.29790556\n",
      "Trained batch 436 batch loss 1.27794623 epoch total loss 1.29785979\n",
      "Trained batch 437 batch loss 1.21756947 epoch total loss 1.29767609\n",
      "Trained batch 438 batch loss 1.21369064 epoch total loss 1.2974844\n",
      "Trained batch 439 batch loss 1.11634088 epoch total loss 1.2970717\n",
      "Trained batch 440 batch loss 1.34578097 epoch total loss 1.29718244\n",
      "Trained batch 441 batch loss 1.13480544 epoch total loss 1.2968142\n",
      "Trained batch 442 batch loss 1.1877979 epoch total loss 1.29656756\n",
      "Trained batch 443 batch loss 1.16537571 epoch total loss 1.29627156\n",
      "Trained batch 444 batch loss 1.32359838 epoch total loss 1.29633307\n",
      "Trained batch 445 batch loss 1.14642477 epoch total loss 1.29599619\n",
      "Trained batch 446 batch loss 1.1650387 epoch total loss 1.29570258\n",
      "Trained batch 447 batch loss 1.2926743 epoch total loss 1.29569578\n",
      "Trained batch 448 batch loss 1.35809124 epoch total loss 1.29583514\n",
      "Trained batch 449 batch loss 1.33874834 epoch total loss 1.29593062\n",
      "Trained batch 450 batch loss 1.362149 epoch total loss 1.29607773\n",
      "Trained batch 451 batch loss 1.33798301 epoch total loss 1.29617071\n",
      "Trained batch 452 batch loss 1.29960954 epoch total loss 1.29617834\n",
      "Trained batch 453 batch loss 1.32033479 epoch total loss 1.29623163\n",
      "Trained batch 454 batch loss 1.21225333 epoch total loss 1.29604673\n",
      "Trained batch 455 batch loss 1.22345233 epoch total loss 1.29588711\n",
      "Trained batch 456 batch loss 1.25556517 epoch total loss 1.29579878\n",
      "Trained batch 457 batch loss 1.24392414 epoch total loss 1.29568517\n",
      "Trained batch 458 batch loss 1.12993598 epoch total loss 1.29532325\n",
      "Trained batch 459 batch loss 1.06234252 epoch total loss 1.29481566\n",
      "Trained batch 460 batch loss 1.13762248 epoch total loss 1.29447389\n",
      "Trained batch 461 batch loss 1.28960562 epoch total loss 1.2944634\n",
      "Trained batch 462 batch loss 1.27788794 epoch total loss 1.29442751\n",
      "Trained batch 463 batch loss 1.27934039 epoch total loss 1.29439497\n",
      "Trained batch 464 batch loss 1.38662887 epoch total loss 1.29459381\n",
      "Trained batch 465 batch loss 1.39436483 epoch total loss 1.29480839\n",
      "Trained batch 466 batch loss 1.24279606 epoch total loss 1.29469669\n",
      "Trained batch 467 batch loss 1.19396293 epoch total loss 1.29448104\n",
      "Trained batch 468 batch loss 1.29219401 epoch total loss 1.29447615\n",
      "Trained batch 469 batch loss 1.27122819 epoch total loss 1.29442656\n",
      "Trained batch 470 batch loss 1.29008329 epoch total loss 1.29441738\n",
      "Trained batch 471 batch loss 0.945669949 epoch total loss 1.29367697\n",
      "Trained batch 472 batch loss 0.987312555 epoch total loss 1.29302788\n",
      "Trained batch 473 batch loss 1.21473098 epoch total loss 1.2928623\n",
      "Trained batch 474 batch loss 1.34264696 epoch total loss 1.29296732\n",
      "Trained batch 475 batch loss 1.51562333 epoch total loss 1.29343605\n",
      "Trained batch 476 batch loss 1.47104931 epoch total loss 1.29380929\n",
      "Trained batch 477 batch loss 1.39853823 epoch total loss 1.29402888\n",
      "Trained batch 478 batch loss 1.31045091 epoch total loss 1.29406321\n",
      "Trained batch 479 batch loss 1.30563569 epoch total loss 1.29408741\n",
      "Trained batch 480 batch loss 1.3881433 epoch total loss 1.29428327\n",
      "Trained batch 481 batch loss 1.30769396 epoch total loss 1.29431117\n",
      "Trained batch 482 batch loss 1.28443694 epoch total loss 1.29429066\n",
      "Trained batch 483 batch loss 1.33627117 epoch total loss 1.29437745\n",
      "Trained batch 484 batch loss 1.35397923 epoch total loss 1.29450071\n",
      "Trained batch 485 batch loss 1.37041545 epoch total loss 1.29465723\n",
      "Trained batch 486 batch loss 1.39923775 epoch total loss 1.2948724\n",
      "Trained batch 487 batch loss 1.16174591 epoch total loss 1.29459906\n",
      "Trained batch 488 batch loss 1.34446633 epoch total loss 1.29470122\n",
      "Trained batch 489 batch loss 1.39103961 epoch total loss 1.29489827\n",
      "Trained batch 490 batch loss 1.42102087 epoch total loss 1.29515564\n",
      "Trained batch 491 batch loss 1.327806 epoch total loss 1.29522216\n",
      "Trained batch 492 batch loss 1.23730874 epoch total loss 1.2951045\n",
      "Trained batch 493 batch loss 1.1651206 epoch total loss 1.29484081\n",
      "Trained batch 494 batch loss 1.15842247 epoch total loss 1.29456472\n",
      "Trained batch 495 batch loss 1.31224453 epoch total loss 1.29460037\n",
      "Trained batch 496 batch loss 1.21936536 epoch total loss 1.29444873\n",
      "Trained batch 497 batch loss 1.32911134 epoch total loss 1.29451847\n",
      "Trained batch 498 batch loss 1.37485206 epoch total loss 1.29467976\n",
      "Trained batch 499 batch loss 1.30420578 epoch total loss 1.29469895\n",
      "Trained batch 500 batch loss 1.2343564 epoch total loss 1.29457819\n",
      "Trained batch 501 batch loss 1.41787195 epoch total loss 1.29482424\n",
      "Trained batch 502 batch loss 1.47724581 epoch total loss 1.29518771\n",
      "Trained batch 503 batch loss 1.22244692 epoch total loss 1.29504311\n",
      "Trained batch 504 batch loss 1.17839551 epoch total loss 1.29481173\n",
      "Trained batch 505 batch loss 1.18807793 epoch total loss 1.29460025\n",
      "Trained batch 506 batch loss 1.25333011 epoch total loss 1.29451871\n",
      "Trained batch 507 batch loss 1.35137749 epoch total loss 1.29463089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 508 batch loss 1.33524716 epoch total loss 1.29471087\n",
      "Trained batch 509 batch loss 1.29406047 epoch total loss 1.29470968\n",
      "Trained batch 510 batch loss 1.29028749 epoch total loss 1.29470098\n",
      "Trained batch 511 batch loss 1.30119061 epoch total loss 1.29471374\n",
      "Trained batch 512 batch loss 1.16548896 epoch total loss 1.29446125\n",
      "Trained batch 513 batch loss 1.32065344 epoch total loss 1.29451239\n",
      "Trained batch 514 batch loss 1.32930386 epoch total loss 1.29458\n",
      "Trained batch 515 batch loss 1.47477281 epoch total loss 1.29493\n",
      "Trained batch 516 batch loss 1.37344337 epoch total loss 1.29508209\n",
      "Trained batch 517 batch loss 1.22746098 epoch total loss 1.29495132\n",
      "Trained batch 518 batch loss 1.28542387 epoch total loss 1.29493284\n",
      "Trained batch 519 batch loss 1.19612825 epoch total loss 1.29474247\n",
      "Trained batch 520 batch loss 1.43644786 epoch total loss 1.29501498\n",
      "Trained batch 521 batch loss 1.27653623 epoch total loss 1.29497957\n",
      "Trained batch 522 batch loss 1.39802063 epoch total loss 1.29517686\n",
      "Trained batch 523 batch loss 1.33168173 epoch total loss 1.2952466\n",
      "Trained batch 524 batch loss 1.2860775 epoch total loss 1.2952292\n",
      "Trained batch 525 batch loss 1.3966738 epoch total loss 1.29542232\n",
      "Trained batch 526 batch loss 1.32707608 epoch total loss 1.29548252\n",
      "Trained batch 527 batch loss 1.34139466 epoch total loss 1.29556966\n",
      "Trained batch 528 batch loss 1.37390172 epoch total loss 1.29571795\n",
      "Trained batch 529 batch loss 1.3079505 epoch total loss 1.29574108\n",
      "Trained batch 530 batch loss 1.26495492 epoch total loss 1.29568303\n",
      "Trained batch 531 batch loss 1.21074235 epoch total loss 1.29552305\n",
      "Trained batch 532 batch loss 1.27803636 epoch total loss 1.29549015\n",
      "Trained batch 533 batch loss 1.3320148 epoch total loss 1.29555869\n",
      "Trained batch 534 batch loss 1.25906193 epoch total loss 1.29549026\n",
      "Trained batch 535 batch loss 1.16282511 epoch total loss 1.29524231\n",
      "Trained batch 536 batch loss 1.13812375 epoch total loss 1.29494917\n",
      "Trained batch 537 batch loss 1.24338317 epoch total loss 1.29485321\n",
      "Trained batch 538 batch loss 1.29662824 epoch total loss 1.29485655\n",
      "Trained batch 539 batch loss 1.26702881 epoch total loss 1.29480493\n",
      "Trained batch 540 batch loss 1.27949381 epoch total loss 1.29477656\n",
      "Trained batch 541 batch loss 1.30874288 epoch total loss 1.29480231\n",
      "Trained batch 542 batch loss 1.35688698 epoch total loss 1.29491675\n",
      "Trained batch 543 batch loss 1.30641758 epoch total loss 1.29493797\n",
      "Trained batch 544 batch loss 1.25167072 epoch total loss 1.29485834\n",
      "Trained batch 545 batch loss 1.22446704 epoch total loss 1.29472923\n",
      "Trained batch 546 batch loss 1.2377131 epoch total loss 1.29462481\n",
      "Trained batch 547 batch loss 1.21127772 epoch total loss 1.29447258\n",
      "Trained batch 548 batch loss 1.29246843 epoch total loss 1.29446888\n",
      "Trained batch 549 batch loss 1.24778211 epoch total loss 1.29438388\n",
      "Trained batch 550 batch loss 1.26357257 epoch total loss 1.29432786\n",
      "Trained batch 551 batch loss 1.19644797 epoch total loss 1.29415023\n",
      "Trained batch 552 batch loss 0.931001544 epoch total loss 1.29349244\n",
      "Trained batch 553 batch loss 1.13787985 epoch total loss 1.29321098\n",
      "Trained batch 554 batch loss 1.28884053 epoch total loss 1.29320312\n",
      "Trained batch 555 batch loss 1.32938504 epoch total loss 1.29326832\n",
      "Trained batch 556 batch loss 1.30290568 epoch total loss 1.29328573\n",
      "Trained batch 557 batch loss 1.33173645 epoch total loss 1.29335463\n",
      "Trained batch 558 batch loss 1.26374626 epoch total loss 1.29330158\n",
      "Trained batch 559 batch loss 1.19931722 epoch total loss 1.2931335\n",
      "Trained batch 560 batch loss 1.18120241 epoch total loss 1.2929337\n",
      "Trained batch 561 batch loss 1.16190255 epoch total loss 1.29270017\n",
      "Trained batch 562 batch loss 1.2422961 epoch total loss 1.29261053\n",
      "Trained batch 563 batch loss 1.33155537 epoch total loss 1.29267967\n",
      "Trained batch 564 batch loss 1.21770334 epoch total loss 1.29254675\n",
      "Trained batch 565 batch loss 1.27306604 epoch total loss 1.29251218\n",
      "Trained batch 566 batch loss 1.22927082 epoch total loss 1.29240048\n",
      "Trained batch 567 batch loss 1.32619607 epoch total loss 1.29246\n",
      "Trained batch 568 batch loss 1.18788958 epoch total loss 1.29227591\n",
      "Trained batch 569 batch loss 1.22838986 epoch total loss 1.29216361\n",
      "Trained batch 570 batch loss 1.22955847 epoch total loss 1.2920537\n",
      "Trained batch 571 batch loss 1.28869176 epoch total loss 1.29204786\n",
      "Trained batch 572 batch loss 1.28717422 epoch total loss 1.29203939\n",
      "Trained batch 573 batch loss 1.24070311 epoch total loss 1.29194975\n",
      "Trained batch 574 batch loss 1.28307796 epoch total loss 1.29193437\n",
      "Trained batch 575 batch loss 1.42501879 epoch total loss 1.29216588\n",
      "Trained batch 576 batch loss 1.18291426 epoch total loss 1.29197621\n",
      "Trained batch 577 batch loss 1.29582167 epoch total loss 1.29198289\n",
      "Trained batch 578 batch loss 1.32499468 epoch total loss 1.29204\n",
      "Trained batch 579 batch loss 1.18906498 epoch total loss 1.29186225\n",
      "Trained batch 580 batch loss 1.45151973 epoch total loss 1.2921375\n",
      "Trained batch 581 batch loss 1.27978349 epoch total loss 1.29211628\n",
      "Trained batch 582 batch loss 1.32506585 epoch total loss 1.29217291\n",
      "Trained batch 583 batch loss 1.30573809 epoch total loss 1.29219615\n",
      "Trained batch 584 batch loss 1.1831429 epoch total loss 1.29200947\n",
      "Trained batch 585 batch loss 1.35397649 epoch total loss 1.29211545\n",
      "Trained batch 586 batch loss 1.22942722 epoch total loss 1.2920084\n",
      "Trained batch 587 batch loss 1.26038539 epoch total loss 1.29195452\n",
      "Trained batch 588 batch loss 1.24348664 epoch total loss 1.29187214\n",
      "Trained batch 589 batch loss 1.24374127 epoch total loss 1.29179037\n",
      "Trained batch 590 batch loss 1.21409023 epoch total loss 1.29165864\n",
      "Trained batch 591 batch loss 1.20796597 epoch total loss 1.29151702\n",
      "Trained batch 592 batch loss 1.25785542 epoch total loss 1.29146016\n",
      "Trained batch 593 batch loss 1.17037237 epoch total loss 1.29125595\n",
      "Trained batch 594 batch loss 1.17459834 epoch total loss 1.29105961\n",
      "Trained batch 595 batch loss 1.20048356 epoch total loss 1.29090738\n",
      "Trained batch 596 batch loss 1.24226809 epoch total loss 1.29082572\n",
      "Trained batch 597 batch loss 1.19769382 epoch total loss 1.2906698\n",
      "Trained batch 598 batch loss 1.05919945 epoch total loss 1.29028273\n",
      "Trained batch 599 batch loss 1.16269529 epoch total loss 1.2900697\n",
      "Trained batch 600 batch loss 1.35681033 epoch total loss 1.29018092\n",
      "Trained batch 601 batch loss 1.27665353 epoch total loss 1.29015851\n",
      "Trained batch 602 batch loss 1.25250101 epoch total loss 1.29009593\n",
      "Trained batch 603 batch loss 1.37234163 epoch total loss 1.2902323\n",
      "Trained batch 604 batch loss 1.3209312 epoch total loss 1.29028308\n",
      "Trained batch 605 batch loss 1.40897822 epoch total loss 1.2904793\n",
      "Trained batch 606 batch loss 1.34961116 epoch total loss 1.29057693\n",
      "Trained batch 607 batch loss 1.21522832 epoch total loss 1.29045272\n",
      "Trained batch 608 batch loss 1.26983321 epoch total loss 1.29041886\n",
      "Trained batch 609 batch loss 1.43220627 epoch total loss 1.29065156\n",
      "Trained batch 610 batch loss 1.4059757 epoch total loss 1.29084074\n",
      "Trained batch 611 batch loss 1.39904034 epoch total loss 1.29101777\n",
      "Trained batch 612 batch loss 1.3556577 epoch total loss 1.29112339\n",
      "Trained batch 613 batch loss 1.35838747 epoch total loss 1.29123318\n",
      "Trained batch 614 batch loss 1.37808251 epoch total loss 1.29137468\n",
      "Trained batch 615 batch loss 1.23750126 epoch total loss 1.29128706\n",
      "Trained batch 616 batch loss 1.26345098 epoch total loss 1.29124188\n",
      "Trained batch 617 batch loss 1.34158421 epoch total loss 1.29132342\n",
      "Trained batch 618 batch loss 1.27375627 epoch total loss 1.29129505\n",
      "Trained batch 619 batch loss 1.30528498 epoch total loss 1.2913177\n",
      "Trained batch 620 batch loss 1.31341887 epoch total loss 1.29135334\n",
      "Trained batch 621 batch loss 1.18501556 epoch total loss 1.29118204\n",
      "Trained batch 622 batch loss 1.25886822 epoch total loss 1.29113007\n",
      "Trained batch 623 batch loss 1.13817143 epoch total loss 1.29088449\n",
      "Trained batch 624 batch loss 1.10192537 epoch total loss 1.2905817\n",
      "Trained batch 625 batch loss 1.2694298 epoch total loss 1.29054785\n",
      "Trained batch 626 batch loss 1.35137534 epoch total loss 1.290645\n",
      "Trained batch 627 batch loss 1.37925255 epoch total loss 1.29078639\n",
      "Trained batch 628 batch loss 1.52075529 epoch total loss 1.2911526\n",
      "Trained batch 629 batch loss 1.52122426 epoch total loss 1.29151833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 630 batch loss 1.52031481 epoch total loss 1.29188156\n",
      "Trained batch 631 batch loss 1.48553848 epoch total loss 1.29218841\n",
      "Trained batch 632 batch loss 1.23877728 epoch total loss 1.29210389\n",
      "Trained batch 633 batch loss 1.25435305 epoch total loss 1.29204428\n",
      "Trained batch 634 batch loss 1.27998567 epoch total loss 1.29202521\n",
      "Trained batch 635 batch loss 1.27225101 epoch total loss 1.29199409\n",
      "Trained batch 636 batch loss 1.26919448 epoch total loss 1.29195821\n",
      "Trained batch 637 batch loss 1.32694256 epoch total loss 1.29201317\n",
      "Trained batch 638 batch loss 1.27788126 epoch total loss 1.291991\n",
      "Trained batch 639 batch loss 1.39475548 epoch total loss 1.29215193\n",
      "Trained batch 640 batch loss 1.50240707 epoch total loss 1.29248035\n",
      "Trained batch 641 batch loss 1.36161566 epoch total loss 1.29258823\n",
      "Trained batch 642 batch loss 1.31444216 epoch total loss 1.29262233\n",
      "Trained batch 643 batch loss 1.24081576 epoch total loss 1.29254174\n",
      "Trained batch 644 batch loss 1.22062135 epoch total loss 1.29243016\n",
      "Trained batch 645 batch loss 1.3376416 epoch total loss 1.29250026\n",
      "Trained batch 646 batch loss 1.30203259 epoch total loss 1.29251504\n",
      "Trained batch 647 batch loss 1.27649176 epoch total loss 1.29249024\n",
      "Trained batch 648 batch loss 1.25020719 epoch total loss 1.29242504\n",
      "Trained batch 649 batch loss 1.17359424 epoch total loss 1.29224193\n",
      "Trained batch 650 batch loss 1.18241572 epoch total loss 1.29207289\n",
      "Trained batch 651 batch loss 1.21767676 epoch total loss 1.29195857\n",
      "Trained batch 652 batch loss 1.06863809 epoch total loss 1.2916162\n",
      "Trained batch 653 batch loss 1.11776376 epoch total loss 1.29134989\n",
      "Trained batch 654 batch loss 1.22820973 epoch total loss 1.29125333\n",
      "Trained batch 655 batch loss 1.10831761 epoch total loss 1.29097402\n",
      "Trained batch 656 batch loss 1.21882868 epoch total loss 1.29086411\n",
      "Trained batch 657 batch loss 1.09276557 epoch total loss 1.29056251\n",
      "Trained batch 658 batch loss 1.14224601 epoch total loss 1.2903372\n",
      "Trained batch 659 batch loss 1.36435235 epoch total loss 1.2904495\n",
      "Trained batch 660 batch loss 1.18827224 epoch total loss 1.29029477\n",
      "Trained batch 661 batch loss 1.29228306 epoch total loss 1.29029775\n",
      "Trained batch 662 batch loss 1.17861688 epoch total loss 1.29012907\n",
      "Trained batch 663 batch loss 1.19265223 epoch total loss 1.28998196\n",
      "Trained batch 664 batch loss 1.21843 epoch total loss 1.2898742\n",
      "Trained batch 665 batch loss 1.2196908 epoch total loss 1.2897687\n",
      "Trained batch 666 batch loss 1.24493134 epoch total loss 1.28970134\n",
      "Trained batch 667 batch loss 1.45351768 epoch total loss 1.28994691\n",
      "Trained batch 668 batch loss 1.5290072 epoch total loss 1.29030478\n",
      "Trained batch 669 batch loss 1.41072774 epoch total loss 1.29048479\n",
      "Trained batch 670 batch loss 1.17817688 epoch total loss 1.29031706\n",
      "Trained batch 671 batch loss 1.3125062 epoch total loss 1.2903502\n",
      "Trained batch 672 batch loss 1.25046253 epoch total loss 1.29029083\n",
      "Trained batch 673 batch loss 1.23096418 epoch total loss 1.29020262\n",
      "Trained batch 674 batch loss 1.31413949 epoch total loss 1.29023814\n",
      "Trained batch 675 batch loss 1.2515527 epoch total loss 1.2901808\n",
      "Trained batch 676 batch loss 1.30308902 epoch total loss 1.2902\n",
      "Trained batch 677 batch loss 1.47629631 epoch total loss 1.29047489\n",
      "Trained batch 678 batch loss 1.53744197 epoch total loss 1.29083908\n",
      "Trained batch 679 batch loss 1.56375921 epoch total loss 1.29124105\n",
      "Trained batch 680 batch loss 1.2064935 epoch total loss 1.29111648\n",
      "Trained batch 681 batch loss 1.20853186 epoch total loss 1.29099524\n",
      "Trained batch 682 batch loss 1.27736497 epoch total loss 1.29097521\n",
      "Trained batch 683 batch loss 1.20585012 epoch total loss 1.29085052\n",
      "Trained batch 684 batch loss 1.16609287 epoch total loss 1.29066813\n",
      "Trained batch 685 batch loss 1.17906046 epoch total loss 1.29050529\n",
      "Trained batch 686 batch loss 1.20812452 epoch total loss 1.29038513\n",
      "Trained batch 687 batch loss 1.19619584 epoch total loss 1.29024804\n",
      "Trained batch 688 batch loss 1.16407216 epoch total loss 1.29006457\n",
      "Trained batch 689 batch loss 1.10828733 epoch total loss 1.28980076\n",
      "Trained batch 690 batch loss 1.18650389 epoch total loss 1.28965104\n",
      "Trained batch 691 batch loss 1.26525462 epoch total loss 1.28961575\n",
      "Trained batch 692 batch loss 1.17841625 epoch total loss 1.28945506\n",
      "Trained batch 693 batch loss 1.13427806 epoch total loss 1.28923118\n",
      "Trained batch 694 batch loss 1.19881439 epoch total loss 1.28910089\n",
      "Trained batch 695 batch loss 1.26283038 epoch total loss 1.28906298\n",
      "Trained batch 696 batch loss 1.25202107 epoch total loss 1.28900981\n",
      "Trained batch 697 batch loss 1.14411736 epoch total loss 1.28880191\n",
      "Trained batch 698 batch loss 1.00635183 epoch total loss 1.28839719\n",
      "Trained batch 699 batch loss 1.17187333 epoch total loss 1.28823054\n",
      "Trained batch 700 batch loss 1.12422574 epoch total loss 1.28799617\n",
      "Trained batch 701 batch loss 1.21058059 epoch total loss 1.28788579\n",
      "Trained batch 702 batch loss 1.25513411 epoch total loss 1.28783906\n",
      "Trained batch 703 batch loss 1.23666048 epoch total loss 1.28776622\n",
      "Trained batch 704 batch loss 1.33388543 epoch total loss 1.28783178\n",
      "Trained batch 705 batch loss 1.32631481 epoch total loss 1.28788626\n",
      "Trained batch 706 batch loss 1.3974086 epoch total loss 1.28804147\n",
      "Trained batch 707 batch loss 1.32905126 epoch total loss 1.28809941\n",
      "Trained batch 708 batch loss 1.25841558 epoch total loss 1.28805745\n",
      "Trained batch 709 batch loss 1.17372608 epoch total loss 1.28789616\n",
      "Trained batch 710 batch loss 1.31454086 epoch total loss 1.28793371\n",
      "Trained batch 711 batch loss 1.38647926 epoch total loss 1.28807235\n",
      "Trained batch 712 batch loss 1.39203835 epoch total loss 1.28821826\n",
      "Trained batch 713 batch loss 1.34526145 epoch total loss 1.28829837\n",
      "Trained batch 714 batch loss 1.27542436 epoch total loss 1.28828037\n",
      "Trained batch 715 batch loss 1.13432121 epoch total loss 1.28806508\n",
      "Trained batch 716 batch loss 1.28558886 epoch total loss 1.2880615\n",
      "Trained batch 717 batch loss 1.26789582 epoch total loss 1.28803337\n",
      "Trained batch 718 batch loss 1.28061461 epoch total loss 1.28802311\n",
      "Trained batch 719 batch loss 1.30661392 epoch total loss 1.28804898\n",
      "Trained batch 720 batch loss 1.35587013 epoch total loss 1.28814328\n",
      "Trained batch 721 batch loss 1.1882093 epoch total loss 1.28800464\n",
      "Trained batch 722 batch loss 1.32694912 epoch total loss 1.28805864\n",
      "Trained batch 723 batch loss 1.31430149 epoch total loss 1.288095\n",
      "Trained batch 724 batch loss 1.41493678 epoch total loss 1.28827012\n",
      "Trained batch 725 batch loss 1.09528887 epoch total loss 1.28800392\n",
      "Trained batch 726 batch loss 1.12347317 epoch total loss 1.2877773\n",
      "Trained batch 727 batch loss 1.11905169 epoch total loss 1.28754532\n",
      "Trained batch 728 batch loss 1.17444873 epoch total loss 1.28738987\n",
      "Trained batch 729 batch loss 1.25989711 epoch total loss 1.2873522\n",
      "Trained batch 730 batch loss 1.18538952 epoch total loss 1.28721249\n",
      "Trained batch 731 batch loss 1.22986269 epoch total loss 1.28713405\n",
      "Trained batch 732 batch loss 1.28892756 epoch total loss 1.28713644\n",
      "Trained batch 733 batch loss 1.44585752 epoch total loss 1.28735304\n",
      "Trained batch 734 batch loss 1.29455447 epoch total loss 1.28736281\n",
      "Trained batch 735 batch loss 1.42269909 epoch total loss 1.28754699\n",
      "Trained batch 736 batch loss 1.41699374 epoch total loss 1.28772283\n",
      "Trained batch 737 batch loss 1.47494233 epoch total loss 1.28797686\n",
      "Trained batch 738 batch loss 1.07886255 epoch total loss 1.2876935\n",
      "Trained batch 739 batch loss 1.25070572 epoch total loss 1.28764355\n",
      "Trained batch 740 batch loss 1.37631357 epoch total loss 1.28776336\n",
      "Trained batch 741 batch loss 1.38027465 epoch total loss 1.28788817\n",
      "Trained batch 742 batch loss 1.21489 epoch total loss 1.28778982\n",
      "Trained batch 743 batch loss 1.41804 epoch total loss 1.28796506\n",
      "Trained batch 744 batch loss 1.33015966 epoch total loss 1.2880218\n",
      "Trained batch 745 batch loss 1.2747817 epoch total loss 1.28800404\n",
      "Trained batch 746 batch loss 1.16129744 epoch total loss 1.28783417\n",
      "Trained batch 747 batch loss 1.18722856 epoch total loss 1.28769958\n",
      "Trained batch 748 batch loss 1.22027397 epoch total loss 1.28760946\n",
      "Trained batch 749 batch loss 1.31854892 epoch total loss 1.2876507\n",
      "Trained batch 750 batch loss 1.30121446 epoch total loss 1.28766882\n",
      "Trained batch 751 batch loss 1.44159138 epoch total loss 1.28787374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 752 batch loss 1.25325072 epoch total loss 1.28782773\n",
      "Trained batch 753 batch loss 1.4278816 epoch total loss 1.28801358\n",
      "Trained batch 754 batch loss 1.44532752 epoch total loss 1.28822231\n",
      "Trained batch 755 batch loss 1.46208632 epoch total loss 1.28845251\n",
      "Trained batch 756 batch loss 1.36787701 epoch total loss 1.28855753\n",
      "Trained batch 757 batch loss 1.42330456 epoch total loss 1.28873551\n",
      "Trained batch 758 batch loss 1.37764585 epoch total loss 1.28885281\n",
      "Trained batch 759 batch loss 1.34318948 epoch total loss 1.28892446\n",
      "Trained batch 760 batch loss 1.30954862 epoch total loss 1.28895164\n",
      "Trained batch 761 batch loss 1.32632959 epoch total loss 1.28900075\n",
      "Trained batch 762 batch loss 1.29160929 epoch total loss 1.28900421\n",
      "Trained batch 763 batch loss 1.38721287 epoch total loss 1.28913295\n",
      "Trained batch 764 batch loss 1.23973775 epoch total loss 1.28906822\n",
      "Trained batch 765 batch loss 1.22984099 epoch total loss 1.28899086\n",
      "Trained batch 766 batch loss 1.17003846 epoch total loss 1.28883553\n",
      "Trained batch 767 batch loss 1.1172142 epoch total loss 1.28861177\n",
      "Trained batch 768 batch loss 1.20552111 epoch total loss 1.28850353\n",
      "Trained batch 769 batch loss 1.3104099 epoch total loss 1.28853202\n",
      "Trained batch 770 batch loss 1.34937024 epoch total loss 1.28861105\n",
      "Trained batch 771 batch loss 1.24791038 epoch total loss 1.28855836\n",
      "Trained batch 772 batch loss 1.20490921 epoch total loss 1.28844988\n",
      "Trained batch 773 batch loss 1.20846295 epoch total loss 1.28834641\n",
      "Trained batch 774 batch loss 1.31844664 epoch total loss 1.28838527\n",
      "Trained batch 775 batch loss 1.31623268 epoch total loss 1.28842115\n",
      "Trained batch 776 batch loss 1.35189402 epoch total loss 1.28850293\n",
      "Trained batch 777 batch loss 1.23076129 epoch total loss 1.28842866\n",
      "Trained batch 778 batch loss 1.06752992 epoch total loss 1.28814471\n",
      "Trained batch 779 batch loss 0.97067 epoch total loss 1.28773713\n",
      "Trained batch 780 batch loss 1.05127585 epoch total loss 1.28743398\n",
      "Trained batch 781 batch loss 1.29462814 epoch total loss 1.28744316\n",
      "Trained batch 782 batch loss 1.16882551 epoch total loss 1.28729141\n",
      "Trained batch 783 batch loss 1.25765646 epoch total loss 1.28725362\n",
      "Trained batch 784 batch loss 1.3589741 epoch total loss 1.28734505\n",
      "Trained batch 785 batch loss 1.3368696 epoch total loss 1.28740811\n",
      "Trained batch 786 batch loss 1.15553629 epoch total loss 1.28724027\n",
      "Trained batch 787 batch loss 1.27465558 epoch total loss 1.28722429\n",
      "Trained batch 788 batch loss 1.21919513 epoch total loss 1.28713799\n",
      "Trained batch 789 batch loss 1.3419255 epoch total loss 1.28720737\n",
      "Trained batch 790 batch loss 1.24694228 epoch total loss 1.28715646\n",
      "Trained batch 791 batch loss 1.26590776 epoch total loss 1.28712964\n",
      "Trained batch 792 batch loss 1.29346275 epoch total loss 1.28713763\n",
      "Trained batch 793 batch loss 1.34544051 epoch total loss 1.28721118\n",
      "Trained batch 794 batch loss 1.23376393 epoch total loss 1.28714383\n",
      "Trained batch 795 batch loss 1.29484 epoch total loss 1.28715348\n",
      "Trained batch 796 batch loss 1.23364854 epoch total loss 1.28708637\n",
      "Trained batch 797 batch loss 1.33763945 epoch total loss 1.28714979\n",
      "Trained batch 798 batch loss 1.38487375 epoch total loss 1.28727233\n",
      "Trained batch 799 batch loss 1.29337847 epoch total loss 1.28727984\n",
      "Trained batch 800 batch loss 1.15235305 epoch total loss 1.28711116\n",
      "Trained batch 801 batch loss 1.24610174 epoch total loss 1.28706\n",
      "Trained batch 802 batch loss 1.24392521 epoch total loss 1.28700614\n",
      "Trained batch 803 batch loss 1.28715634 epoch total loss 1.28700626\n",
      "Trained batch 804 batch loss 1.33790493 epoch total loss 1.28706956\n",
      "Trained batch 805 batch loss 1.36121917 epoch total loss 1.28716171\n",
      "Trained batch 806 batch loss 1.30006099 epoch total loss 1.28717768\n",
      "Trained batch 807 batch loss 1.43163908 epoch total loss 1.28735673\n",
      "Trained batch 808 batch loss 1.28554082 epoch total loss 1.28735447\n",
      "Trained batch 809 batch loss 1.3062259 epoch total loss 1.28737783\n",
      "Trained batch 810 batch loss 1.31847799 epoch total loss 1.28741622\n",
      "Trained batch 811 batch loss 1.38379824 epoch total loss 1.28753507\n",
      "Trained batch 812 batch loss 1.41993356 epoch total loss 1.28769803\n",
      "Trained batch 813 batch loss 1.20846093 epoch total loss 1.28760064\n",
      "Trained batch 814 batch loss 1.34276295 epoch total loss 1.28766847\n",
      "Trained batch 815 batch loss 1.41940391 epoch total loss 1.28783011\n",
      "Trained batch 816 batch loss 1.29320157 epoch total loss 1.28783667\n",
      "Trained batch 817 batch loss 1.298455 epoch total loss 1.28784966\n",
      "Trained batch 818 batch loss 1.26697016 epoch total loss 1.28782415\n",
      "Trained batch 819 batch loss 1.23397398 epoch total loss 1.28775847\n",
      "Trained batch 820 batch loss 1.37910926 epoch total loss 1.28786993\n",
      "Trained batch 821 batch loss 1.31518626 epoch total loss 1.28790319\n",
      "Trained batch 822 batch loss 1.37449026 epoch total loss 1.28800857\n",
      "Trained batch 823 batch loss 1.3041172 epoch total loss 1.28802812\n",
      "Trained batch 824 batch loss 1.24580169 epoch total loss 1.28797686\n",
      "Trained batch 825 batch loss 1.37054682 epoch total loss 1.28807712\n",
      "Trained batch 826 batch loss 1.3348484 epoch total loss 1.28813362\n",
      "Trained batch 827 batch loss 1.31496811 epoch total loss 1.28816605\n",
      "Trained batch 828 batch loss 1.21921968 epoch total loss 1.28808284\n",
      "Trained batch 829 batch loss 1.24178588 epoch total loss 1.28802705\n",
      "Trained batch 830 batch loss 1.27368832 epoch total loss 1.28800976\n",
      "Trained batch 831 batch loss 1.23073673 epoch total loss 1.28794086\n",
      "Trained batch 832 batch loss 1.17703223 epoch total loss 1.28780746\n",
      "Trained batch 833 batch loss 1.04987216 epoch total loss 1.28752184\n",
      "Trained batch 834 batch loss 1.14632797 epoch total loss 1.28735268\n",
      "Trained batch 835 batch loss 1.20350385 epoch total loss 1.28725219\n",
      "Trained batch 836 batch loss 1.1143465 epoch total loss 1.28704548\n",
      "Trained batch 837 batch loss 1.15253818 epoch total loss 1.28688478\n",
      "Trained batch 838 batch loss 1.22189975 epoch total loss 1.2868073\n",
      "Trained batch 839 batch loss 1.25254929 epoch total loss 1.28676641\n",
      "Trained batch 840 batch loss 1.30290556 epoch total loss 1.2867856\n",
      "Trained batch 841 batch loss 1.10875547 epoch total loss 1.28657389\n",
      "Trained batch 842 batch loss 1.2215035 epoch total loss 1.28649676\n",
      "Trained batch 843 batch loss 1.26516914 epoch total loss 1.28647137\n",
      "Trained batch 844 batch loss 1.32625318 epoch total loss 1.28651857\n",
      "Trained batch 845 batch loss 1.32425714 epoch total loss 1.28656316\n",
      "Trained batch 846 batch loss 1.33467782 epoch total loss 1.28662014\n",
      "Trained batch 847 batch loss 1.32331157 epoch total loss 1.28666341\n",
      "Trained batch 848 batch loss 1.36465728 epoch total loss 1.28675544\n",
      "Trained batch 849 batch loss 1.20896471 epoch total loss 1.28666377\n",
      "Trained batch 850 batch loss 1.22311 epoch total loss 1.28658903\n",
      "Trained batch 851 batch loss 1.22843206 epoch total loss 1.28652072\n",
      "Trained batch 852 batch loss 1.22210014 epoch total loss 1.28644502\n",
      "Trained batch 853 batch loss 1.20545828 epoch total loss 1.28635\n",
      "Trained batch 854 batch loss 1.30356586 epoch total loss 1.28637028\n",
      "Trained batch 855 batch loss 1.29509211 epoch total loss 1.28638041\n",
      "Trained batch 856 batch loss 1.27991891 epoch total loss 1.28637278\n",
      "Trained batch 857 batch loss 1.2789315 epoch total loss 1.28636408\n",
      "Trained batch 858 batch loss 1.21994936 epoch total loss 1.28628671\n",
      "Trained batch 859 batch loss 1.19661 epoch total loss 1.2861824\n",
      "Trained batch 860 batch loss 1.19846106 epoch total loss 1.28608048\n",
      "Trained batch 861 batch loss 1.15497565 epoch total loss 1.28592825\n",
      "Trained batch 862 batch loss 1.25669372 epoch total loss 1.28589439\n",
      "Trained batch 863 batch loss 1.28931963 epoch total loss 1.28589833\n",
      "Trained batch 864 batch loss 1.25828159 epoch total loss 1.28586638\n",
      "Trained batch 865 batch loss 1.20419097 epoch total loss 1.28577197\n",
      "Trained batch 866 batch loss 1.19203269 epoch total loss 1.28566372\n",
      "Trained batch 867 batch loss 1.15597582 epoch total loss 1.28551412\n",
      "Trained batch 868 batch loss 1.17785656 epoch total loss 1.28539014\n",
      "Trained batch 869 batch loss 1.31611335 epoch total loss 1.28542554\n",
      "Trained batch 870 batch loss 1.16971135 epoch total loss 1.28529251\n",
      "Trained batch 871 batch loss 1.10148215 epoch total loss 1.28508139\n",
      "Trained batch 872 batch loss 1.19806921 epoch total loss 1.28498173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 873 batch loss 1.15728354 epoch total loss 1.28483534\n",
      "Trained batch 874 batch loss 1.1477716 epoch total loss 1.28467858\n",
      "Trained batch 875 batch loss 1.24355674 epoch total loss 1.28463161\n",
      "Trained batch 876 batch loss 1.0882659 epoch total loss 1.28440738\n",
      "Trained batch 877 batch loss 1.12756312 epoch total loss 1.28422856\n",
      "Trained batch 878 batch loss 1.24721599 epoch total loss 1.28418636\n",
      "Trained batch 879 batch loss 1.22002411 epoch total loss 1.28411329\n",
      "Trained batch 880 batch loss 1.1520946 epoch total loss 1.28396332\n",
      "Trained batch 881 batch loss 1.27614927 epoch total loss 1.28395438\n",
      "Trained batch 882 batch loss 1.54492056 epoch total loss 1.28425026\n",
      "Trained batch 883 batch loss 1.23360968 epoch total loss 1.28419292\n",
      "Trained batch 884 batch loss 1.38153148 epoch total loss 1.28430319\n",
      "Trained batch 885 batch loss 1.45724523 epoch total loss 1.28449857\n",
      "Trained batch 886 batch loss 1.42763758 epoch total loss 1.2846601\n",
      "Trained batch 887 batch loss 1.27111614 epoch total loss 1.28464484\n",
      "Trained batch 888 batch loss 1.33477139 epoch total loss 1.28470123\n",
      "Trained batch 889 batch loss 1.17256808 epoch total loss 1.2845751\n",
      "Trained batch 890 batch loss 1.35420513 epoch total loss 1.28465343\n",
      "Trained batch 891 batch loss 1.26461136 epoch total loss 1.28463101\n",
      "Trained batch 892 batch loss 1.27226114 epoch total loss 1.28461707\n",
      "Trained batch 893 batch loss 1.3625778 epoch total loss 1.28470433\n",
      "Trained batch 894 batch loss 1.23604107 epoch total loss 1.28465\n",
      "Trained batch 895 batch loss 1.27206635 epoch total loss 1.2846359\n",
      "Trained batch 896 batch loss 1.3244803 epoch total loss 1.28468037\n",
      "Trained batch 897 batch loss 1.26552665 epoch total loss 1.28465903\n",
      "Trained batch 898 batch loss 1.19108272 epoch total loss 1.28455472\n",
      "Trained batch 899 batch loss 1.22722614 epoch total loss 1.28449094\n",
      "Trained batch 900 batch loss 1.31801319 epoch total loss 1.28452814\n",
      "Trained batch 901 batch loss 1.32515407 epoch total loss 1.28457332\n",
      "Trained batch 902 batch loss 1.42724514 epoch total loss 1.28473139\n",
      "Trained batch 903 batch loss 1.29772317 epoch total loss 1.28474581\n",
      "Trained batch 904 batch loss 1.33730292 epoch total loss 1.28480399\n",
      "Trained batch 905 batch loss 1.26551569 epoch total loss 1.28478265\n",
      "Trained batch 906 batch loss 1.37177718 epoch total loss 1.28487873\n",
      "Trained batch 907 batch loss 1.2305789 epoch total loss 1.28481889\n",
      "Trained batch 908 batch loss 1.29428 epoch total loss 1.28482926\n",
      "Trained batch 909 batch loss 1.21882534 epoch total loss 1.28475678\n",
      "Trained batch 910 batch loss 1.22130036 epoch total loss 1.28468704\n",
      "Trained batch 911 batch loss 1.19173598 epoch total loss 1.284585\n",
      "Trained batch 912 batch loss 1.33363914 epoch total loss 1.28463876\n",
      "Trained batch 913 batch loss 1.26893592 epoch total loss 1.2846216\n",
      "Trained batch 914 batch loss 1.26190424 epoch total loss 1.2845968\n",
      "Trained batch 915 batch loss 1.20568252 epoch total loss 1.28451049\n",
      "Trained batch 916 batch loss 1.25081897 epoch total loss 1.28447378\n",
      "Trained batch 917 batch loss 1.20587182 epoch total loss 1.28438818\n",
      "Trained batch 918 batch loss 1.23197937 epoch total loss 1.28433096\n",
      "Trained batch 919 batch loss 1.20114088 epoch total loss 1.28424048\n",
      "Trained batch 920 batch loss 1.2451911 epoch total loss 1.28419816\n",
      "Trained batch 921 batch loss 1.35112512 epoch total loss 1.28427076\n",
      "Trained batch 922 batch loss 1.21121609 epoch total loss 1.28419149\n",
      "Trained batch 923 batch loss 1.2689122 epoch total loss 1.28417492\n",
      "Trained batch 924 batch loss 1.21799409 epoch total loss 1.28410327\n",
      "Trained batch 925 batch loss 1.27607954 epoch total loss 1.28409469\n",
      "Trained batch 926 batch loss 1.30028594 epoch total loss 1.28411222\n",
      "Trained batch 927 batch loss 1.31534171 epoch total loss 1.28414583\n",
      "Trained batch 928 batch loss 1.29183888 epoch total loss 1.28415418\n",
      "Trained batch 929 batch loss 1.15954375 epoch total loss 1.28402007\n",
      "Trained batch 930 batch loss 1.1272409 epoch total loss 1.28385139\n",
      "Trained batch 931 batch loss 1.1747179 epoch total loss 1.2837342\n",
      "Trained batch 932 batch loss 1.26523113 epoch total loss 1.28371429\n",
      "Trained batch 933 batch loss 1.24348068 epoch total loss 1.28367126\n",
      "Trained batch 934 batch loss 1.24187112 epoch total loss 1.28362644\n",
      "Trained batch 935 batch loss 1.20651484 epoch total loss 1.28354406\n",
      "Trained batch 936 batch loss 1.35194874 epoch total loss 1.28361702\n",
      "Trained batch 937 batch loss 1.16682804 epoch total loss 1.28349245\n",
      "Trained batch 938 batch loss 1.14994216 epoch total loss 1.28335011\n",
      "Trained batch 939 batch loss 1.25003099 epoch total loss 1.28331459\n",
      "Trained batch 940 batch loss 1.26684391 epoch total loss 1.28329706\n",
      "Trained batch 941 batch loss 1.19457436 epoch total loss 1.28320277\n",
      "Trained batch 942 batch loss 1.142362 epoch total loss 1.28305316\n",
      "Trained batch 943 batch loss 1.04348445 epoch total loss 1.28279912\n",
      "Trained batch 944 batch loss 1.14660263 epoch total loss 1.28265488\n",
      "Trained batch 945 batch loss 1.20049465 epoch total loss 1.28256786\n",
      "Trained batch 946 batch loss 1.11564016 epoch total loss 1.28239131\n",
      "Trained batch 947 batch loss 1.1991452 epoch total loss 1.28230333\n",
      "Trained batch 948 batch loss 1.28938854 epoch total loss 1.28231096\n",
      "Trained batch 949 batch loss 1.15230441 epoch total loss 1.28217399\n",
      "Trained batch 950 batch loss 1.37024117 epoch total loss 1.28226662\n",
      "Trained batch 951 batch loss 1.36281598 epoch total loss 1.28235137\n",
      "Trained batch 952 batch loss 1.34070718 epoch total loss 1.28241265\n",
      "Trained batch 953 batch loss 1.24849343 epoch total loss 1.28237712\n",
      "Trained batch 954 batch loss 1.29095781 epoch total loss 1.28238606\n",
      "Trained batch 955 batch loss 1.30676579 epoch total loss 1.28241169\n",
      "Trained batch 956 batch loss 1.29488277 epoch total loss 1.28242469\n",
      "Trained batch 957 batch loss 1.28214025 epoch total loss 1.28242445\n",
      "Trained batch 958 batch loss 1.19688761 epoch total loss 1.28233516\n",
      "Trained batch 959 batch loss 1.27840686 epoch total loss 1.28233111\n",
      "Trained batch 960 batch loss 1.19047666 epoch total loss 1.28223538\n",
      "Trained batch 961 batch loss 1.21502209 epoch total loss 1.28216529\n",
      "Trained batch 962 batch loss 1.22634709 epoch total loss 1.28210723\n",
      "Trained batch 963 batch loss 1.36587882 epoch total loss 1.28219426\n",
      "Trained batch 964 batch loss 1.45116043 epoch total loss 1.28236949\n",
      "Trained batch 965 batch loss 1.36566353 epoch total loss 1.28245592\n",
      "Trained batch 966 batch loss 1.4311254 epoch total loss 1.28260982\n",
      "Trained batch 967 batch loss 1.26777101 epoch total loss 1.28259456\n",
      "Trained batch 968 batch loss 1.3176049 epoch total loss 1.28263068\n",
      "Trained batch 969 batch loss 1.31484175 epoch total loss 1.28266394\n",
      "Trained batch 970 batch loss 1.3563273 epoch total loss 1.28273988\n",
      "Trained batch 971 batch loss 1.24920619 epoch total loss 1.28270531\n",
      "Trained batch 972 batch loss 1.29900086 epoch total loss 1.282722\n",
      "Trained batch 973 batch loss 1.24661994 epoch total loss 1.2826848\n",
      "Trained batch 974 batch loss 1.25329113 epoch total loss 1.28265464\n",
      "Trained batch 975 batch loss 1.12131238 epoch total loss 1.28248918\n",
      "Trained batch 976 batch loss 1.2656877 epoch total loss 1.28247213\n",
      "Trained batch 977 batch loss 1.21857929 epoch total loss 1.28240669\n",
      "Trained batch 978 batch loss 1.22846413 epoch total loss 1.28235161\n",
      "Trained batch 979 batch loss 1.24597669 epoch total loss 1.28231442\n",
      "Trained batch 980 batch loss 1.29154122 epoch total loss 1.28232384\n",
      "Trained batch 981 batch loss 1.23770165 epoch total loss 1.2822783\n",
      "Trained batch 982 batch loss 1.18393099 epoch total loss 1.28217816\n",
      "Trained batch 983 batch loss 1.21212721 epoch total loss 1.282107\n",
      "Trained batch 984 batch loss 1.29707849 epoch total loss 1.28212225\n",
      "Trained batch 985 batch loss 1.38006222 epoch total loss 1.28222156\n",
      "Trained batch 986 batch loss 1.31560779 epoch total loss 1.28225541\n",
      "Trained batch 987 batch loss 1.23119295 epoch total loss 1.28220367\n",
      "Trained batch 988 batch loss 1.11852944 epoch total loss 1.28203797\n",
      "Trained batch 989 batch loss 1.202214 epoch total loss 1.28195739\n",
      "Trained batch 990 batch loss 1.26011646 epoch total loss 1.28193533\n",
      "Trained batch 991 batch loss 1.37185931 epoch total loss 1.28202605\n",
      "Trained batch 992 batch loss 1.29467928 epoch total loss 1.28203881\n",
      "Trained batch 993 batch loss 1.42898285 epoch total loss 1.28218675\n",
      "Trained batch 994 batch loss 1.42393374 epoch total loss 1.28232932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 995 batch loss 1.31153977 epoch total loss 1.28235865\n",
      "Trained batch 996 batch loss 1.41276932 epoch total loss 1.28248954\n",
      "Trained batch 997 batch loss 1.181494 epoch total loss 1.28238833\n",
      "Trained batch 998 batch loss 1.17906797 epoch total loss 1.28228474\n",
      "Trained batch 999 batch loss 1.27920389 epoch total loss 1.28228164\n",
      "Trained batch 1000 batch loss 1.2773838 epoch total loss 1.28227675\n",
      "Trained batch 1001 batch loss 1.22796047 epoch total loss 1.28222239\n",
      "Trained batch 1002 batch loss 1.38116419 epoch total loss 1.2823211\n",
      "Trained batch 1003 batch loss 1.42683709 epoch total loss 1.28246522\n",
      "Trained batch 1004 batch loss 1.41915751 epoch total loss 1.28260136\n",
      "Trained batch 1005 batch loss 1.23369074 epoch total loss 1.28255272\n",
      "Trained batch 1006 batch loss 1.27553034 epoch total loss 1.28254569\n",
      "Trained batch 1007 batch loss 1.32031727 epoch total loss 1.28258324\n",
      "Trained batch 1008 batch loss 1.22313118 epoch total loss 1.28252423\n",
      "Trained batch 1009 batch loss 1.37616563 epoch total loss 1.28261709\n",
      "Trained batch 1010 batch loss 1.37742627 epoch total loss 1.28271103\n",
      "Trained batch 1011 batch loss 1.56149173 epoch total loss 1.28298676\n",
      "Trained batch 1012 batch loss 1.37461567 epoch total loss 1.28307736\n",
      "Trained batch 1013 batch loss 1.37815928 epoch total loss 1.28317118\n",
      "Trained batch 1014 batch loss 1.42355597 epoch total loss 1.2833097\n",
      "Trained batch 1015 batch loss 1.36523008 epoch total loss 1.2833904\n",
      "Trained batch 1016 batch loss 1.40816259 epoch total loss 1.28351319\n",
      "Trained batch 1017 batch loss 1.37063777 epoch total loss 1.2835989\n",
      "Trained batch 1018 batch loss 1.26299322 epoch total loss 1.28357852\n",
      "Trained batch 1019 batch loss 1.22061276 epoch total loss 1.28351676\n",
      "Trained batch 1020 batch loss 1.11318624 epoch total loss 1.28334975\n",
      "Trained batch 1021 batch loss 1.31486273 epoch total loss 1.28338051\n",
      "Trained batch 1022 batch loss 1.21752131 epoch total loss 1.28331614\n",
      "Trained batch 1023 batch loss 1.35266173 epoch total loss 1.28338385\n",
      "Trained batch 1024 batch loss 1.44786739 epoch total loss 1.28354454\n",
      "Trained batch 1025 batch loss 1.31782103 epoch total loss 1.28357804\n",
      "Trained batch 1026 batch loss 1.28674507 epoch total loss 1.28358114\n",
      "Trained batch 1027 batch loss 1.34613848 epoch total loss 1.28364205\n",
      "Trained batch 1028 batch loss 1.37953734 epoch total loss 1.28373539\n",
      "Trained batch 1029 batch loss 1.27630746 epoch total loss 1.28372812\n",
      "Trained batch 1030 batch loss 1.38226056 epoch total loss 1.28382373\n",
      "Trained batch 1031 batch loss 1.29123425 epoch total loss 1.283831\n",
      "Trained batch 1032 batch loss 1.33820343 epoch total loss 1.28388369\n",
      "Trained batch 1033 batch loss 1.3727479 epoch total loss 1.28396976\n",
      "Trained batch 1034 batch loss 1.2457943 epoch total loss 1.28393292\n",
      "Trained batch 1035 batch loss 1.3078022 epoch total loss 1.28395605\n",
      "Trained batch 1036 batch loss 1.2865231 epoch total loss 1.28395855\n",
      "Trained batch 1037 batch loss 1.25503969 epoch total loss 1.28393066\n",
      "Trained batch 1038 batch loss 1.26699066 epoch total loss 1.28391421\n",
      "Trained batch 1039 batch loss 1.31366229 epoch total loss 1.28394294\n",
      "Trained batch 1040 batch loss 1.16205251 epoch total loss 1.28382576\n",
      "Trained batch 1041 batch loss 1.19386137 epoch total loss 1.28373933\n",
      "Trained batch 1042 batch loss 1.40061212 epoch total loss 1.2838515\n",
      "Trained batch 1043 batch loss 1.29222202 epoch total loss 1.28385961\n",
      "Trained batch 1044 batch loss 1.35680723 epoch total loss 1.28392947\n",
      "Trained batch 1045 batch loss 1.31054354 epoch total loss 1.28395498\n",
      "Trained batch 1046 batch loss 1.38989615 epoch total loss 1.28405619\n",
      "Trained batch 1047 batch loss 1.299698 epoch total loss 1.28407109\n",
      "Trained batch 1048 batch loss 1.16829443 epoch total loss 1.2839607\n",
      "Trained batch 1049 batch loss 1.2292074 epoch total loss 1.28390861\n",
      "Trained batch 1050 batch loss 1.12233412 epoch total loss 1.28375471\n",
      "Trained batch 1051 batch loss 1.05204821 epoch total loss 1.28353417\n",
      "Trained batch 1052 batch loss 1.15206063 epoch total loss 1.28340924\n",
      "Trained batch 1053 batch loss 1.16388726 epoch total loss 1.28329575\n",
      "Trained batch 1054 batch loss 1.08889961 epoch total loss 1.28311133\n",
      "Trained batch 1055 batch loss 0.950301588 epoch total loss 1.28279579\n",
      "Trained batch 1056 batch loss 0.983913422 epoch total loss 1.28251278\n",
      "Trained batch 1057 batch loss 0.941847205 epoch total loss 1.28219056\n",
      "Trained batch 1058 batch loss 1.15623486 epoch total loss 1.28207147\n",
      "Trained batch 1059 batch loss 1.18116343 epoch total loss 1.28197622\n",
      "Trained batch 1060 batch loss 1.27522814 epoch total loss 1.2819699\n",
      "Trained batch 1061 batch loss 1.25964689 epoch total loss 1.2819488\n",
      "Trained batch 1062 batch loss 1.24662602 epoch total loss 1.28191555\n",
      "Trained batch 1063 batch loss 1.11499929 epoch total loss 1.28175855\n",
      "Trained batch 1064 batch loss 1.23771477 epoch total loss 1.28171706\n",
      "Trained batch 1065 batch loss 1.19526219 epoch total loss 1.28163588\n",
      "Trained batch 1066 batch loss 1.25369847 epoch total loss 1.28160965\n",
      "Trained batch 1067 batch loss 1.38990009 epoch total loss 1.28171122\n",
      "Trained batch 1068 batch loss 1.33376849 epoch total loss 1.28175986\n",
      "Trained batch 1069 batch loss 1.2858727 epoch total loss 1.28176379\n",
      "Trained batch 1070 batch loss 1.17760849 epoch total loss 1.2816664\n",
      "Trained batch 1071 batch loss 1.27308917 epoch total loss 1.28165841\n",
      "Trained batch 1072 batch loss 1.26166296 epoch total loss 1.28163981\n",
      "Trained batch 1073 batch loss 1.28361893 epoch total loss 1.2816416\n",
      "Trained batch 1074 batch loss 1.14773238 epoch total loss 1.28151691\n",
      "Trained batch 1075 batch loss 1.1214031 epoch total loss 1.28136802\n",
      "Trained batch 1076 batch loss 1.21450734 epoch total loss 1.28130579\n",
      "Trained batch 1077 batch loss 1.39085 epoch total loss 1.28140759\n",
      "Trained batch 1078 batch loss 1.36572802 epoch total loss 1.2814858\n",
      "Trained batch 1079 batch loss 1.24786329 epoch total loss 1.28145456\n",
      "Trained batch 1080 batch loss 1.1471653 epoch total loss 1.28133023\n",
      "Trained batch 1081 batch loss 1.11215615 epoch total loss 1.28117383\n",
      "Trained batch 1082 batch loss 1.03918409 epoch total loss 1.28095007\n",
      "Trained batch 1083 batch loss 1.07820773 epoch total loss 1.28076291\n",
      "Trained batch 1084 batch loss 1.05065846 epoch total loss 1.28055072\n",
      "Trained batch 1085 batch loss 1.10018015 epoch total loss 1.28038454\n",
      "Trained batch 1086 batch loss 1.13035607 epoch total loss 1.28024638\n",
      "Trained batch 1087 batch loss 1.15673864 epoch total loss 1.28013277\n",
      "Trained batch 1088 batch loss 1.39000034 epoch total loss 1.28023374\n",
      "Trained batch 1089 batch loss 1.43271923 epoch total loss 1.28037381\n",
      "Trained batch 1090 batch loss 1.33634818 epoch total loss 1.28042507\n",
      "Trained batch 1091 batch loss 1.45049822 epoch total loss 1.28058088\n",
      "Trained batch 1092 batch loss 1.25183654 epoch total loss 1.28055453\n",
      "Trained batch 1093 batch loss 1.34885776 epoch total loss 1.28061712\n",
      "Trained batch 1094 batch loss 1.34026742 epoch total loss 1.2806716\n",
      "Trained batch 1095 batch loss 1.40798008 epoch total loss 1.28078783\n",
      "Trained batch 1096 batch loss 1.24994087 epoch total loss 1.28075969\n",
      "Trained batch 1097 batch loss 1.19709182 epoch total loss 1.28068352\n",
      "Trained batch 1098 batch loss 1.23382044 epoch total loss 1.28064072\n",
      "Trained batch 1099 batch loss 1.07963932 epoch total loss 1.28045785\n",
      "Trained batch 1100 batch loss 1.11099935 epoch total loss 1.28030372\n",
      "Trained batch 1101 batch loss 1.25151134 epoch total loss 1.28027749\n",
      "Trained batch 1102 batch loss 1.27294767 epoch total loss 1.28027093\n",
      "Trained batch 1103 batch loss 1.37412941 epoch total loss 1.28035605\n",
      "Trained batch 1104 batch loss 1.34358788 epoch total loss 1.28041327\n",
      "Trained batch 1105 batch loss 1.4076674 epoch total loss 1.28052855\n",
      "Trained batch 1106 batch loss 1.34520102 epoch total loss 1.28058696\n",
      "Trained batch 1107 batch loss 1.3866291 epoch total loss 1.2806828\n",
      "Trained batch 1108 batch loss 1.34170938 epoch total loss 1.28073776\n",
      "Trained batch 1109 batch loss 1.17618585 epoch total loss 1.28064346\n",
      "Trained batch 1110 batch loss 1.09350216 epoch total loss 1.2804749\n",
      "Trained batch 1111 batch loss 1.2100153 epoch total loss 1.28041148\n",
      "Trained batch 1112 batch loss 1.15623474 epoch total loss 1.28029978\n",
      "Trained batch 1113 batch loss 1.21758294 epoch total loss 1.2802434\n",
      "Trained batch 1114 batch loss 1.1648854 epoch total loss 1.2801398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1115 batch loss 1.2263093 epoch total loss 1.28009164\n",
      "Trained batch 1116 batch loss 1.22847736 epoch total loss 1.28004539\n",
      "Trained batch 1117 batch loss 1.37179351 epoch total loss 1.28012753\n",
      "Trained batch 1118 batch loss 1.36133885 epoch total loss 1.28020012\n",
      "Trained batch 1119 batch loss 1.33443832 epoch total loss 1.28024864\n",
      "Trained batch 1120 batch loss 1.47173429 epoch total loss 1.28041959\n",
      "Trained batch 1121 batch loss 1.55008602 epoch total loss 1.28066015\n",
      "Trained batch 1122 batch loss 1.39422679 epoch total loss 1.28076136\n",
      "Trained batch 1123 batch loss 1.17842746 epoch total loss 1.28067029\n",
      "Trained batch 1124 batch loss 1.14822674 epoch total loss 1.28055239\n",
      "Trained batch 1125 batch loss 1.43969429 epoch total loss 1.28069389\n",
      "Trained batch 1126 batch loss 1.39456606 epoch total loss 1.28079498\n",
      "Trained batch 1127 batch loss 1.40840065 epoch total loss 1.28090823\n",
      "Trained batch 1128 batch loss 1.34830308 epoch total loss 1.28096795\n",
      "Trained batch 1129 batch loss 1.31426859 epoch total loss 1.2809974\n",
      "Trained batch 1130 batch loss 1.33441019 epoch total loss 1.2810446\n",
      "Trained batch 1131 batch loss 1.25041962 epoch total loss 1.28101754\n",
      "Trained batch 1132 batch loss 1.23914576 epoch total loss 1.28098047\n",
      "Trained batch 1133 batch loss 1.29321897 epoch total loss 1.28099132\n",
      "Trained batch 1134 batch loss 1.34646237 epoch total loss 1.28104901\n",
      "Trained batch 1135 batch loss 1.31786942 epoch total loss 1.28108144\n",
      "Trained batch 1136 batch loss 1.35290194 epoch total loss 1.28114474\n",
      "Trained batch 1137 batch loss 1.2522577 epoch total loss 1.28111923\n",
      "Trained batch 1138 batch loss 1.15409875 epoch total loss 1.28100753\n",
      "Trained batch 1139 batch loss 1.31502473 epoch total loss 1.28103745\n",
      "Trained batch 1140 batch loss 1.2337184 epoch total loss 1.28099597\n",
      "Trained batch 1141 batch loss 1.27206814 epoch total loss 1.28098822\n",
      "Trained batch 1142 batch loss 1.31711435 epoch total loss 1.28101981\n",
      "Trained batch 1143 batch loss 1.28178167 epoch total loss 1.28102052\n",
      "Trained batch 1144 batch loss 1.37216496 epoch total loss 1.28110015\n",
      "Trained batch 1145 batch loss 1.33120787 epoch total loss 1.2811439\n",
      "Trained batch 1146 batch loss 1.29495084 epoch total loss 1.28115594\n",
      "Trained batch 1147 batch loss 1.32106113 epoch total loss 1.28119075\n",
      "Trained batch 1148 batch loss 1.38024735 epoch total loss 1.28127706\n",
      "Trained batch 1149 batch loss 1.22616279 epoch total loss 1.28122902\n",
      "Trained batch 1150 batch loss 1.02893126 epoch total loss 1.28100967\n",
      "Trained batch 1151 batch loss 1.00333357 epoch total loss 1.28076839\n",
      "Trained batch 1152 batch loss 1.08508539 epoch total loss 1.28059852\n",
      "Trained batch 1153 batch loss 1.3121171 epoch total loss 1.28062594\n",
      "Trained batch 1154 batch loss 1.51884174 epoch total loss 1.28083229\n",
      "Trained batch 1155 batch loss 1.62707925 epoch total loss 1.2811321\n",
      "Trained batch 1156 batch loss 1.20686591 epoch total loss 1.28106785\n",
      "Trained batch 1157 batch loss 1.23813772 epoch total loss 1.28103077\n",
      "Trained batch 1158 batch loss 1.26677322 epoch total loss 1.28101838\n",
      "Trained batch 1159 batch loss 1.33083248 epoch total loss 1.28106141\n",
      "Trained batch 1160 batch loss 1.33805251 epoch total loss 1.28111041\n",
      "Trained batch 1161 batch loss 1.28658915 epoch total loss 1.28111517\n",
      "Trained batch 1162 batch loss 1.28898585 epoch total loss 1.28112197\n",
      "Trained batch 1163 batch loss 1.40596282 epoch total loss 1.28122938\n",
      "Trained batch 1164 batch loss 1.23563266 epoch total loss 1.28119016\n",
      "Trained batch 1165 batch loss 1.28622329 epoch total loss 1.28119445\n",
      "Trained batch 1166 batch loss 1.19377494 epoch total loss 1.28111947\n",
      "Trained batch 1167 batch loss 1.1378721 epoch total loss 1.28099668\n",
      "Trained batch 1168 batch loss 1.26827586 epoch total loss 1.28098583\n",
      "Trained batch 1169 batch loss 1.23080564 epoch total loss 1.28094292\n",
      "Trained batch 1170 batch loss 1.17268956 epoch total loss 1.28085041\n",
      "Trained batch 1171 batch loss 1.16680217 epoch total loss 1.28075302\n",
      "Trained batch 1172 batch loss 1.27329898 epoch total loss 1.28074658\n",
      "Trained batch 1173 batch loss 1.25257146 epoch total loss 1.28072262\n",
      "Trained batch 1174 batch loss 1.31956434 epoch total loss 1.28075564\n",
      "Trained batch 1175 batch loss 1.30720675 epoch total loss 1.28077829\n",
      "Trained batch 1176 batch loss 1.17384624 epoch total loss 1.28068733\n",
      "Trained batch 1177 batch loss 1.09957218 epoch total loss 1.28053343\n",
      "Trained batch 1178 batch loss 1.12494874 epoch total loss 1.28040147\n",
      "Trained batch 1179 batch loss 1.28038692 epoch total loss 1.28040147\n",
      "Trained batch 1180 batch loss 1.32087469 epoch total loss 1.2804358\n",
      "Trained batch 1181 batch loss 1.32493734 epoch total loss 1.28047347\n",
      "Trained batch 1182 batch loss 1.42337477 epoch total loss 1.28059435\n",
      "Trained batch 1183 batch loss 1.34575367 epoch total loss 1.2806493\n",
      "Trained batch 1184 batch loss 1.23657846 epoch total loss 1.28061211\n",
      "Trained batch 1185 batch loss 1.39595747 epoch total loss 1.28070951\n",
      "Trained batch 1186 batch loss 1.20870399 epoch total loss 1.28064883\n",
      "Trained batch 1187 batch loss 1.30030704 epoch total loss 1.2806654\n",
      "Trained batch 1188 batch loss 1.33897638 epoch total loss 1.28071451\n",
      "Trained batch 1189 batch loss 1.33891487 epoch total loss 1.28076339\n",
      "Trained batch 1190 batch loss 1.40258658 epoch total loss 1.28086579\n",
      "Trained batch 1191 batch loss 1.35955739 epoch total loss 1.28093171\n",
      "Trained batch 1192 batch loss 1.27876496 epoch total loss 1.28092992\n",
      "Trained batch 1193 batch loss 1.37855887 epoch total loss 1.28101182\n",
      "Trained batch 1194 batch loss 1.50459075 epoch total loss 1.2811991\n",
      "Trained batch 1195 batch loss 1.33776474 epoch total loss 1.28124642\n",
      "Trained batch 1196 batch loss 1.2446847 epoch total loss 1.28121579\n",
      "Trained batch 1197 batch loss 1.35958624 epoch total loss 1.28128135\n",
      "Trained batch 1198 batch loss 1.45369732 epoch total loss 1.28142524\n",
      "Trained batch 1199 batch loss 1.34740865 epoch total loss 1.28148031\n",
      "Trained batch 1200 batch loss 1.24487138 epoch total loss 1.28144979\n",
      "Trained batch 1201 batch loss 1.20604813 epoch total loss 1.28138697\n",
      "Trained batch 1202 batch loss 1.21197438 epoch total loss 1.28132915\n",
      "Trained batch 1203 batch loss 1.30505705 epoch total loss 1.28134894\n",
      "Trained batch 1204 batch loss 1.31416368 epoch total loss 1.28137624\n",
      "Trained batch 1205 batch loss 1.28855062 epoch total loss 1.2813822\n",
      "Trained batch 1206 batch loss 1.41206813 epoch total loss 1.28149056\n",
      "Trained batch 1207 batch loss 1.25036561 epoch total loss 1.28146482\n",
      "Trained batch 1208 batch loss 1.36066771 epoch total loss 1.28153038\n",
      "Trained batch 1209 batch loss 1.25297463 epoch total loss 1.28150678\n",
      "Trained batch 1210 batch loss 1.13166332 epoch total loss 1.28138292\n",
      "Trained batch 1211 batch loss 1.19648516 epoch total loss 1.28131294\n",
      "Trained batch 1212 batch loss 1.22581959 epoch total loss 1.28126717\n",
      "Trained batch 1213 batch loss 1.3799485 epoch total loss 1.28134847\n",
      "Trained batch 1214 batch loss 1.16008949 epoch total loss 1.28124857\n",
      "Trained batch 1215 batch loss 1.2151804 epoch total loss 1.28119421\n",
      "Trained batch 1216 batch loss 1.25408363 epoch total loss 1.28117192\n",
      "Trained batch 1217 batch loss 1.24910736 epoch total loss 1.28114557\n",
      "Trained batch 1218 batch loss 1.33508754 epoch total loss 1.28118992\n",
      "Trained batch 1219 batch loss 1.24997759 epoch total loss 1.28116429\n",
      "Trained batch 1220 batch loss 1.13015318 epoch total loss 1.28104043\n",
      "Trained batch 1221 batch loss 1.30343878 epoch total loss 1.28105879\n",
      "Trained batch 1222 batch loss 1.30066156 epoch total loss 1.28107488\n",
      "Trained batch 1223 batch loss 1.30755842 epoch total loss 1.28109658\n",
      "Trained batch 1224 batch loss 1.17829669 epoch total loss 1.28101265\n",
      "Trained batch 1225 batch loss 1.02037764 epoch total loss 1.28079987\n",
      "Trained batch 1226 batch loss 1.17178035 epoch total loss 1.28071094\n",
      "Trained batch 1227 batch loss 1.0749259 epoch total loss 1.28054321\n",
      "Trained batch 1228 batch loss 1.09155154 epoch total loss 1.28038931\n",
      "Trained batch 1229 batch loss 1.17466116 epoch total loss 1.28030336\n",
      "Trained batch 1230 batch loss 1.12372482 epoch total loss 1.28017604\n",
      "Trained batch 1231 batch loss 1.21961832 epoch total loss 1.28012693\n",
      "Trained batch 1232 batch loss 1.15750754 epoch total loss 1.28002727\n",
      "Trained batch 1233 batch loss 1.27569556 epoch total loss 1.28002369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1234 batch loss 1.28623641 epoch total loss 1.28002882\n",
      "Trained batch 1235 batch loss 1.22191012 epoch total loss 1.27998173\n",
      "Trained batch 1236 batch loss 1.29213762 epoch total loss 1.27999163\n",
      "Trained batch 1237 batch loss 1.23660338 epoch total loss 1.27995646\n",
      "Trained batch 1238 batch loss 1.38795495 epoch total loss 1.28004372\n",
      "Trained batch 1239 batch loss 1.17297339 epoch total loss 1.27995729\n",
      "Trained batch 1240 batch loss 1.18083215 epoch total loss 1.27987731\n",
      "Trained batch 1241 batch loss 1.02844977 epoch total loss 1.27967465\n",
      "Trained batch 1242 batch loss 1.14903128 epoch total loss 1.27956951\n",
      "Trained batch 1243 batch loss 1.28305364 epoch total loss 1.27957237\n",
      "Trained batch 1244 batch loss 1.292961 epoch total loss 1.2795831\n",
      "Trained batch 1245 batch loss 1.42591572 epoch total loss 1.27970064\n",
      "Trained batch 1246 batch loss 1.53049397 epoch total loss 1.27990198\n",
      "Trained batch 1247 batch loss 1.42875433 epoch total loss 1.28002131\n",
      "Trained batch 1248 batch loss 1.34255445 epoch total loss 1.28007138\n",
      "Trained batch 1249 batch loss 1.28368354 epoch total loss 1.28007424\n",
      "Trained batch 1250 batch loss 1.19435453 epoch total loss 1.28000569\n",
      "Trained batch 1251 batch loss 1.11754036 epoch total loss 1.27987576\n",
      "Trained batch 1252 batch loss 1.26710653 epoch total loss 1.27986562\n",
      "Trained batch 1253 batch loss 1.29034781 epoch total loss 1.27987397\n",
      "Trained batch 1254 batch loss 1.2964499 epoch total loss 1.27988732\n",
      "Trained batch 1255 batch loss 1.28841043 epoch total loss 1.27989411\n",
      "Trained batch 1256 batch loss 1.31873918 epoch total loss 1.27992499\n",
      "Trained batch 1257 batch loss 1.25905502 epoch total loss 1.27990842\n",
      "Trained batch 1258 batch loss 1.18249071 epoch total loss 1.27983093\n",
      "Trained batch 1259 batch loss 1.25738025 epoch total loss 1.27981305\n",
      "Trained batch 1260 batch loss 1.1867727 epoch total loss 1.27973926\n",
      "Trained batch 1261 batch loss 1.12528682 epoch total loss 1.27961671\n",
      "Trained batch 1262 batch loss 1.2347343 epoch total loss 1.27958119\n",
      "Trained batch 1263 batch loss 1.31809938 epoch total loss 1.27961171\n",
      "Trained batch 1264 batch loss 1.43017709 epoch total loss 1.2797308\n",
      "Trained batch 1265 batch loss 1.51845562 epoch total loss 1.27991951\n",
      "Trained batch 1266 batch loss 1.28149164 epoch total loss 1.2799207\n",
      "Trained batch 1267 batch loss 1.34573734 epoch total loss 1.27997267\n",
      "Trained batch 1268 batch loss 1.29112124 epoch total loss 1.27998149\n",
      "Trained batch 1269 batch loss 1.40912628 epoch total loss 1.2800833\n",
      "Trained batch 1270 batch loss 1.30948424 epoch total loss 1.28010643\n",
      "Trained batch 1271 batch loss 1.30673695 epoch total loss 1.28012741\n",
      "Trained batch 1272 batch loss 1.23505437 epoch total loss 1.280092\n",
      "Trained batch 1273 batch loss 1.22110856 epoch total loss 1.28004563\n",
      "Trained batch 1274 batch loss 1.30821228 epoch total loss 1.28006768\n",
      "Trained batch 1275 batch loss 1.29901612 epoch total loss 1.28008258\n",
      "Trained batch 1276 batch loss 1.28803432 epoch total loss 1.2800889\n",
      "Trained batch 1277 batch loss 1.13607228 epoch total loss 1.27997613\n",
      "Trained batch 1278 batch loss 1.24696183 epoch total loss 1.27995026\n",
      "Trained batch 1279 batch loss 1.24081242 epoch total loss 1.27991974\n",
      "Trained batch 1280 batch loss 1.26153362 epoch total loss 1.27990532\n",
      "Trained batch 1281 batch loss 1.40595913 epoch total loss 1.28000379\n",
      "Trained batch 1282 batch loss 1.2487452 epoch total loss 1.27997935\n",
      "Trained batch 1283 batch loss 1.00502813 epoch total loss 1.27976513\n",
      "Trained batch 1284 batch loss 1.21163762 epoch total loss 1.27971208\n",
      "Trained batch 1285 batch loss 1.38611412 epoch total loss 1.27979481\n",
      "Trained batch 1286 batch loss 1.36112046 epoch total loss 1.27985811\n",
      "Trained batch 1287 batch loss 1.40110815 epoch total loss 1.27995229\n",
      "Trained batch 1288 batch loss 1.29407871 epoch total loss 1.27996325\n",
      "Trained batch 1289 batch loss 1.26221812 epoch total loss 1.27994943\n",
      "Trained batch 1290 batch loss 1.34352541 epoch total loss 1.27999878\n",
      "Trained batch 1291 batch loss 1.27244329 epoch total loss 1.27999294\n",
      "Trained batch 1292 batch loss 1.19678199 epoch total loss 1.27992845\n",
      "Trained batch 1293 batch loss 1.23217928 epoch total loss 1.27989161\n",
      "Trained batch 1294 batch loss 1.29446959 epoch total loss 1.27990282\n",
      "Trained batch 1295 batch loss 1.25558794 epoch total loss 1.27988398\n",
      "Trained batch 1296 batch loss 1.275563 epoch total loss 1.27988064\n",
      "Trained batch 1297 batch loss 1.28779185 epoch total loss 1.27988684\n",
      "Trained batch 1298 batch loss 1.25699496 epoch total loss 1.27986908\n",
      "Trained batch 1299 batch loss 1.32791722 epoch total loss 1.27990615\n",
      "Trained batch 1300 batch loss 1.34606194 epoch total loss 1.27995694\n",
      "Trained batch 1301 batch loss 1.29906476 epoch total loss 1.27997172\n",
      "Trained batch 1302 batch loss 1.28640604 epoch total loss 1.27997661\n",
      "Trained batch 1303 batch loss 1.2396543 epoch total loss 1.27994561\n",
      "Trained batch 1304 batch loss 1.24159908 epoch total loss 1.27991617\n",
      "Trained batch 1305 batch loss 1.18501914 epoch total loss 1.27984357\n",
      "Trained batch 1306 batch loss 1.14126527 epoch total loss 1.27973735\n",
      "Trained batch 1307 batch loss 1.21401894 epoch total loss 1.27968705\n",
      "Trained batch 1308 batch loss 1.24428189 epoch total loss 1.27966\n",
      "Trained batch 1309 batch loss 1.34481645 epoch total loss 1.27970982\n",
      "Trained batch 1310 batch loss 1.22147226 epoch total loss 1.27966535\n",
      "Trained batch 1311 batch loss 1.19776356 epoch total loss 1.27960289\n",
      "Trained batch 1312 batch loss 1.21863508 epoch total loss 1.27955639\n",
      "Trained batch 1313 batch loss 1.28427863 epoch total loss 1.27956\n",
      "Trained batch 1314 batch loss 1.33257771 epoch total loss 1.27960026\n",
      "Trained batch 1315 batch loss 1.29008043 epoch total loss 1.27960825\n",
      "Trained batch 1316 batch loss 1.23195696 epoch total loss 1.27957201\n",
      "Trained batch 1317 batch loss 1.24646127 epoch total loss 1.27954686\n",
      "Trained batch 1318 batch loss 1.2392292 epoch total loss 1.27951634\n",
      "Trained batch 1319 batch loss 1.14659083 epoch total loss 1.27941549\n",
      "Trained batch 1320 batch loss 1.23017597 epoch total loss 1.27937829\n",
      "Trained batch 1321 batch loss 1.29244733 epoch total loss 1.27938819\n",
      "Trained batch 1322 batch loss 1.13960648 epoch total loss 1.27928245\n",
      "Trained batch 1323 batch loss 1.31646693 epoch total loss 1.27931058\n",
      "Trained batch 1324 batch loss 1.26124227 epoch total loss 1.27929688\n",
      "Trained batch 1325 batch loss 1.33533096 epoch total loss 1.27933919\n",
      "Trained batch 1326 batch loss 1.1972239 epoch total loss 1.27927732\n",
      "Trained batch 1327 batch loss 1.20810032 epoch total loss 1.27922368\n",
      "Trained batch 1328 batch loss 1.09799218 epoch total loss 1.27908719\n",
      "Trained batch 1329 batch loss 1.14514494 epoch total loss 1.27898645\n",
      "Trained batch 1330 batch loss 1.1665659 epoch total loss 1.27890193\n",
      "Trained batch 1331 batch loss 1.07943797 epoch total loss 1.27875209\n",
      "Trained batch 1332 batch loss 1.14352906 epoch total loss 1.27865064\n",
      "Trained batch 1333 batch loss 1.18140793 epoch total loss 1.27857769\n",
      "Trained batch 1334 batch loss 1.13407123 epoch total loss 1.27846932\n",
      "Trained batch 1335 batch loss 1.11146796 epoch total loss 1.27834415\n",
      "Trained batch 1336 batch loss 1.21422303 epoch total loss 1.27829623\n",
      "Trained batch 1337 batch loss 1.50712979 epoch total loss 1.2784673\n",
      "Trained batch 1338 batch loss 1.19304621 epoch total loss 1.2784034\n",
      "Trained batch 1339 batch loss 1.20280385 epoch total loss 1.2783469\n",
      "Trained batch 1340 batch loss 1.28307676 epoch total loss 1.27835047\n",
      "Trained batch 1341 batch loss 1.15187764 epoch total loss 1.27825618\n",
      "Trained batch 1342 batch loss 1.22082269 epoch total loss 1.27821338\n",
      "Trained batch 1343 batch loss 1.26002836 epoch total loss 1.27819979\n",
      "Trained batch 1344 batch loss 1.49273455 epoch total loss 1.27835941\n",
      "Trained batch 1345 batch loss 1.22420633 epoch total loss 1.27831912\n",
      "Trained batch 1346 batch loss 1.08680117 epoch total loss 1.27817678\n",
      "Trained batch 1347 batch loss 1.19327867 epoch total loss 1.27811372\n",
      "Trained batch 1348 batch loss 1.14129007 epoch total loss 1.27801228\n",
      "Trained batch 1349 batch loss 1.21708655 epoch total loss 1.2779671\n",
      "Trained batch 1350 batch loss 1.25838 epoch total loss 1.27795255\n",
      "Trained batch 1351 batch loss 1.31182957 epoch total loss 1.27797771\n",
      "Trained batch 1352 batch loss 1.23920321 epoch total loss 1.27794909\n",
      "Trained batch 1353 batch loss 1.32673359 epoch total loss 1.2779851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1354 batch loss 1.32288218 epoch total loss 1.27801824\n",
      "Trained batch 1355 batch loss 1.34539592 epoch total loss 1.27806795\n",
      "Trained batch 1356 batch loss 1.27418709 epoch total loss 1.27806509\n",
      "Trained batch 1357 batch loss 1.25633872 epoch total loss 1.27804911\n",
      "Trained batch 1358 batch loss 1.18391991 epoch total loss 1.27797985\n",
      "Trained batch 1359 batch loss 1.21915317 epoch total loss 1.27793646\n",
      "Trained batch 1360 batch loss 1.27887547 epoch total loss 1.27793717\n",
      "Trained batch 1361 batch loss 1.29081929 epoch total loss 1.27794671\n",
      "Trained batch 1362 batch loss 1.26329637 epoch total loss 1.27793586\n",
      "Trained batch 1363 batch loss 1.40004098 epoch total loss 1.27802551\n",
      "Trained batch 1364 batch loss 1.15866816 epoch total loss 1.27793801\n",
      "Trained batch 1365 batch loss 1.15193558 epoch total loss 1.27784574\n",
      "Trained batch 1366 batch loss 1.3606658 epoch total loss 1.27790642\n",
      "Trained batch 1367 batch loss 1.22517693 epoch total loss 1.27786779\n",
      "Trained batch 1368 batch loss 1.39473605 epoch total loss 1.27795327\n",
      "Trained batch 1369 batch loss 1.35768509 epoch total loss 1.27801156\n",
      "Trained batch 1370 batch loss 1.25396514 epoch total loss 1.27799392\n",
      "Trained batch 1371 batch loss 1.39973462 epoch total loss 1.27808273\n",
      "Trained batch 1372 batch loss 1.38969755 epoch total loss 1.27816403\n",
      "Trained batch 1373 batch loss 1.36359322 epoch total loss 1.27822638\n",
      "Trained batch 1374 batch loss 1.25368738 epoch total loss 1.27820849\n",
      "Trained batch 1375 batch loss 1.24528384 epoch total loss 1.27818453\n",
      "Trained batch 1376 batch loss 1.21485925 epoch total loss 1.2781384\n",
      "Trained batch 1377 batch loss 1.26271379 epoch total loss 1.27812719\n",
      "Trained batch 1378 batch loss 1.27138972 epoch total loss 1.27812231\n",
      "Trained batch 1379 batch loss 1.37620425 epoch total loss 1.27819347\n",
      "Trained batch 1380 batch loss 1.40770376 epoch total loss 1.27828729\n",
      "Trained batch 1381 batch loss 1.39124918 epoch total loss 1.27836907\n",
      "Trained batch 1382 batch loss 1.43966949 epoch total loss 1.27848589\n",
      "Trained batch 1383 batch loss 1.40938878 epoch total loss 1.27858055\n",
      "Trained batch 1384 batch loss 1.41887712 epoch total loss 1.27868187\n",
      "Trained batch 1385 batch loss 1.35963655 epoch total loss 1.27874029\n",
      "Trained batch 1386 batch loss 1.32322371 epoch total loss 1.27877235\n",
      "Trained batch 1387 batch loss 1.34784818 epoch total loss 1.27882218\n",
      "Trained batch 1388 batch loss 1.3860091 epoch total loss 1.27889943\n",
      "Epoch 3 train loss 1.2788994312286377\n",
      "Validated batch 1 batch loss 1.25645566\n",
      "Validated batch 2 batch loss 1.23397696\n",
      "Validated batch 3 batch loss 1.1826787\n",
      "Validated batch 4 batch loss 1.31610823\n",
      "Validated batch 5 batch loss 1.21275067\n",
      "Validated batch 6 batch loss 1.31397724\n",
      "Validated batch 7 batch loss 1.35032928\n",
      "Validated batch 8 batch loss 1.30167806\n",
      "Validated batch 9 batch loss 1.34068584\n",
      "Validated batch 10 batch loss 1.26080894\n",
      "Validated batch 11 batch loss 1.33783746\n",
      "Validated batch 12 batch loss 1.25356412\n",
      "Validated batch 13 batch loss 1.26736414\n",
      "Validated batch 14 batch loss 1.39374948\n",
      "Validated batch 15 batch loss 1.3488071\n",
      "Validated batch 16 batch loss 1.27344871\n",
      "Validated batch 17 batch loss 1.41798687\n",
      "Validated batch 18 batch loss 1.15152192\n",
      "Validated batch 19 batch loss 1.3274076\n",
      "Validated batch 20 batch loss 1.10781217\n",
      "Validated batch 21 batch loss 1.3159306\n",
      "Validated batch 22 batch loss 1.34956837\n",
      "Validated batch 23 batch loss 1.20608532\n",
      "Validated batch 24 batch loss 1.24250495\n",
      "Validated batch 25 batch loss 1.22027349\n",
      "Validated batch 26 batch loss 1.22885215\n",
      "Validated batch 27 batch loss 1.19896519\n",
      "Validated batch 28 batch loss 1.15639222\n",
      "Validated batch 29 batch loss 1.3121897\n",
      "Validated batch 30 batch loss 1.23291445\n",
      "Validated batch 31 batch loss 1.14144635\n",
      "Validated batch 32 batch loss 1.23387885\n",
      "Validated batch 33 batch loss 1.24536347\n",
      "Validated batch 34 batch loss 1.20593715\n",
      "Validated batch 35 batch loss 1.23738813\n",
      "Validated batch 36 batch loss 1.26669168\n",
      "Validated batch 37 batch loss 1.21154392\n",
      "Validated batch 38 batch loss 1.31066036\n",
      "Validated batch 39 batch loss 1.35905182\n",
      "Validated batch 40 batch loss 1.23128676\n",
      "Validated batch 41 batch loss 1.33571184\n",
      "Validated batch 42 batch loss 1.10010552\n",
      "Validated batch 43 batch loss 1.21461654\n",
      "Validated batch 44 batch loss 1.16849566\n",
      "Validated batch 45 batch loss 1.28561223\n",
      "Validated batch 46 batch loss 1.43094301\n",
      "Validated batch 47 batch loss 1.28408253\n",
      "Validated batch 48 batch loss 1.25754035\n",
      "Validated batch 49 batch loss 1.2509799\n",
      "Validated batch 50 batch loss 1.25011027\n",
      "Validated batch 51 batch loss 1.30980861\n",
      "Validated batch 52 batch loss 1.39908957\n",
      "Validated batch 53 batch loss 1.20776153\n",
      "Validated batch 54 batch loss 1.31740212\n",
      "Validated batch 55 batch loss 1.24971092\n",
      "Validated batch 56 batch loss 1.296278\n",
      "Validated batch 57 batch loss 1.29606485\n",
      "Validated batch 58 batch loss 1.17028248\n",
      "Validated batch 59 batch loss 1.44205213\n",
      "Validated batch 60 batch loss 1.21782923\n",
      "Validated batch 61 batch loss 1.34719861\n",
      "Validated batch 62 batch loss 1.23476\n",
      "Validated batch 63 batch loss 1.35669744\n",
      "Validated batch 64 batch loss 1.14716661\n",
      "Validated batch 65 batch loss 1.26617026\n",
      "Validated batch 66 batch loss 1.22090411\n",
      "Validated batch 67 batch loss 1.22060752\n",
      "Validated batch 68 batch loss 1.30570304\n",
      "Validated batch 69 batch loss 1.26724708\n",
      "Validated batch 70 batch loss 1.21391082\n",
      "Validated batch 71 batch loss 1.2949388\n",
      "Validated batch 72 batch loss 1.33455849\n",
      "Validated batch 73 batch loss 1.22675586\n",
      "Validated batch 74 batch loss 1.33004951\n",
      "Validated batch 75 batch loss 1.42160928\n",
      "Validated batch 76 batch loss 1.13979173\n",
      "Validated batch 77 batch loss 1.26443243\n",
      "Validated batch 78 batch loss 1.27133763\n",
      "Validated batch 79 batch loss 1.33715796\n",
      "Validated batch 80 batch loss 1.29944074\n",
      "Validated batch 81 batch loss 1.1834209\n",
      "Validated batch 82 batch loss 1.10386\n",
      "Validated batch 83 batch loss 1.29677868\n",
      "Validated batch 84 batch loss 1.25120878\n",
      "Validated batch 85 batch loss 1.22411275\n",
      "Validated batch 86 batch loss 1.30390453\n",
      "Validated batch 87 batch loss 1.20815134\n",
      "Validated batch 88 batch loss 1.29087842\n",
      "Validated batch 89 batch loss 1.33486879\n",
      "Validated batch 90 batch loss 1.30829823\n",
      "Validated batch 91 batch loss 1.26486135\n",
      "Validated batch 92 batch loss 1.17416227\n",
      "Validated batch 93 batch loss 1.2880069\n",
      "Validated batch 94 batch loss 1.33527851\n",
      "Validated batch 95 batch loss 1.22702658\n",
      "Validated batch 96 batch loss 1.27496517\n",
      "Validated batch 97 batch loss 1.25149107\n",
      "Validated batch 98 batch loss 1.25853145\n",
      "Validated batch 99 batch loss 1.26980376\n",
      "Validated batch 100 batch loss 1.25537241\n",
      "Validated batch 101 batch loss 1.20346439\n",
      "Validated batch 102 batch loss 1.36154854\n",
      "Validated batch 103 batch loss 1.19545805\n",
      "Validated batch 104 batch loss 1.17636871\n",
      "Validated batch 105 batch loss 1.25017786\n",
      "Validated batch 106 batch loss 1.37095547\n",
      "Validated batch 107 batch loss 1.33241129\n",
      "Validated batch 108 batch loss 1.37369108\n",
      "Validated batch 109 batch loss 1.19142222\n",
      "Validated batch 110 batch loss 1.3700242\n",
      "Validated batch 111 batch loss 1.24886501\n",
      "Validated batch 112 batch loss 1.32505238\n",
      "Validated batch 113 batch loss 1.30973828\n",
      "Validated batch 114 batch loss 1.03886724\n",
      "Validated batch 115 batch loss 1.25228858\n",
      "Validated batch 116 batch loss 1.24144673\n",
      "Validated batch 117 batch loss 1.15032816\n",
      "Validated batch 118 batch loss 1.28021502\n",
      "Validated batch 119 batch loss 1.19685125\n",
      "Validated batch 120 batch loss 1.223858\n",
      "Validated batch 121 batch loss 1.2667563\n",
      "Validated batch 122 batch loss 1.23432732\n",
      "Validated batch 123 batch loss 1.27189744\n",
      "Validated batch 124 batch loss 1.3078506\n",
      "Validated batch 125 batch loss 1.17976952\n",
      "Validated batch 126 batch loss 1.33231902\n",
      "Validated batch 127 batch loss 1.30325365\n",
      "Validated batch 128 batch loss 1.17443645\n",
      "Validated batch 129 batch loss 1.26611972\n",
      "Validated batch 130 batch loss 1.26089728\n",
      "Validated batch 131 batch loss 1.23945308\n",
      "Validated batch 132 batch loss 1.40315\n",
      "Validated batch 133 batch loss 1.29183531\n",
      "Validated batch 134 batch loss 1.26997972\n",
      "Validated batch 135 batch loss 1.26288176\n",
      "Validated batch 136 batch loss 1.24270236\n",
      "Validated batch 137 batch loss 1.24745011\n",
      "Validated batch 138 batch loss 1.23965478\n",
      "Validated batch 139 batch loss 1.19333708\n",
      "Validated batch 140 batch loss 1.2462877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 141 batch loss 1.21615601\n",
      "Validated batch 142 batch loss 1.20324898\n",
      "Validated batch 143 batch loss 1.19331717\n",
      "Validated batch 144 batch loss 1.33269477\n",
      "Validated batch 145 batch loss 1.24310613\n",
      "Validated batch 146 batch loss 1.34193659\n",
      "Validated batch 147 batch loss 1.34937167\n",
      "Validated batch 148 batch loss 1.32516646\n",
      "Validated batch 149 batch loss 1.27441239\n",
      "Validated batch 150 batch loss 1.38547182\n",
      "Validated batch 151 batch loss 1.28732896\n",
      "Validated batch 152 batch loss 1.32435036\n",
      "Validated batch 153 batch loss 1.34097266\n",
      "Validated batch 154 batch loss 1.413257\n",
      "Validated batch 155 batch loss 1.33220267\n",
      "Validated batch 156 batch loss 1.22132409\n",
      "Validated batch 157 batch loss 1.27726901\n",
      "Validated batch 158 batch loss 1.34649706\n",
      "Validated batch 159 batch loss 1.32967889\n",
      "Validated batch 160 batch loss 1.23281479\n",
      "Validated batch 161 batch loss 1.26782608\n",
      "Validated batch 162 batch loss 1.32127237\n",
      "Validated batch 163 batch loss 1.27450919\n",
      "Validated batch 164 batch loss 1.28572369\n",
      "Validated batch 165 batch loss 1.25366902\n",
      "Validated batch 166 batch loss 1.15281153\n",
      "Validated batch 167 batch loss 1.30750716\n",
      "Validated batch 168 batch loss 1.30484605\n",
      "Validated batch 169 batch loss 1.21007013\n",
      "Validated batch 170 batch loss 1.17809165\n",
      "Validated batch 171 batch loss 1.29200983\n",
      "Validated batch 172 batch loss 1.25279498\n",
      "Validated batch 173 batch loss 1.34672451\n",
      "Validated batch 174 batch loss 1.27882218\n",
      "Validated batch 175 batch loss 1.18198204\n",
      "Validated batch 176 batch loss 1.26926577\n",
      "Validated batch 177 batch loss 1.256459\n",
      "Validated batch 178 batch loss 1.26591277\n",
      "Validated batch 179 batch loss 1.32393956\n",
      "Validated batch 180 batch loss 1.33399618\n",
      "Validated batch 181 batch loss 1.43790913\n",
      "Validated batch 182 batch loss 1.46636474\n",
      "Validated batch 183 batch loss 1.28271675\n",
      "Validated batch 184 batch loss 1.23312795\n",
      "Validated batch 185 batch loss 1.13587368\n",
      "Epoch 3 val loss 1.2695021629333496\n",
      "Model /aiffel/aiffel/mpii/a/model-epoch-3-loss-1.2695.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.25209928 epoch total loss 1.25209928\n",
      "Trained batch 2 batch loss 1.20976365 epoch total loss 1.23093152\n",
      "Trained batch 3 batch loss 1.18859184 epoch total loss 1.21681833\n",
      "Trained batch 4 batch loss 1.19630671 epoch total loss 1.21169043\n",
      "Trained batch 5 batch loss 1.16085339 epoch total loss 1.20152307\n",
      "Trained batch 6 batch loss 1.23080122 epoch total loss 1.20640266\n",
      "Trained batch 7 batch loss 1.24974489 epoch total loss 1.21259439\n",
      "Trained batch 8 batch loss 1.15483308 epoch total loss 1.20537424\n",
      "Trained batch 9 batch loss 1.08500433 epoch total loss 1.19199991\n",
      "Trained batch 10 batch loss 1.01620638 epoch total loss 1.1744206\n",
      "Trained batch 11 batch loss 1.08417296 epoch total loss 1.16621625\n",
      "Trained batch 12 batch loss 1.27527237 epoch total loss 1.17530429\n",
      "Trained batch 13 batch loss 1.24379253 epoch total loss 1.18057263\n",
      "Trained batch 14 batch loss 1.23350573 epoch total loss 1.18435359\n",
      "Trained batch 15 batch loss 1.32714796 epoch total loss 1.19387317\n",
      "Trained batch 16 batch loss 1.21554613 epoch total loss 1.19522774\n",
      "Trained batch 17 batch loss 1.11221 epoch total loss 1.19034433\n",
      "Trained batch 18 batch loss 1.16821837 epoch total loss 1.18911505\n",
      "Trained batch 19 batch loss 1.25951791 epoch total loss 1.19282055\n",
      "Trained batch 20 batch loss 1.16319871 epoch total loss 1.19133937\n",
      "Trained batch 21 batch loss 1.34545016 epoch total loss 1.19867802\n",
      "Trained batch 22 batch loss 1.28631735 epoch total loss 1.20266163\n",
      "Trained batch 23 batch loss 1.21245766 epoch total loss 1.20308757\n",
      "Trained batch 24 batch loss 1.27451718 epoch total loss 1.20606375\n",
      "Trained batch 25 batch loss 1.26879084 epoch total loss 1.20857286\n",
      "Trained batch 26 batch loss 1.29426646 epoch total loss 1.21186876\n",
      "Trained batch 27 batch loss 1.40038633 epoch total loss 1.21885085\n",
      "Trained batch 28 batch loss 1.22342587 epoch total loss 1.21901429\n",
      "Trained batch 29 batch loss 1.32334614 epoch total loss 1.2226119\n",
      "Trained batch 30 batch loss 1.46435511 epoch total loss 1.23067009\n",
      "Trained batch 31 batch loss 1.40641141 epoch total loss 1.23633909\n",
      "Trained batch 32 batch loss 1.29077339 epoch total loss 1.23804021\n",
      "Trained batch 33 batch loss 1.26520276 epoch total loss 1.23886323\n",
      "Trained batch 34 batch loss 1.16488123 epoch total loss 1.23668742\n",
      "Trained batch 35 batch loss 1.10983908 epoch total loss 1.23306322\n",
      "Trained batch 36 batch loss 1.26852715 epoch total loss 1.23404837\n",
      "Trained batch 37 batch loss 1.29912686 epoch total loss 1.23580718\n",
      "Trained batch 38 batch loss 1.30369556 epoch total loss 1.23759365\n",
      "Trained batch 39 batch loss 1.20820737 epoch total loss 1.23684013\n",
      "Trained batch 40 batch loss 1.17975521 epoch total loss 1.23541307\n",
      "Trained batch 41 batch loss 1.16894472 epoch total loss 1.23379195\n",
      "Trained batch 42 batch loss 1.25381017 epoch total loss 1.23426855\n",
      "Trained batch 43 batch loss 1.21065342 epoch total loss 1.23371935\n",
      "Trained batch 44 batch loss 1.21283937 epoch total loss 1.2332449\n",
      "Trained batch 45 batch loss 1.28509319 epoch total loss 1.23439705\n",
      "Trained batch 46 batch loss 1.50462747 epoch total loss 1.24027157\n",
      "Trained batch 47 batch loss 1.47087193 epoch total loss 1.24517798\n",
      "Trained batch 48 batch loss 1.30283082 epoch total loss 1.24637902\n",
      "Trained batch 49 batch loss 1.28003454 epoch total loss 1.2470659\n",
      "Trained batch 50 batch loss 1.26325393 epoch total loss 1.24738955\n",
      "Trained batch 51 batch loss 1.19177 epoch total loss 1.24629903\n",
      "Trained batch 52 batch loss 1.19496882 epoch total loss 1.24531186\n",
      "Trained batch 53 batch loss 1.25381505 epoch total loss 1.24547231\n",
      "Trained batch 54 batch loss 1.25717783 epoch total loss 1.24568915\n",
      "Trained batch 55 batch loss 1.1197269 epoch total loss 1.2433989\n",
      "Trained batch 56 batch loss 1.08320224 epoch total loss 1.24053824\n",
      "Trained batch 57 batch loss 1.08178973 epoch total loss 1.23775303\n",
      "Trained batch 58 batch loss 1.04721951 epoch total loss 1.23446798\n",
      "Trained batch 59 batch loss 1.29051733 epoch total loss 1.23541808\n",
      "Trained batch 60 batch loss 1.55405855 epoch total loss 1.24072874\n",
      "Trained batch 61 batch loss 1.54437113 epoch total loss 1.24570656\n",
      "Trained batch 62 batch loss 1.46754646 epoch total loss 1.24928451\n",
      "Trained batch 63 batch loss 1.38904727 epoch total loss 1.25150299\n",
      "Trained batch 64 batch loss 1.37919056 epoch total loss 1.25349808\n",
      "Trained batch 65 batch loss 1.36879635 epoch total loss 1.25527191\n",
      "Trained batch 66 batch loss 1.46883118 epoch total loss 1.25850773\n",
      "Trained batch 67 batch loss 1.27254546 epoch total loss 1.25871718\n",
      "Trained batch 68 batch loss 1.28023863 epoch total loss 1.25903368\n",
      "Trained batch 69 batch loss 1.27756572 epoch total loss 1.25930226\n",
      "Trained batch 70 batch loss 1.27145779 epoch total loss 1.25947595\n",
      "Trained batch 71 batch loss 1.26691818 epoch total loss 1.25958085\n",
      "Trained batch 72 batch loss 1.25829089 epoch total loss 1.25956297\n",
      "Trained batch 73 batch loss 1.25711048 epoch total loss 1.25952935\n",
      "Trained batch 74 batch loss 1.37152886 epoch total loss 1.26104283\n",
      "Trained batch 75 batch loss 1.40558553 epoch total loss 1.26297009\n",
      "Trained batch 76 batch loss 1.31030393 epoch total loss 1.26359284\n",
      "Trained batch 77 batch loss 1.40626609 epoch total loss 1.26544571\n",
      "Trained batch 78 batch loss 1.21763253 epoch total loss 1.26483285\n",
      "Trained batch 79 batch loss 1.18441284 epoch total loss 1.26381481\n",
      "Trained batch 80 batch loss 1.23560929 epoch total loss 1.26346231\n",
      "Trained batch 81 batch loss 1.3129276 epoch total loss 1.2640729\n",
      "Trained batch 82 batch loss 1.30256581 epoch total loss 1.26454234\n",
      "Trained batch 83 batch loss 1.16981804 epoch total loss 1.26340103\n",
      "Trained batch 84 batch loss 1.24662232 epoch total loss 1.26320136\n",
      "Trained batch 85 batch loss 1.33888149 epoch total loss 1.26409173\n",
      "Trained batch 86 batch loss 1.18987012 epoch total loss 1.26322865\n",
      "Trained batch 87 batch loss 1.25058484 epoch total loss 1.26308334\n",
      "Trained batch 88 batch loss 1.28667665 epoch total loss 1.26335144\n",
      "Trained batch 89 batch loss 1.20331597 epoch total loss 1.26267684\n",
      "Trained batch 90 batch loss 1.15774155 epoch total loss 1.26151085\n",
      "Trained batch 91 batch loss 1.12451768 epoch total loss 1.26000547\n",
      "Trained batch 92 batch loss 1.09535503 epoch total loss 1.25821579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 93 batch loss 1.10378313 epoch total loss 1.2565552\n",
      "Trained batch 94 batch loss 1.18371201 epoch total loss 1.25578034\n",
      "Trained batch 95 batch loss 1.35931 epoch total loss 1.25687\n",
      "Trained batch 96 batch loss 1.36215675 epoch total loss 1.25796688\n",
      "Trained batch 97 batch loss 1.44111407 epoch total loss 1.25985503\n",
      "Trained batch 98 batch loss 1.24955475 epoch total loss 1.25974989\n",
      "Trained batch 99 batch loss 1.30873609 epoch total loss 1.26024473\n",
      "Trained batch 100 batch loss 1.27717602 epoch total loss 1.260414\n",
      "Trained batch 101 batch loss 1.15772843 epoch total loss 1.25939739\n",
      "Trained batch 102 batch loss 1.15629804 epoch total loss 1.25838661\n",
      "Trained batch 103 batch loss 1.18174028 epoch total loss 1.25764263\n",
      "Trained batch 104 batch loss 1.25333834 epoch total loss 1.25760126\n",
      "Trained batch 105 batch loss 1.28859293 epoch total loss 1.2578963\n",
      "Trained batch 106 batch loss 1.28154564 epoch total loss 1.25811934\n",
      "Trained batch 107 batch loss 1.25381756 epoch total loss 1.25807917\n",
      "Trained batch 108 batch loss 1.18536615 epoch total loss 1.25740588\n",
      "Trained batch 109 batch loss 1.15428185 epoch total loss 1.25645983\n",
      "Trained batch 110 batch loss 1.11723208 epoch total loss 1.25519407\n",
      "Trained batch 111 batch loss 1.21062732 epoch total loss 1.25479269\n",
      "Trained batch 112 batch loss 1.13295591 epoch total loss 1.25370479\n",
      "Trained batch 113 batch loss 1.20402956 epoch total loss 1.25326514\n",
      "Trained batch 114 batch loss 1.23981035 epoch total loss 1.25314701\n",
      "Trained batch 115 batch loss 1.28266275 epoch total loss 1.25340378\n",
      "Trained batch 116 batch loss 1.24032629 epoch total loss 1.25329101\n",
      "Trained batch 117 batch loss 1.40056515 epoch total loss 1.25454974\n",
      "Trained batch 118 batch loss 1.28291476 epoch total loss 1.25479007\n",
      "Trained batch 119 batch loss 1.27146614 epoch total loss 1.25493026\n",
      "Trained batch 120 batch loss 1.12395835 epoch total loss 1.2538389\n",
      "Trained batch 121 batch loss 1.26324332 epoch total loss 1.25391662\n",
      "Trained batch 122 batch loss 1.34265447 epoch total loss 1.25464392\n",
      "Trained batch 123 batch loss 1.34858179 epoch total loss 1.25540769\n",
      "Trained batch 124 batch loss 1.25955486 epoch total loss 1.25544107\n",
      "Trained batch 125 batch loss 1.2194016 epoch total loss 1.25515282\n",
      "Trained batch 126 batch loss 1.40180182 epoch total loss 1.25631666\n",
      "Trained batch 127 batch loss 1.32904017 epoch total loss 1.25688922\n",
      "Trained batch 128 batch loss 1.36798811 epoch total loss 1.25775719\n",
      "Trained batch 129 batch loss 1.2134974 epoch total loss 1.2574141\n",
      "Trained batch 130 batch loss 1.19915831 epoch total loss 1.25696599\n",
      "Trained batch 131 batch loss 1.15670848 epoch total loss 1.25620067\n",
      "Trained batch 132 batch loss 1.14080548 epoch total loss 1.25532651\n",
      "Trained batch 133 batch loss 1.19947481 epoch total loss 1.25490654\n",
      "Trained batch 134 batch loss 1.20537758 epoch total loss 1.25453699\n",
      "Trained batch 135 batch loss 1.12755525 epoch total loss 1.25359631\n",
      "Trained batch 136 batch loss 1.26885545 epoch total loss 1.2537086\n",
      "Trained batch 137 batch loss 1.3175528 epoch total loss 1.25417459\n",
      "Trained batch 138 batch loss 1.33672285 epoch total loss 1.25477266\n",
      "Trained batch 139 batch loss 1.24633384 epoch total loss 1.25471199\n",
      "Trained batch 140 batch loss 1.26808572 epoch total loss 1.25480747\n",
      "Trained batch 141 batch loss 1.27270269 epoch total loss 1.25493443\n",
      "Trained batch 142 batch loss 1.14026213 epoch total loss 1.25412691\n",
      "Trained batch 143 batch loss 1.10954762 epoch total loss 1.25311577\n",
      "Trained batch 144 batch loss 1.21073508 epoch total loss 1.25282145\n",
      "Trained batch 145 batch loss 1.25637507 epoch total loss 1.252846\n",
      "Trained batch 146 batch loss 1.38707638 epoch total loss 1.25376534\n",
      "Trained batch 147 batch loss 1.27152646 epoch total loss 1.25388622\n",
      "Trained batch 148 batch loss 1.20479202 epoch total loss 1.25355446\n",
      "Trained batch 149 batch loss 1.2200737 epoch total loss 1.25332975\n",
      "Trained batch 150 batch loss 1.2248342 epoch total loss 1.25313985\n",
      "Trained batch 151 batch loss 1.10359955 epoch total loss 1.25214958\n",
      "Trained batch 152 batch loss 1.12424421 epoch total loss 1.25130808\n",
      "Trained batch 153 batch loss 1.2159555 epoch total loss 1.25107694\n",
      "Trained batch 154 batch loss 1.40910983 epoch total loss 1.25210309\n",
      "Trained batch 155 batch loss 1.33743286 epoch total loss 1.2526536\n",
      "Trained batch 156 batch loss 1.26292264 epoch total loss 1.25271952\n",
      "Trained batch 157 batch loss 1.40484345 epoch total loss 1.25368845\n",
      "Trained batch 158 batch loss 1.30615 epoch total loss 1.25402045\n",
      "Trained batch 159 batch loss 1.26752782 epoch total loss 1.25410545\n",
      "Trained batch 160 batch loss 1.15325403 epoch total loss 1.25347519\n",
      "Trained batch 161 batch loss 1.23576558 epoch total loss 1.25336516\n",
      "Trained batch 162 batch loss 1.46826482 epoch total loss 1.25469172\n",
      "Trained batch 163 batch loss 1.40834045 epoch total loss 1.25563431\n",
      "Trained batch 164 batch loss 1.25282645 epoch total loss 1.25561714\n",
      "Trained batch 165 batch loss 1.2612865 epoch total loss 1.25565159\n",
      "Trained batch 166 batch loss 1.32747889 epoch total loss 1.25608432\n",
      "Trained batch 167 batch loss 1.14018369 epoch total loss 1.25539029\n",
      "Trained batch 168 batch loss 1.38375115 epoch total loss 1.25615442\n",
      "Trained batch 169 batch loss 1.36556101 epoch total loss 1.25680172\n",
      "Trained batch 170 batch loss 1.35899472 epoch total loss 1.2574029\n",
      "Trained batch 171 batch loss 1.30775404 epoch total loss 1.25769734\n",
      "Trained batch 172 batch loss 1.1748656 epoch total loss 1.25721574\n",
      "Trained batch 173 batch loss 1.04653811 epoch total loss 1.2559979\n",
      "Trained batch 174 batch loss 1.28592622 epoch total loss 1.25616992\n",
      "Trained batch 175 batch loss 1.29901767 epoch total loss 1.25641465\n",
      "Trained batch 176 batch loss 1.21082711 epoch total loss 1.25615573\n",
      "Trained batch 177 batch loss 1.15013433 epoch total loss 1.2555567\n",
      "Trained batch 178 batch loss 1.15320587 epoch total loss 1.25498176\n",
      "Trained batch 179 batch loss 1.10618556 epoch total loss 1.25415051\n",
      "Trained batch 180 batch loss 1.19524598 epoch total loss 1.25382328\n",
      "Trained batch 181 batch loss 1.11607587 epoch total loss 1.25306225\n",
      "Trained batch 182 batch loss 1.29655933 epoch total loss 1.25330114\n",
      "Trained batch 183 batch loss 1.23006368 epoch total loss 1.25317419\n",
      "Trained batch 184 batch loss 1.19301391 epoch total loss 1.25284719\n",
      "Trained batch 185 batch loss 1.28774881 epoch total loss 1.25303578\n",
      "Trained batch 186 batch loss 1.26305389 epoch total loss 1.25308967\n",
      "Trained batch 187 batch loss 1.42880607 epoch total loss 1.25402927\n",
      "Trained batch 188 batch loss 1.48009586 epoch total loss 1.25523186\n",
      "Trained batch 189 batch loss 1.4221133 epoch total loss 1.25611484\n",
      "Trained batch 190 batch loss 1.31364393 epoch total loss 1.25641763\n",
      "Trained batch 191 batch loss 1.14943302 epoch total loss 1.25585747\n",
      "Trained batch 192 batch loss 1.29026425 epoch total loss 1.25603664\n",
      "Trained batch 193 batch loss 1.39579606 epoch total loss 1.25676084\n",
      "Trained batch 194 batch loss 1.36359572 epoch total loss 1.25731158\n",
      "Trained batch 195 batch loss 1.23348784 epoch total loss 1.25718939\n",
      "Trained batch 196 batch loss 1.3123399 epoch total loss 1.25747085\n",
      "Trained batch 197 batch loss 1.25953078 epoch total loss 1.25748134\n",
      "Trained batch 198 batch loss 1.30462325 epoch total loss 1.2577194\n",
      "Trained batch 199 batch loss 1.30202043 epoch total loss 1.25794196\n",
      "Trained batch 200 batch loss 1.13446569 epoch total loss 1.25732458\n",
      "Trained batch 201 batch loss 1.09584916 epoch total loss 1.25652122\n",
      "Trained batch 202 batch loss 1.06888545 epoch total loss 1.25559235\n",
      "Trained batch 203 batch loss 1.24394131 epoch total loss 1.25553501\n",
      "Trained batch 204 batch loss 1.34979987 epoch total loss 1.25599706\n",
      "Trained batch 205 batch loss 1.36260378 epoch total loss 1.25651705\n",
      "Trained batch 206 batch loss 1.37539363 epoch total loss 1.25709414\n",
      "Trained batch 207 batch loss 1.2574141 epoch total loss 1.25709569\n",
      "Trained batch 208 batch loss 1.31600928 epoch total loss 1.25737894\n",
      "Trained batch 209 batch loss 1.37844718 epoch total loss 1.25795817\n",
      "Trained batch 210 batch loss 1.27613318 epoch total loss 1.25804472\n",
      "Trained batch 211 batch loss 1.22070372 epoch total loss 1.25786769\n",
      "Trained batch 212 batch loss 1.32670724 epoch total loss 1.25819254\n",
      "Trained batch 213 batch loss 1.30763924 epoch total loss 1.25842476\n",
      "Trained batch 214 batch loss 1.37313747 epoch total loss 1.25896072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 215 batch loss 1.25421059 epoch total loss 1.25893867\n",
      "Trained batch 216 batch loss 1.24575114 epoch total loss 1.25887764\n",
      "Trained batch 217 batch loss 1.28220749 epoch total loss 1.25898516\n",
      "Trained batch 218 batch loss 1.32047331 epoch total loss 1.25926709\n",
      "Trained batch 219 batch loss 1.23420691 epoch total loss 1.25915265\n",
      "Trained batch 220 batch loss 1.19582498 epoch total loss 1.25886476\n",
      "Trained batch 221 batch loss 1.34734416 epoch total loss 1.25926518\n",
      "Trained batch 222 batch loss 1.28781164 epoch total loss 1.25939381\n",
      "Trained batch 223 batch loss 1.26735032 epoch total loss 1.25942957\n",
      "Trained batch 224 batch loss 1.2518692 epoch total loss 1.25939572\n",
      "Trained batch 225 batch loss 1.23580909 epoch total loss 1.25929093\n",
      "Trained batch 226 batch loss 1.18593 epoch total loss 1.25896633\n",
      "Trained batch 227 batch loss 1.07453203 epoch total loss 1.2581538\n",
      "Trained batch 228 batch loss 1.11991358 epoch total loss 1.2575475\n",
      "Trained batch 229 batch loss 1.20912266 epoch total loss 1.25733602\n",
      "Trained batch 230 batch loss 1.31197619 epoch total loss 1.25757372\n",
      "Trained batch 231 batch loss 1.42675161 epoch total loss 1.25830603\n",
      "Trained batch 232 batch loss 1.37897277 epoch total loss 1.25882614\n",
      "Trained batch 233 batch loss 1.3878361 epoch total loss 1.25937986\n",
      "Trained batch 234 batch loss 1.35809577 epoch total loss 1.25980175\n",
      "Trained batch 235 batch loss 1.11880171 epoch total loss 1.25920177\n",
      "Trained batch 236 batch loss 1.07627177 epoch total loss 1.25842655\n",
      "Trained batch 237 batch loss 0.951391518 epoch total loss 1.2571311\n",
      "Trained batch 238 batch loss 1.05385506 epoch total loss 1.25627697\n",
      "Trained batch 239 batch loss 1.08953524 epoch total loss 1.25557935\n",
      "Trained batch 240 batch loss 1.08301818 epoch total loss 1.25486028\n",
      "Trained batch 241 batch loss 1.1456989 epoch total loss 1.25440729\n",
      "Trained batch 242 batch loss 1.18551302 epoch total loss 1.25412261\n",
      "Trained batch 243 batch loss 1.28090525 epoch total loss 1.25423288\n",
      "Trained batch 244 batch loss 1.24607885 epoch total loss 1.2541995\n",
      "Trained batch 245 batch loss 1.27220905 epoch total loss 1.25427306\n",
      "Trained batch 246 batch loss 1.31809115 epoch total loss 1.25453246\n",
      "Trained batch 247 batch loss 1.48239422 epoch total loss 1.25545502\n",
      "Trained batch 248 batch loss 1.38361657 epoch total loss 1.25597167\n",
      "Trained batch 249 batch loss 1.31585956 epoch total loss 1.25621223\n",
      "Trained batch 250 batch loss 1.21725738 epoch total loss 1.25605643\n",
      "Trained batch 251 batch loss 1.2538054 epoch total loss 1.25604749\n",
      "Trained batch 252 batch loss 1.35547173 epoch total loss 1.25644195\n",
      "Trained batch 253 batch loss 1.20159972 epoch total loss 1.25622523\n",
      "Trained batch 254 batch loss 1.17387795 epoch total loss 1.2559011\n",
      "Trained batch 255 batch loss 1.1358434 epoch total loss 1.25543022\n",
      "Trained batch 256 batch loss 1.15559292 epoch total loss 1.25504017\n",
      "Trained batch 257 batch loss 1.15690279 epoch total loss 1.25465822\n",
      "Trained batch 258 batch loss 1.14944947 epoch total loss 1.25425041\n",
      "Trained batch 259 batch loss 1.14123714 epoch total loss 1.2538141\n",
      "Trained batch 260 batch loss 1.31977975 epoch total loss 1.2540679\n",
      "Trained batch 261 batch loss 1.54596341 epoch total loss 1.2551862\n",
      "Trained batch 262 batch loss 1.45827961 epoch total loss 1.25596142\n",
      "Trained batch 263 batch loss 1.32945538 epoch total loss 1.25624084\n",
      "Trained batch 264 batch loss 1.18090177 epoch total loss 1.25595558\n",
      "Trained batch 265 batch loss 1.17030191 epoch total loss 1.25563228\n",
      "Trained batch 266 batch loss 1.27838624 epoch total loss 1.25571775\n",
      "Trained batch 267 batch loss 1.15153515 epoch total loss 1.2553277\n",
      "Trained batch 268 batch loss 1.3037281 epoch total loss 1.2555083\n",
      "Trained batch 269 batch loss 1.29232705 epoch total loss 1.25564516\n",
      "Trained batch 270 batch loss 1.32011878 epoch total loss 1.25588405\n",
      "Trained batch 271 batch loss 1.33338583 epoch total loss 1.25616992\n",
      "Trained batch 272 batch loss 1.41600156 epoch total loss 1.25675762\n",
      "Trained batch 273 batch loss 1.29718018 epoch total loss 1.25690567\n",
      "Trained batch 274 batch loss 1.26125717 epoch total loss 1.25692153\n",
      "Trained batch 275 batch loss 1.17114973 epoch total loss 1.25660968\n",
      "Trained batch 276 batch loss 1.29273522 epoch total loss 1.25674057\n",
      "Trained batch 277 batch loss 1.31534266 epoch total loss 1.25695205\n",
      "Trained batch 278 batch loss 1.22505391 epoch total loss 1.25683737\n",
      "Trained batch 279 batch loss 1.18638706 epoch total loss 1.25658488\n",
      "Trained batch 280 batch loss 1.18267512 epoch total loss 1.25632095\n",
      "Trained batch 281 batch loss 1.24155056 epoch total loss 1.25626838\n",
      "Trained batch 282 batch loss 1.1720736 epoch total loss 1.25596988\n",
      "Trained batch 283 batch loss 1.22457981 epoch total loss 1.2558589\n",
      "Trained batch 284 batch loss 1.14209378 epoch total loss 1.25545835\n",
      "Trained batch 285 batch loss 1.13572824 epoch total loss 1.25503826\n",
      "Trained batch 286 batch loss 1.21186686 epoch total loss 1.25488734\n",
      "Trained batch 287 batch loss 1.35172844 epoch total loss 1.2552247\n",
      "Trained batch 288 batch loss 1.31848431 epoch total loss 1.25544429\n",
      "Trained batch 289 batch loss 1.250283 epoch total loss 1.25542641\n",
      "Trained batch 290 batch loss 1.25854576 epoch total loss 1.25543714\n",
      "Trained batch 291 batch loss 1.2190665 epoch total loss 1.2553122\n",
      "Trained batch 292 batch loss 1.3364116 epoch total loss 1.25559\n",
      "Trained batch 293 batch loss 1.29627275 epoch total loss 1.25572872\n",
      "Trained batch 294 batch loss 1.1795702 epoch total loss 1.25546968\n",
      "Trained batch 295 batch loss 1.16987026 epoch total loss 1.25517952\n",
      "Trained batch 296 batch loss 1.20731091 epoch total loss 1.25501776\n",
      "Trained batch 297 batch loss 1.11692154 epoch total loss 1.25455272\n",
      "Trained batch 298 batch loss 1.18867064 epoch total loss 1.25433171\n",
      "Trained batch 299 batch loss 1.2116524 epoch total loss 1.2541889\n",
      "Trained batch 300 batch loss 1.19941735 epoch total loss 1.25400639\n",
      "Trained batch 301 batch loss 1.1596005 epoch total loss 1.25369275\n",
      "Trained batch 302 batch loss 1.17338204 epoch total loss 1.25342679\n",
      "Trained batch 303 batch loss 1.18402565 epoch total loss 1.25319767\n",
      "Trained batch 304 batch loss 1.32614946 epoch total loss 1.25343764\n",
      "Trained batch 305 batch loss 1.32041121 epoch total loss 1.25365722\n",
      "Trained batch 306 batch loss 1.30567336 epoch total loss 1.25382721\n",
      "Trained batch 307 batch loss 1.34288228 epoch total loss 1.25411725\n",
      "Trained batch 308 batch loss 1.36085677 epoch total loss 1.25446391\n",
      "Trained batch 309 batch loss 1.31308794 epoch total loss 1.25465357\n",
      "Trained batch 310 batch loss 1.24294746 epoch total loss 1.25461578\n",
      "Trained batch 311 batch loss 1.17383742 epoch total loss 1.25435603\n",
      "Trained batch 312 batch loss 1.26354527 epoch total loss 1.25438559\n",
      "Trained batch 313 batch loss 1.0258007 epoch total loss 1.2536552\n",
      "Trained batch 314 batch loss 1.12431192 epoch total loss 1.25324321\n",
      "Trained batch 315 batch loss 1.09734106 epoch total loss 1.25274837\n",
      "Trained batch 316 batch loss 1.09573185 epoch total loss 1.25225139\n",
      "Trained batch 317 batch loss 0.964060605 epoch total loss 1.2513423\n",
      "Trained batch 318 batch loss 0.950102806 epoch total loss 1.25039506\n",
      "Trained batch 319 batch loss 0.972815454 epoch total loss 1.24952483\n",
      "Trained batch 320 batch loss 1.00397861 epoch total loss 1.24875748\n",
      "Trained batch 321 batch loss 1.26757932 epoch total loss 1.24881613\n",
      "Trained batch 322 batch loss 1.19743502 epoch total loss 1.24865651\n",
      "Trained batch 323 batch loss 1.22450876 epoch total loss 1.24858189\n",
      "Trained batch 324 batch loss 1.19634318 epoch total loss 1.2484206\n",
      "Trained batch 325 batch loss 1.16532278 epoch total loss 1.24816489\n",
      "Trained batch 326 batch loss 1.20581686 epoch total loss 1.24803495\n",
      "Trained batch 327 batch loss 1.22567475 epoch total loss 1.24796665\n",
      "Trained batch 328 batch loss 1.31301439 epoch total loss 1.24816489\n",
      "Trained batch 329 batch loss 1.17050099 epoch total loss 1.24792886\n",
      "Trained batch 330 batch loss 1.13657713 epoch total loss 1.24759138\n",
      "Trained batch 331 batch loss 1.15729773 epoch total loss 1.24731863\n",
      "Trained batch 332 batch loss 1.23053336 epoch total loss 1.24726808\n",
      "Trained batch 333 batch loss 1.26980197 epoch total loss 1.24733567\n",
      "Trained batch 334 batch loss 1.19261563 epoch total loss 1.24717188\n",
      "Trained batch 335 batch loss 1.25475717 epoch total loss 1.24719453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 336 batch loss 1.25724423 epoch total loss 1.24722445\n",
      "Trained batch 337 batch loss 1.14537811 epoch total loss 1.24692225\n",
      "Trained batch 338 batch loss 1.25703859 epoch total loss 1.24695218\n",
      "Trained batch 339 batch loss 1.22279918 epoch total loss 1.24688101\n",
      "Trained batch 340 batch loss 1.16389453 epoch total loss 1.24663687\n",
      "Trained batch 341 batch loss 1.34408689 epoch total loss 1.24692261\n",
      "Trained batch 342 batch loss 1.32543111 epoch total loss 1.24715221\n",
      "Trained batch 343 batch loss 1.16869318 epoch total loss 1.24692357\n",
      "Trained batch 344 batch loss 1.36831152 epoch total loss 1.24727643\n",
      "Trained batch 345 batch loss 1.12298906 epoch total loss 1.24691617\n",
      "Trained batch 346 batch loss 1.21908236 epoch total loss 1.24683571\n",
      "Trained batch 347 batch loss 1.25773811 epoch total loss 1.24686718\n",
      "Trained batch 348 batch loss 1.2453084 epoch total loss 1.24686265\n",
      "Trained batch 349 batch loss 1.21040285 epoch total loss 1.2467581\n",
      "Trained batch 350 batch loss 1.30614686 epoch total loss 1.24692786\n",
      "Trained batch 351 batch loss 1.17175579 epoch total loss 1.24671364\n",
      "Trained batch 352 batch loss 1.24060225 epoch total loss 1.24669635\n",
      "Trained batch 353 batch loss 1.20558548 epoch total loss 1.24657989\n",
      "Trained batch 354 batch loss 1.22677469 epoch total loss 1.24652398\n",
      "Trained batch 355 batch loss 1.14402127 epoch total loss 1.24623513\n",
      "Trained batch 356 batch loss 1.41449726 epoch total loss 1.2467078\n",
      "Trained batch 357 batch loss 1.33385587 epoch total loss 1.24695194\n",
      "Trained batch 358 batch loss 1.37858295 epoch total loss 1.24731958\n",
      "Trained batch 359 batch loss 1.37844372 epoch total loss 1.24768484\n",
      "Trained batch 360 batch loss 1.24784851 epoch total loss 1.24768519\n",
      "Trained batch 361 batch loss 1.32918584 epoch total loss 1.24791098\n",
      "Trained batch 362 batch loss 1.27459455 epoch total loss 1.24798477\n",
      "Trained batch 363 batch loss 1.23456669 epoch total loss 1.24794781\n",
      "Trained batch 364 batch loss 1.31445909 epoch total loss 1.24813044\n",
      "Trained batch 365 batch loss 1.17766058 epoch total loss 1.24793744\n",
      "Trained batch 366 batch loss 1.20493758 epoch total loss 1.2478199\n",
      "Trained batch 367 batch loss 1.15985107 epoch total loss 1.24758017\n",
      "Trained batch 368 batch loss 1.08789873 epoch total loss 1.24714625\n",
      "Trained batch 369 batch loss 1.12309575 epoch total loss 1.24681008\n",
      "Trained batch 370 batch loss 1.28863013 epoch total loss 1.24692321\n",
      "Trained batch 371 batch loss 1.25815487 epoch total loss 1.24695337\n",
      "Trained batch 372 batch loss 1.46422601 epoch total loss 1.24753749\n",
      "Trained batch 373 batch loss 1.47104573 epoch total loss 1.24813676\n",
      "Trained batch 374 batch loss 1.34808159 epoch total loss 1.24840391\n",
      "Trained batch 375 batch loss 1.3941561 epoch total loss 1.24879265\n",
      "Trained batch 376 batch loss 1.33577847 epoch total loss 1.24902403\n",
      "Trained batch 377 batch loss 1.16886067 epoch total loss 1.24881136\n",
      "Trained batch 378 batch loss 1.12975645 epoch total loss 1.24849641\n",
      "Trained batch 379 batch loss 1.17821288 epoch total loss 1.24831104\n",
      "Trained batch 380 batch loss 1.29449916 epoch total loss 1.24843252\n",
      "Trained batch 381 batch loss 1.29579878 epoch total loss 1.24855685\n",
      "Trained batch 382 batch loss 1.30939317 epoch total loss 1.24871612\n",
      "Trained batch 383 batch loss 1.1614176 epoch total loss 1.24848819\n",
      "Trained batch 384 batch loss 1.10258698 epoch total loss 1.24810827\n",
      "Trained batch 385 batch loss 1.13186753 epoch total loss 1.24780631\n",
      "Trained batch 386 batch loss 1.07510448 epoch total loss 1.24735892\n",
      "Trained batch 387 batch loss 1.20145988 epoch total loss 1.2472403\n",
      "Trained batch 388 batch loss 1.16602111 epoch total loss 1.24703085\n",
      "Trained batch 389 batch loss 1.16260993 epoch total loss 1.24681389\n",
      "Trained batch 390 batch loss 1.19706678 epoch total loss 1.24668622\n",
      "Trained batch 391 batch loss 1.21836042 epoch total loss 1.24661374\n",
      "Trained batch 392 batch loss 1.27644157 epoch total loss 1.2466898\n",
      "Trained batch 393 batch loss 1.35476398 epoch total loss 1.24696481\n",
      "Trained batch 394 batch loss 1.13366556 epoch total loss 1.24667728\n",
      "Trained batch 395 batch loss 1.16539955 epoch total loss 1.24647152\n",
      "Trained batch 396 batch loss 1.19316947 epoch total loss 1.24633694\n",
      "Trained batch 397 batch loss 1.15451825 epoch total loss 1.24610567\n",
      "Trained batch 398 batch loss 1.08811474 epoch total loss 1.2457087\n",
      "Trained batch 399 batch loss 1.25286937 epoch total loss 1.24572659\n",
      "Trained batch 400 batch loss 1.2547493 epoch total loss 1.24574924\n",
      "Trained batch 401 batch loss 1.14162064 epoch total loss 1.2454896\n",
      "Trained batch 402 batch loss 1.31405067 epoch total loss 1.24566019\n",
      "Trained batch 403 batch loss 1.13364971 epoch total loss 1.24538219\n",
      "Trained batch 404 batch loss 1.25126529 epoch total loss 1.24539673\n",
      "Trained batch 405 batch loss 1.26982093 epoch total loss 1.24545693\n",
      "Trained batch 406 batch loss 1.12270963 epoch total loss 1.24515462\n",
      "Trained batch 407 batch loss 1.16073549 epoch total loss 1.2449472\n",
      "Trained batch 408 batch loss 1.1994772 epoch total loss 1.24483573\n",
      "Trained batch 409 batch loss 1.23600924 epoch total loss 1.24481416\n",
      "Trained batch 410 batch loss 1.13010013 epoch total loss 1.24453437\n",
      "Trained batch 411 batch loss 1.19325471 epoch total loss 1.24440968\n",
      "Trained batch 412 batch loss 1.31987214 epoch total loss 1.24459279\n",
      "Trained batch 413 batch loss 1.31891787 epoch total loss 1.24477267\n",
      "Trained batch 414 batch loss 1.22260094 epoch total loss 1.24471915\n",
      "Trained batch 415 batch loss 1.19709754 epoch total loss 1.24460435\n",
      "Trained batch 416 batch loss 1.28982556 epoch total loss 1.24471307\n",
      "Trained batch 417 batch loss 1.17912078 epoch total loss 1.24455583\n",
      "Trained batch 418 batch loss 1.15951347 epoch total loss 1.24435234\n",
      "Trained batch 419 batch loss 1.18195641 epoch total loss 1.24420345\n",
      "Trained batch 420 batch loss 1.29682994 epoch total loss 1.24432862\n",
      "Trained batch 421 batch loss 1.28397906 epoch total loss 1.24442291\n",
      "Trained batch 422 batch loss 1.2225033 epoch total loss 1.24437094\n",
      "Trained batch 423 batch loss 1.19135809 epoch total loss 1.24424553\n",
      "Trained batch 424 batch loss 1.31204474 epoch total loss 1.24440551\n",
      "Trained batch 425 batch loss 1.08517683 epoch total loss 1.24403095\n",
      "Trained batch 426 batch loss 1.15298629 epoch total loss 1.24381721\n",
      "Trained batch 427 batch loss 1.2780751 epoch total loss 1.24389744\n",
      "Trained batch 428 batch loss 1.25356674 epoch total loss 1.24392\n",
      "Trained batch 429 batch loss 1.24811721 epoch total loss 1.24392974\n",
      "Trained batch 430 batch loss 1.2491802 epoch total loss 1.24394202\n",
      "Trained batch 431 batch loss 1.10625815 epoch total loss 1.24362266\n",
      "Trained batch 432 batch loss 1.18508959 epoch total loss 1.24348724\n",
      "Trained batch 433 batch loss 1.34539902 epoch total loss 1.24372256\n",
      "Trained batch 434 batch loss 1.18706679 epoch total loss 1.24359202\n",
      "Trained batch 435 batch loss 1.31448066 epoch total loss 1.24375486\n",
      "Trained batch 436 batch loss 1.24007034 epoch total loss 1.2437464\n",
      "Trained batch 437 batch loss 1.29885137 epoch total loss 1.2438724\n",
      "Trained batch 438 batch loss 1.27452171 epoch total loss 1.2439425\n",
      "Trained batch 439 batch loss 1.51334369 epoch total loss 1.24455619\n",
      "Trained batch 440 batch loss 1.43198669 epoch total loss 1.24498224\n",
      "Trained batch 441 batch loss 1.27293348 epoch total loss 1.24504566\n",
      "Trained batch 442 batch loss 1.2642622 epoch total loss 1.24508917\n",
      "Trained batch 443 batch loss 0.998125315 epoch total loss 1.24453163\n",
      "Trained batch 444 batch loss 1.05673623 epoch total loss 1.24410868\n",
      "Trained batch 445 batch loss 1.16749644 epoch total loss 1.24393654\n",
      "Trained batch 446 batch loss 1.11297226 epoch total loss 1.24364293\n",
      "Trained batch 447 batch loss 0.976354599 epoch total loss 1.24304497\n",
      "Trained batch 448 batch loss 0.972273 epoch total loss 1.24244058\n",
      "Trained batch 449 batch loss 1.02213907 epoch total loss 1.24195\n",
      "Trained batch 450 batch loss 1.12698662 epoch total loss 1.24169457\n",
      "Trained batch 451 batch loss 1.11934102 epoch total loss 1.24142325\n",
      "Trained batch 452 batch loss 1.3007071 epoch total loss 1.2415545\n",
      "Trained batch 453 batch loss 1.24850011 epoch total loss 1.24156976\n",
      "Trained batch 454 batch loss 1.28052723 epoch total loss 1.24165547\n",
      "Trained batch 455 batch loss 1.26698565 epoch total loss 1.24171114\n",
      "Trained batch 456 batch loss 1.18954122 epoch total loss 1.2415967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 457 batch loss 1.12874198 epoch total loss 1.2413497\n",
      "Trained batch 458 batch loss 1.27750516 epoch total loss 1.24142873\n",
      "Trained batch 459 batch loss 1.33001089 epoch total loss 1.24162173\n",
      "Trained batch 460 batch loss 1.35109246 epoch total loss 1.24185967\n",
      "Trained batch 461 batch loss 1.24170864 epoch total loss 1.24185932\n",
      "Trained batch 462 batch loss 1.28957605 epoch total loss 1.24196255\n",
      "Trained batch 463 batch loss 1.30676734 epoch total loss 1.2421025\n",
      "Trained batch 464 batch loss 1.2961781 epoch total loss 1.24221909\n",
      "Trained batch 465 batch loss 1.23598599 epoch total loss 1.24220562\n",
      "Trained batch 466 batch loss 1.37672257 epoch total loss 1.24249423\n",
      "Trained batch 467 batch loss 1.26093364 epoch total loss 1.24253368\n",
      "Trained batch 468 batch loss 1.25653934 epoch total loss 1.24256361\n",
      "Trained batch 469 batch loss 1.33605933 epoch total loss 1.24276292\n",
      "Trained batch 470 batch loss 1.30333567 epoch total loss 1.24289191\n",
      "Trained batch 471 batch loss 1.24010265 epoch total loss 1.24288595\n",
      "Trained batch 472 batch loss 1.31701291 epoch total loss 1.24304307\n",
      "Trained batch 473 batch loss 1.22344863 epoch total loss 1.24300158\n",
      "Trained batch 474 batch loss 1.20615971 epoch total loss 1.24292386\n",
      "Trained batch 475 batch loss 1.2950685 epoch total loss 1.24303365\n",
      "Trained batch 476 batch loss 1.10449862 epoch total loss 1.24274254\n",
      "Trained batch 477 batch loss 1.14493346 epoch total loss 1.24253762\n",
      "Trained batch 478 batch loss 1.17085075 epoch total loss 1.24238753\n",
      "Trained batch 479 batch loss 1.28696656 epoch total loss 1.24248075\n",
      "Trained batch 480 batch loss 1.16248119 epoch total loss 1.24231398\n",
      "Trained batch 481 batch loss 1.14986348 epoch total loss 1.24212182\n",
      "Trained batch 482 batch loss 1.08973765 epoch total loss 1.24180555\n",
      "Trained batch 483 batch loss 1.17271113 epoch total loss 1.24166262\n",
      "Trained batch 484 batch loss 1.10111034 epoch total loss 1.24137223\n",
      "Trained batch 485 batch loss 0.993783236 epoch total loss 1.24086177\n",
      "Trained batch 486 batch loss 1.14536154 epoch total loss 1.24066532\n",
      "Trained batch 487 batch loss 1.23302746 epoch total loss 1.24064958\n",
      "Trained batch 488 batch loss 1.10333753 epoch total loss 1.24036825\n",
      "Trained batch 489 batch loss 1.11909199 epoch total loss 1.24012017\n",
      "Trained batch 490 batch loss 1.21367502 epoch total loss 1.24006617\n",
      "Trained batch 491 batch loss 1.32753181 epoch total loss 1.24024427\n",
      "Trained batch 492 batch loss 1.13543046 epoch total loss 1.24003124\n",
      "Trained batch 493 batch loss 1.3886553 epoch total loss 1.24033284\n",
      "Trained batch 494 batch loss 1.31322229 epoch total loss 1.24048042\n",
      "Trained batch 495 batch loss 1.18369579 epoch total loss 1.24036574\n",
      "Trained batch 496 batch loss 1.20102155 epoch total loss 1.24028647\n",
      "Trained batch 497 batch loss 1.18206525 epoch total loss 1.24016929\n",
      "Trained batch 498 batch loss 1.23937035 epoch total loss 1.24016774\n",
      "Trained batch 499 batch loss 1.12025 epoch total loss 1.23992741\n",
      "Trained batch 500 batch loss 1.27383661 epoch total loss 1.23999524\n",
      "Trained batch 501 batch loss 1.14472651 epoch total loss 1.2398051\n",
      "Trained batch 502 batch loss 1.04257894 epoch total loss 1.23941219\n",
      "Trained batch 503 batch loss 1.04133594 epoch total loss 1.23901844\n",
      "Trained batch 504 batch loss 1.02390921 epoch total loss 1.23859167\n",
      "Trained batch 505 batch loss 1.19375956 epoch total loss 1.23850286\n",
      "Trained batch 506 batch loss 1.14996243 epoch total loss 1.23832798\n",
      "Trained batch 507 batch loss 1.12730885 epoch total loss 1.23810899\n",
      "Trained batch 508 batch loss 1.23043931 epoch total loss 1.23809397\n",
      "Trained batch 509 batch loss 1.41553068 epoch total loss 1.23844254\n",
      "Trained batch 510 batch loss 1.22357011 epoch total loss 1.23841333\n",
      "Trained batch 511 batch loss 1.36661291 epoch total loss 1.23866427\n",
      "Trained batch 512 batch loss 1.17211211 epoch total loss 1.23853433\n",
      "Trained batch 513 batch loss 1.08540177 epoch total loss 1.23823583\n",
      "Trained batch 514 batch loss 1.20195508 epoch total loss 1.23816526\n",
      "Trained batch 515 batch loss 1.22390389 epoch total loss 1.23813748\n",
      "Trained batch 516 batch loss 1.3589282 epoch total loss 1.23837161\n",
      "Trained batch 517 batch loss 1.3787477 epoch total loss 1.23864305\n",
      "Trained batch 518 batch loss 1.2512362 epoch total loss 1.23866737\n",
      "Trained batch 519 batch loss 1.23469567 epoch total loss 1.23865974\n",
      "Trained batch 520 batch loss 1.17663026 epoch total loss 1.23854041\n",
      "Trained batch 521 batch loss 1.21151972 epoch total loss 1.23848855\n",
      "Trained batch 522 batch loss 1.22147322 epoch total loss 1.23845601\n",
      "Trained batch 523 batch loss 1.21234167 epoch total loss 1.23840606\n",
      "Trained batch 524 batch loss 1.2789712 epoch total loss 1.23848355\n",
      "Trained batch 525 batch loss 1.19270504 epoch total loss 1.23839629\n",
      "Trained batch 526 batch loss 1.0514468 epoch total loss 1.23804092\n",
      "Trained batch 527 batch loss 1.21123791 epoch total loss 1.23799014\n",
      "Trained batch 528 batch loss 1.39156628 epoch total loss 1.23828089\n",
      "Trained batch 529 batch loss 1.4353894 epoch total loss 1.23865342\n",
      "Trained batch 530 batch loss 1.14663434 epoch total loss 1.23847973\n",
      "Trained batch 531 batch loss 1.06615293 epoch total loss 1.23815525\n",
      "Trained batch 532 batch loss 1.16175163 epoch total loss 1.2380116\n",
      "Trained batch 533 batch loss 1.03919172 epoch total loss 1.23763859\n",
      "Trained batch 534 batch loss 1.05069208 epoch total loss 1.23728859\n",
      "Trained batch 535 batch loss 1.02399552 epoch total loss 1.23688984\n",
      "Trained batch 536 batch loss 1.13213897 epoch total loss 1.23669446\n",
      "Trained batch 537 batch loss 1.15385067 epoch total loss 1.2365402\n",
      "Trained batch 538 batch loss 1.18331814 epoch total loss 1.23644125\n",
      "Trained batch 539 batch loss 1.16533077 epoch total loss 1.23630929\n",
      "Trained batch 540 batch loss 1.27325368 epoch total loss 1.23637772\n",
      "Trained batch 541 batch loss 1.21512771 epoch total loss 1.2363385\n",
      "Trained batch 542 batch loss 1.24511874 epoch total loss 1.23635471\n",
      "Trained batch 543 batch loss 1.23692024 epoch total loss 1.23635578\n",
      "Trained batch 544 batch loss 1.22459698 epoch total loss 1.2363342\n",
      "Trained batch 545 batch loss 1.14539099 epoch total loss 1.23616731\n",
      "Trained batch 546 batch loss 1.17204809 epoch total loss 1.23604989\n",
      "Trained batch 547 batch loss 1.26987422 epoch total loss 1.23611176\n",
      "Trained batch 548 batch loss 1.27658057 epoch total loss 1.23618555\n",
      "Trained batch 549 batch loss 1.6015718 epoch total loss 1.2368511\n",
      "Trained batch 550 batch loss 1.22787094 epoch total loss 1.23683476\n",
      "Trained batch 551 batch loss 1.27862203 epoch total loss 1.23691058\n",
      "Trained batch 552 batch loss 1.23649275 epoch total loss 1.23690987\n",
      "Trained batch 553 batch loss 1.40195513 epoch total loss 1.23720837\n",
      "Trained batch 554 batch loss 1.31163621 epoch total loss 1.23734272\n",
      "Trained batch 555 batch loss 1.28749919 epoch total loss 1.23743308\n",
      "Trained batch 556 batch loss 1.21345425 epoch total loss 1.23738992\n",
      "Trained batch 557 batch loss 1.18548226 epoch total loss 1.2372967\n",
      "Trained batch 558 batch loss 1.19584155 epoch total loss 1.23722243\n",
      "Trained batch 559 batch loss 1.33364153 epoch total loss 1.23739493\n",
      "Trained batch 560 batch loss 1.19816899 epoch total loss 1.23732483\n",
      "Trained batch 561 batch loss 1.13071811 epoch total loss 1.23713481\n",
      "Trained batch 562 batch loss 1.13437963 epoch total loss 1.23695207\n",
      "Trained batch 563 batch loss 1.23544264 epoch total loss 1.23694932\n",
      "Trained batch 564 batch loss 1.14172435 epoch total loss 1.23678052\n",
      "Trained batch 565 batch loss 1.17516661 epoch total loss 1.23667145\n",
      "Trained batch 566 batch loss 1.18984032 epoch total loss 1.2365886\n",
      "Trained batch 567 batch loss 1.17064786 epoch total loss 1.23647237\n",
      "Trained batch 568 batch loss 1.18912101 epoch total loss 1.23638904\n",
      "Trained batch 569 batch loss 1.18532145 epoch total loss 1.23629928\n",
      "Trained batch 570 batch loss 1.12817764 epoch total loss 1.23610961\n",
      "Trained batch 571 batch loss 1.08676386 epoch total loss 1.23584807\n",
      "Trained batch 572 batch loss 1.18321657 epoch total loss 1.23575604\n",
      "Trained batch 573 batch loss 1.16779411 epoch total loss 1.23563743\n",
      "Trained batch 574 batch loss 1.18262136 epoch total loss 1.23554504\n",
      "Trained batch 575 batch loss 1.10444045 epoch total loss 1.23531711\n",
      "Trained batch 576 batch loss 1.11255717 epoch total loss 1.23510396\n",
      "Trained batch 577 batch loss 1.09139442 epoch total loss 1.23485482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 578 batch loss 1.27851367 epoch total loss 1.23493028\n",
      "Trained batch 579 batch loss 1.21510041 epoch total loss 1.23489606\n",
      "Trained batch 580 batch loss 1.13884139 epoch total loss 1.23473048\n",
      "Trained batch 581 batch loss 1.24132562 epoch total loss 1.23474181\n",
      "Trained batch 582 batch loss 1.29676104 epoch total loss 1.23484838\n",
      "Trained batch 583 batch loss 1.3148396 epoch total loss 1.23498559\n",
      "Trained batch 584 batch loss 1.33712339 epoch total loss 1.23516035\n",
      "Trained batch 585 batch loss 1.26717103 epoch total loss 1.23521507\n",
      "Trained batch 586 batch loss 1.27625084 epoch total loss 1.23528516\n",
      "Trained batch 587 batch loss 1.22094977 epoch total loss 1.23526073\n",
      "Trained batch 588 batch loss 1.26352453 epoch total loss 1.23530877\n",
      "Trained batch 589 batch loss 1.25347877 epoch total loss 1.23533964\n",
      "Trained batch 590 batch loss 1.2156328 epoch total loss 1.23530626\n",
      "Trained batch 591 batch loss 1.32811093 epoch total loss 1.23546326\n",
      "Trained batch 592 batch loss 1.27098536 epoch total loss 1.23552334\n",
      "Trained batch 593 batch loss 1.18198895 epoch total loss 1.2354331\n",
      "Trained batch 594 batch loss 1.12766206 epoch total loss 1.23525167\n",
      "Trained batch 595 batch loss 1.21675634 epoch total loss 1.23522055\n",
      "Trained batch 596 batch loss 1.29107869 epoch total loss 1.23531425\n",
      "Trained batch 597 batch loss 1.29325533 epoch total loss 1.23541141\n",
      "Trained batch 598 batch loss 1.25870967 epoch total loss 1.23545039\n",
      "Trained batch 599 batch loss 1.28751445 epoch total loss 1.23553729\n",
      "Trained batch 600 batch loss 1.30153358 epoch total loss 1.23564732\n",
      "Trained batch 601 batch loss 1.22626281 epoch total loss 1.2356317\n",
      "Trained batch 602 batch loss 1.28794146 epoch total loss 1.23571861\n",
      "Trained batch 603 batch loss 1.13020277 epoch total loss 1.23554361\n",
      "Trained batch 604 batch loss 1.23723245 epoch total loss 1.23554635\n",
      "Trained batch 605 batch loss 1.23548472 epoch total loss 1.23554623\n",
      "Trained batch 606 batch loss 1.2942102 epoch total loss 1.23564303\n",
      "Trained batch 607 batch loss 1.17175663 epoch total loss 1.23553777\n",
      "Trained batch 608 batch loss 1.0181582 epoch total loss 1.23518026\n",
      "Trained batch 609 batch loss 1.05881441 epoch total loss 1.2348907\n",
      "Trained batch 610 batch loss 1.2590512 epoch total loss 1.23493028\n",
      "Trained batch 611 batch loss 1.30330348 epoch total loss 1.23504221\n",
      "Trained batch 612 batch loss 1.26709533 epoch total loss 1.23509455\n",
      "Trained batch 613 batch loss 1.32501268 epoch total loss 1.23524129\n",
      "Trained batch 614 batch loss 1.47035074 epoch total loss 1.23562407\n",
      "Trained batch 615 batch loss 1.27131939 epoch total loss 1.23568213\n",
      "Trained batch 616 batch loss 1.17688513 epoch total loss 1.23558664\n",
      "Trained batch 617 batch loss 1.33621156 epoch total loss 1.23574972\n",
      "Trained batch 618 batch loss 1.30204201 epoch total loss 1.23585701\n",
      "Trained batch 619 batch loss 1.26793909 epoch total loss 1.23590887\n",
      "Trained batch 620 batch loss 1.15471816 epoch total loss 1.23577797\n",
      "Trained batch 621 batch loss 1.08971453 epoch total loss 1.23554277\n",
      "Trained batch 622 batch loss 1.01320541 epoch total loss 1.23518527\n",
      "Trained batch 623 batch loss 1.08401728 epoch total loss 1.23494267\n",
      "Trained batch 624 batch loss 1.2592113 epoch total loss 1.23498154\n",
      "Trained batch 625 batch loss 1.16034091 epoch total loss 1.23486209\n",
      "Trained batch 626 batch loss 1.2376976 epoch total loss 1.23486662\n",
      "Trained batch 627 batch loss 1.31325722 epoch total loss 1.23499155\n",
      "Trained batch 628 batch loss 1.33582687 epoch total loss 1.23515213\n",
      "Trained batch 629 batch loss 1.1063323 epoch total loss 1.23494732\n",
      "Trained batch 630 batch loss 1.02001035 epoch total loss 1.23460615\n",
      "Trained batch 631 batch loss 0.953229189 epoch total loss 1.2341603\n",
      "Trained batch 632 batch loss 1.07870436 epoch total loss 1.23391426\n",
      "Trained batch 633 batch loss 1.35606396 epoch total loss 1.23410726\n",
      "Trained batch 634 batch loss 1.56026 epoch total loss 1.23462164\n",
      "Trained batch 635 batch loss 1.3721242 epoch total loss 1.23483825\n",
      "Trained batch 636 batch loss 1.17018163 epoch total loss 1.23473656\n",
      "Trained batch 637 batch loss 1.14173126 epoch total loss 1.23459053\n",
      "Trained batch 638 batch loss 1.26111603 epoch total loss 1.23463202\n",
      "Trained batch 639 batch loss 1.36500919 epoch total loss 1.2348361\n",
      "Trained batch 640 batch loss 1.29731691 epoch total loss 1.23493361\n",
      "Trained batch 641 batch loss 1.25936222 epoch total loss 1.23497176\n",
      "Trained batch 642 batch loss 1.27005529 epoch total loss 1.23502648\n",
      "Trained batch 643 batch loss 1.30360866 epoch total loss 1.23513305\n",
      "Trained batch 644 batch loss 1.20087314 epoch total loss 1.23507988\n",
      "Trained batch 645 batch loss 1.1786164 epoch total loss 1.23499227\n",
      "Trained batch 646 batch loss 1.11715865 epoch total loss 1.23480988\n",
      "Trained batch 647 batch loss 1.22072959 epoch total loss 1.23478806\n",
      "Trained batch 648 batch loss 1.15745485 epoch total loss 1.23466873\n",
      "Trained batch 649 batch loss 1.13083255 epoch total loss 1.23450887\n",
      "Trained batch 650 batch loss 1.15024233 epoch total loss 1.23437917\n",
      "Trained batch 651 batch loss 1.10135651 epoch total loss 1.23417497\n",
      "Trained batch 652 batch loss 1.26708865 epoch total loss 1.23422539\n",
      "Trained batch 653 batch loss 1.15180159 epoch total loss 1.23409915\n",
      "Trained batch 654 batch loss 1.02420402 epoch total loss 1.23377824\n",
      "Trained batch 655 batch loss 0.994954944 epoch total loss 1.23341358\n",
      "Trained batch 656 batch loss 1.11926496 epoch total loss 1.23323965\n",
      "Trained batch 657 batch loss 1.26640916 epoch total loss 1.23329008\n",
      "Trained batch 658 batch loss 1.38899708 epoch total loss 1.23352671\n",
      "Trained batch 659 batch loss 1.35616446 epoch total loss 1.23371279\n",
      "Trained batch 660 batch loss 1.25923145 epoch total loss 1.23375142\n",
      "Trained batch 661 batch loss 1.13853312 epoch total loss 1.23360741\n",
      "Trained batch 662 batch loss 1.21505833 epoch total loss 1.2335794\n",
      "Trained batch 663 batch loss 1.2062391 epoch total loss 1.23353815\n",
      "Trained batch 664 batch loss 1.23272133 epoch total loss 1.23353696\n",
      "Trained batch 665 batch loss 1.2815516 epoch total loss 1.2336092\n",
      "Trained batch 666 batch loss 1.2446419 epoch total loss 1.23362577\n",
      "Trained batch 667 batch loss 1.35258555 epoch total loss 1.23380411\n",
      "Trained batch 668 batch loss 1.28948545 epoch total loss 1.23388743\n",
      "Trained batch 669 batch loss 1.27138746 epoch total loss 1.23394346\n",
      "Trained batch 670 batch loss 1.268641 epoch total loss 1.2339952\n",
      "Trained batch 671 batch loss 1.16767168 epoch total loss 1.23389637\n",
      "Trained batch 672 batch loss 1.14035118 epoch total loss 1.23375726\n",
      "Trained batch 673 batch loss 1.28099751 epoch total loss 1.23382735\n",
      "Trained batch 674 batch loss 1.19925153 epoch total loss 1.23377609\n",
      "Trained batch 675 batch loss 1.36096609 epoch total loss 1.23396456\n",
      "Trained batch 676 batch loss 1.19536448 epoch total loss 1.23390746\n",
      "Trained batch 677 batch loss 1.09033501 epoch total loss 1.23369539\n",
      "Trained batch 678 batch loss 1.24120212 epoch total loss 1.23370647\n",
      "Trained batch 679 batch loss 1.15106738 epoch total loss 1.23358476\n",
      "Trained batch 680 batch loss 1.25943148 epoch total loss 1.23362279\n",
      "Trained batch 681 batch loss 1.19717109 epoch total loss 1.23356926\n",
      "Trained batch 682 batch loss 1.26020551 epoch total loss 1.23360825\n",
      "Trained batch 683 batch loss 1.2211349 epoch total loss 1.23359\n",
      "Trained batch 684 batch loss 1.17648077 epoch total loss 1.23350644\n",
      "Trained batch 685 batch loss 1.15001106 epoch total loss 1.23338461\n",
      "Trained batch 686 batch loss 1.1922574 epoch total loss 1.23332465\n",
      "Trained batch 687 batch loss 1.14090765 epoch total loss 1.23319018\n",
      "Trained batch 688 batch loss 1.25728703 epoch total loss 1.23322523\n",
      "Trained batch 689 batch loss 1.21529698 epoch total loss 1.23319912\n",
      "Trained batch 690 batch loss 1.2073524 epoch total loss 1.23316169\n",
      "Trained batch 691 batch loss 1.24166799 epoch total loss 1.23317385\n",
      "Trained batch 692 batch loss 1.23679149 epoch total loss 1.23317921\n",
      "Trained batch 693 batch loss 1.09493458 epoch total loss 1.23297966\n",
      "Trained batch 694 batch loss 1.13198793 epoch total loss 1.2328341\n",
      "Trained batch 695 batch loss 1.13949692 epoch total loss 1.23269987\n",
      "Trained batch 696 batch loss 1.13366079 epoch total loss 1.23255754\n",
      "Trained batch 697 batch loss 1.03414738 epoch total loss 1.23227286\n",
      "Trained batch 698 batch loss 1.02900183 epoch total loss 1.23198164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 699 batch loss 1.16115153 epoch total loss 1.23188019\n",
      "Trained batch 700 batch loss 1.22758162 epoch total loss 1.23187411\n",
      "Trained batch 701 batch loss 1.29860127 epoch total loss 1.23196924\n",
      "Trained batch 702 batch loss 1.31880534 epoch total loss 1.23209298\n",
      "Trained batch 703 batch loss 1.42293525 epoch total loss 1.23236442\n",
      "Trained batch 704 batch loss 1.1742866 epoch total loss 1.23228192\n",
      "Trained batch 705 batch loss 1.22799325 epoch total loss 1.23227584\n",
      "Trained batch 706 batch loss 1.21111119 epoch total loss 1.2322458\n",
      "Trained batch 707 batch loss 1.18268013 epoch total loss 1.23217571\n",
      "Trained batch 708 batch loss 1.3075397 epoch total loss 1.23228216\n",
      "Trained batch 709 batch loss 1.07160568 epoch total loss 1.23205554\n",
      "Trained batch 710 batch loss 1.01510084 epoch total loss 1.23175\n",
      "Trained batch 711 batch loss 1.03573132 epoch total loss 1.23147428\n",
      "Trained batch 712 batch loss 1.19207764 epoch total loss 1.23141897\n",
      "Trained batch 713 batch loss 1.35370743 epoch total loss 1.23159039\n",
      "Trained batch 714 batch loss 1.42123294 epoch total loss 1.23185599\n",
      "Trained batch 715 batch loss 1.34206843 epoch total loss 1.23201013\n",
      "Trained batch 716 batch loss 1.1329788 epoch total loss 1.23187184\n",
      "Trained batch 717 batch loss 1.02906024 epoch total loss 1.23158896\n",
      "Trained batch 718 batch loss 1.16588736 epoch total loss 1.23149741\n",
      "Trained batch 719 batch loss 1.11795223 epoch total loss 1.23133957\n",
      "Trained batch 720 batch loss 1.17871499 epoch total loss 1.2312665\n",
      "Trained batch 721 batch loss 1.20494127 epoch total loss 1.2312299\n",
      "Trained batch 722 batch loss 1.15166616 epoch total loss 1.23111975\n",
      "Trained batch 723 batch loss 1.1914829 epoch total loss 1.23106492\n",
      "Trained batch 724 batch loss 1.22026932 epoch total loss 1.23105\n",
      "Trained batch 725 batch loss 1.12302542 epoch total loss 1.230901\n",
      "Trained batch 726 batch loss 1.31660247 epoch total loss 1.23101902\n",
      "Trained batch 727 batch loss 1.14135718 epoch total loss 1.23089576\n",
      "Trained batch 728 batch loss 1.0574975 epoch total loss 1.23065758\n",
      "Trained batch 729 batch loss 1.12426066 epoch total loss 1.23051167\n",
      "Trained batch 730 batch loss 1.20321238 epoch total loss 1.23047423\n",
      "Trained batch 731 batch loss 1.20044291 epoch total loss 1.23043311\n",
      "Trained batch 732 batch loss 1.41273069 epoch total loss 1.23068213\n",
      "Trained batch 733 batch loss 1.40445518 epoch total loss 1.23091924\n",
      "Trained batch 734 batch loss 1.38404298 epoch total loss 1.23112786\n",
      "Trained batch 735 batch loss 1.23575926 epoch total loss 1.23113418\n",
      "Trained batch 736 batch loss 1.05792451 epoch total loss 1.23089886\n",
      "Trained batch 737 batch loss 1.23878789 epoch total loss 1.23090947\n",
      "Trained batch 738 batch loss 1.13013566 epoch total loss 1.23077297\n",
      "Trained batch 739 batch loss 1.32655668 epoch total loss 1.23090255\n",
      "Trained batch 740 batch loss 1.30803883 epoch total loss 1.23100674\n",
      "Trained batch 741 batch loss 1.23384261 epoch total loss 1.23101056\n",
      "Trained batch 742 batch loss 1.3267113 epoch total loss 1.23113954\n",
      "Trained batch 743 batch loss 1.3266468 epoch total loss 1.23126817\n",
      "Trained batch 744 batch loss 1.26299477 epoch total loss 1.23131073\n",
      "Trained batch 745 batch loss 1.31835175 epoch total loss 1.23142767\n",
      "Trained batch 746 batch loss 1.36674631 epoch total loss 1.23160899\n",
      "Trained batch 747 batch loss 1.30321312 epoch total loss 1.23170495\n",
      "Trained batch 748 batch loss 1.19209599 epoch total loss 1.2316519\n",
      "Trained batch 749 batch loss 1.24254417 epoch total loss 1.23166645\n",
      "Trained batch 750 batch loss 1.3100121 epoch total loss 1.23177087\n",
      "Trained batch 751 batch loss 1.28861213 epoch total loss 1.23184669\n",
      "Trained batch 752 batch loss 1.2592411 epoch total loss 1.23188305\n",
      "Trained batch 753 batch loss 1.23581123 epoch total loss 1.23188829\n",
      "Trained batch 754 batch loss 1.28172457 epoch total loss 1.23195446\n",
      "Trained batch 755 batch loss 1.26599073 epoch total loss 1.23199952\n",
      "Trained batch 756 batch loss 1.15371406 epoch total loss 1.23189592\n",
      "Trained batch 757 batch loss 1.39112866 epoch total loss 1.23210621\n",
      "Trained batch 758 batch loss 1.29588211 epoch total loss 1.23219037\n",
      "Trained batch 759 batch loss 1.07030249 epoch total loss 1.23197711\n",
      "Trained batch 760 batch loss 1.13772821 epoch total loss 1.23185313\n",
      "Trained batch 761 batch loss 1.11821115 epoch total loss 1.23170376\n",
      "Trained batch 762 batch loss 1.29066777 epoch total loss 1.23178113\n",
      "Trained batch 763 batch loss 1.21976924 epoch total loss 1.23176539\n",
      "Trained batch 764 batch loss 1.26223898 epoch total loss 1.23180532\n",
      "Trained batch 765 batch loss 1.19798374 epoch total loss 1.23176122\n",
      "Trained batch 766 batch loss 1.24154067 epoch total loss 1.23177397\n",
      "Trained batch 767 batch loss 1.14999068 epoch total loss 1.23166728\n",
      "Trained batch 768 batch loss 1.20875096 epoch total loss 1.23163736\n",
      "Trained batch 769 batch loss 1.27846932 epoch total loss 1.23169827\n",
      "Trained batch 770 batch loss 1.18104 epoch total loss 1.23163247\n",
      "Trained batch 771 batch loss 1.04960024 epoch total loss 1.23139644\n",
      "Trained batch 772 batch loss 1.23806739 epoch total loss 1.23140502\n",
      "Trained batch 773 batch loss 1.11385763 epoch total loss 1.23125291\n",
      "Trained batch 774 batch loss 1.28714693 epoch total loss 1.23132515\n",
      "Trained batch 775 batch loss 1.26303768 epoch total loss 1.23136604\n",
      "Trained batch 776 batch loss 1.21519399 epoch total loss 1.2313453\n",
      "Trained batch 777 batch loss 1.04610932 epoch total loss 1.23110688\n",
      "Trained batch 778 batch loss 1.01063299 epoch total loss 1.2308234\n",
      "Trained batch 779 batch loss 0.961405635 epoch total loss 1.23047757\n",
      "Trained batch 780 batch loss 1.07263041 epoch total loss 1.23027527\n",
      "Trained batch 781 batch loss 1.23677313 epoch total loss 1.2302835\n",
      "Trained batch 782 batch loss 1.22059596 epoch total loss 1.2302711\n",
      "Trained batch 783 batch loss 1.15401351 epoch total loss 1.23017371\n",
      "Trained batch 784 batch loss 1.2156744 epoch total loss 1.23015523\n",
      "Trained batch 785 batch loss 1.22858727 epoch total loss 1.2301532\n",
      "Trained batch 786 batch loss 1.26301825 epoch total loss 1.23019505\n",
      "Trained batch 787 batch loss 1.17463708 epoch total loss 1.23012435\n",
      "Trained batch 788 batch loss 1.18712234 epoch total loss 1.23006988\n",
      "Trained batch 789 batch loss 1.20812678 epoch total loss 1.2300421\n",
      "Trained batch 790 batch loss 1.12499762 epoch total loss 1.22990906\n",
      "Trained batch 791 batch loss 1.13362968 epoch total loss 1.22978735\n",
      "Trained batch 792 batch loss 1.26457059 epoch total loss 1.22983122\n",
      "Trained batch 793 batch loss 1.33242726 epoch total loss 1.22996056\n",
      "Trained batch 794 batch loss 1.23888862 epoch total loss 1.22997189\n",
      "Trained batch 795 batch loss 1.21456015 epoch total loss 1.22995245\n",
      "Trained batch 796 batch loss 1.29154325 epoch total loss 1.23002982\n",
      "Trained batch 797 batch loss 1.21913159 epoch total loss 1.23001611\n",
      "Trained batch 798 batch loss 1.2667712 epoch total loss 1.23006225\n",
      "Trained batch 799 batch loss 1.11487651 epoch total loss 1.229918\n",
      "Trained batch 800 batch loss 1.28376007 epoch total loss 1.22998536\n",
      "Trained batch 801 batch loss 1.04310429 epoch total loss 1.22975206\n",
      "Trained batch 802 batch loss 1.28071797 epoch total loss 1.2298156\n",
      "Trained batch 803 batch loss 1.41551638 epoch total loss 1.23004687\n",
      "Trained batch 804 batch loss 1.29550529 epoch total loss 1.23012829\n",
      "Trained batch 805 batch loss 1.16574585 epoch total loss 1.2300483\n",
      "Trained batch 806 batch loss 1.07239294 epoch total loss 1.22985268\n",
      "Trained batch 807 batch loss 1.21159923 epoch total loss 1.22983015\n",
      "Trained batch 808 batch loss 1.24889207 epoch total loss 1.22985375\n",
      "Trained batch 809 batch loss 1.3832773 epoch total loss 1.23004341\n",
      "Trained batch 810 batch loss 1.25722349 epoch total loss 1.23007691\n",
      "Trained batch 811 batch loss 1.30557215 epoch total loss 1.23017\n",
      "Trained batch 812 batch loss 1.28218889 epoch total loss 1.23023403\n",
      "Trained batch 813 batch loss 1.21693206 epoch total loss 1.23021758\n",
      "Trained batch 814 batch loss 1.20673048 epoch total loss 1.23018873\n",
      "Trained batch 815 batch loss 1.40695882 epoch total loss 1.23040569\n",
      "Trained batch 816 batch loss 1.18007541 epoch total loss 1.23034394\n",
      "Trained batch 817 batch loss 1.20785284 epoch total loss 1.2303164\n",
      "Trained batch 818 batch loss 1.19754052 epoch total loss 1.23027635\n",
      "Trained batch 819 batch loss 1.25977516 epoch total loss 1.23031235\n",
      "Trained batch 820 batch loss 1.26305604 epoch total loss 1.23035228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 821 batch loss 1.26092541 epoch total loss 1.2303896\n",
      "Trained batch 822 batch loss 1.30594659 epoch total loss 1.23048151\n",
      "Trained batch 823 batch loss 1.29524922 epoch total loss 1.23056018\n",
      "Trained batch 824 batch loss 1.33871961 epoch total loss 1.23069143\n",
      "Trained batch 825 batch loss 1.36524296 epoch total loss 1.23085451\n",
      "Trained batch 826 batch loss 1.23426676 epoch total loss 1.23085868\n",
      "Trained batch 827 batch loss 1.24105954 epoch total loss 1.23087108\n",
      "Trained batch 828 batch loss 1.37236702 epoch total loss 1.23104191\n",
      "Trained batch 829 batch loss 1.30361092 epoch total loss 1.23112941\n",
      "Trained batch 830 batch loss 1.35100162 epoch total loss 1.23127389\n",
      "Trained batch 831 batch loss 1.24124932 epoch total loss 1.23128593\n",
      "Trained batch 832 batch loss 1.25244641 epoch total loss 1.23131132\n",
      "Trained batch 833 batch loss 1.26527917 epoch total loss 1.23135209\n",
      "Trained batch 834 batch loss 1.28922808 epoch total loss 1.23142147\n",
      "Trained batch 835 batch loss 1.24845839 epoch total loss 1.23144186\n",
      "Trained batch 836 batch loss 1.27035189 epoch total loss 1.23148835\n",
      "Trained batch 837 batch loss 1.28254247 epoch total loss 1.2315495\n",
      "Trained batch 838 batch loss 1.20688272 epoch total loss 1.23152\n",
      "Trained batch 839 batch loss 1.12514257 epoch total loss 1.23139322\n",
      "Trained batch 840 batch loss 1.02345324 epoch total loss 1.23114562\n",
      "Trained batch 841 batch loss 1.12457049 epoch total loss 1.2310189\n",
      "Trained batch 842 batch loss 1.13468075 epoch total loss 1.23090446\n",
      "Trained batch 843 batch loss 1.18975186 epoch total loss 1.23085546\n",
      "Trained batch 844 batch loss 1.15542376 epoch total loss 1.23076606\n",
      "Trained batch 845 batch loss 0.988319159 epoch total loss 1.23047912\n",
      "Trained batch 846 batch loss 1.17280507 epoch total loss 1.23041105\n",
      "Trained batch 847 batch loss 1.27155876 epoch total loss 1.23045969\n",
      "Trained batch 848 batch loss 1.17923748 epoch total loss 1.23039925\n",
      "Trained batch 849 batch loss 1.17805839 epoch total loss 1.23033762\n",
      "Trained batch 850 batch loss 1.15610337 epoch total loss 1.23025036\n",
      "Trained batch 851 batch loss 1.33074415 epoch total loss 1.23036838\n",
      "Trained batch 852 batch loss 1.41085124 epoch total loss 1.23058021\n",
      "Trained batch 853 batch loss 1.32203257 epoch total loss 1.23068738\n",
      "Trained batch 854 batch loss 1.19674909 epoch total loss 1.23064768\n",
      "Trained batch 855 batch loss 1.1623143 epoch total loss 1.23056781\n",
      "Trained batch 856 batch loss 1.21360791 epoch total loss 1.23054802\n",
      "Trained batch 857 batch loss 1.21433234 epoch total loss 1.23052919\n",
      "Trained batch 858 batch loss 1.17941713 epoch total loss 1.23046958\n",
      "Trained batch 859 batch loss 1.25747585 epoch total loss 1.23050106\n",
      "Trained batch 860 batch loss 1.2104373 epoch total loss 1.23047769\n",
      "Trained batch 861 batch loss 1.30630064 epoch total loss 1.23056567\n",
      "Trained batch 862 batch loss 1.27149129 epoch total loss 1.23061323\n",
      "Trained batch 863 batch loss 1.2802695 epoch total loss 1.23067069\n",
      "Trained batch 864 batch loss 1.30559933 epoch total loss 1.23075736\n",
      "Trained batch 865 batch loss 1.2612927 epoch total loss 1.23079276\n",
      "Trained batch 866 batch loss 1.28413534 epoch total loss 1.23085439\n",
      "Trained batch 867 batch loss 1.16199398 epoch total loss 1.230775\n",
      "Trained batch 868 batch loss 1.23918521 epoch total loss 1.23078465\n",
      "Trained batch 869 batch loss 1.22451949 epoch total loss 1.23077738\n",
      "Trained batch 870 batch loss 1.31271601 epoch total loss 1.23087156\n",
      "Trained batch 871 batch loss 1.31402802 epoch total loss 1.23096716\n",
      "Trained batch 872 batch loss 1.240767 epoch total loss 1.23097825\n",
      "Trained batch 873 batch loss 1.23362899 epoch total loss 1.23098135\n",
      "Trained batch 874 batch loss 1.32390141 epoch total loss 1.23108757\n",
      "Trained batch 875 batch loss 1.29187775 epoch total loss 1.23115706\n",
      "Trained batch 876 batch loss 1.32786703 epoch total loss 1.23126745\n",
      "Trained batch 877 batch loss 1.23427916 epoch total loss 1.23127091\n",
      "Trained batch 878 batch loss 1.25699568 epoch total loss 1.23130012\n",
      "Trained batch 879 batch loss 1.36410332 epoch total loss 1.23145127\n",
      "Trained batch 880 batch loss 1.29653 epoch total loss 1.23152518\n",
      "Trained batch 881 batch loss 1.30234599 epoch total loss 1.23160565\n",
      "Trained batch 882 batch loss 1.22150028 epoch total loss 1.2315942\n",
      "Trained batch 883 batch loss 1.08694875 epoch total loss 1.23143041\n",
      "Trained batch 884 batch loss 1.11328804 epoch total loss 1.23129678\n",
      "Trained batch 885 batch loss 1.19674683 epoch total loss 1.23125768\n",
      "Trained batch 886 batch loss 1.20052266 epoch total loss 1.23122311\n",
      "Trained batch 887 batch loss 1.18706703 epoch total loss 1.23117328\n",
      "Trained batch 888 batch loss 1.29450953 epoch total loss 1.23124456\n",
      "Trained batch 889 batch loss 1.22606587 epoch total loss 1.23123884\n",
      "Trained batch 890 batch loss 1.14881885 epoch total loss 1.23114622\n",
      "Trained batch 891 batch loss 1.16670811 epoch total loss 1.23107386\n",
      "Trained batch 892 batch loss 1.22869325 epoch total loss 1.23107111\n",
      "Trained batch 893 batch loss 1.1651516 epoch total loss 1.23099732\n",
      "Trained batch 894 batch loss 1.24625421 epoch total loss 1.23101437\n",
      "Trained batch 895 batch loss 1.27223086 epoch total loss 1.23106039\n",
      "Trained batch 896 batch loss 1.25230467 epoch total loss 1.23108411\n",
      "Trained batch 897 batch loss 1.23759234 epoch total loss 1.23109138\n",
      "Trained batch 898 batch loss 1.24689281 epoch total loss 1.23110902\n",
      "Trained batch 899 batch loss 1.2577045 epoch total loss 1.23113859\n",
      "Trained batch 900 batch loss 1.42017221 epoch total loss 1.23134863\n",
      "Trained batch 901 batch loss 1.25724912 epoch total loss 1.23137724\n",
      "Trained batch 902 batch loss 1.13856292 epoch total loss 1.23127437\n",
      "Trained batch 903 batch loss 1.13739705 epoch total loss 1.23117054\n",
      "Trained batch 904 batch loss 1.22203743 epoch total loss 1.2311604\n",
      "Trained batch 905 batch loss 1.27593744 epoch total loss 1.23120975\n",
      "Trained batch 906 batch loss 1.28312564 epoch total loss 1.23126709\n",
      "Trained batch 907 batch loss 1.36841059 epoch total loss 1.23141825\n",
      "Trained batch 908 batch loss 1.2849865 epoch total loss 1.23147726\n",
      "Trained batch 909 batch loss 1.08432794 epoch total loss 1.23131549\n",
      "Trained batch 910 batch loss 1.26050639 epoch total loss 1.23134756\n",
      "Trained batch 911 batch loss 1.30685854 epoch total loss 1.23143041\n",
      "Trained batch 912 batch loss 1.21282411 epoch total loss 1.23141\n",
      "Trained batch 913 batch loss 1.23706508 epoch total loss 1.23141611\n",
      "Trained batch 914 batch loss 1.43111598 epoch total loss 1.23163474\n",
      "Trained batch 915 batch loss 1.28372037 epoch total loss 1.2316916\n",
      "Trained batch 916 batch loss 1.14084578 epoch total loss 1.23159242\n",
      "Trained batch 917 batch loss 1.17979193 epoch total loss 1.23153591\n",
      "Trained batch 918 batch loss 1.09459019 epoch total loss 1.23138678\n",
      "Trained batch 919 batch loss 1.1450386 epoch total loss 1.23129284\n",
      "Trained batch 920 batch loss 1.2411108 epoch total loss 1.23130345\n",
      "Trained batch 921 batch loss 1.18411946 epoch total loss 1.23125219\n",
      "Trained batch 922 batch loss 1.2795707 epoch total loss 1.23130453\n",
      "Trained batch 923 batch loss 1.09559143 epoch total loss 1.23115754\n",
      "Trained batch 924 batch loss 1.11896849 epoch total loss 1.23103619\n",
      "Trained batch 925 batch loss 1.21873665 epoch total loss 1.23102283\n",
      "Trained batch 926 batch loss 1.34744656 epoch total loss 1.2311486\n",
      "Trained batch 927 batch loss 1.23773324 epoch total loss 1.23115575\n",
      "Trained batch 928 batch loss 1.20286834 epoch total loss 1.23112524\n",
      "Trained batch 929 batch loss 1.1180222 epoch total loss 1.23100352\n",
      "Trained batch 930 batch loss 0.953482032 epoch total loss 1.23070514\n",
      "Trained batch 931 batch loss 0.867298 epoch total loss 1.23031485\n",
      "Trained batch 932 batch loss 1.02433479 epoch total loss 1.23009372\n",
      "Trained batch 933 batch loss 1.16024554 epoch total loss 1.23001897\n",
      "Trained batch 934 batch loss 1.38237214 epoch total loss 1.23018205\n",
      "Trained batch 935 batch loss 1.24763358 epoch total loss 1.23020077\n",
      "Trained batch 936 batch loss 1.26415598 epoch total loss 1.23023701\n",
      "Trained batch 937 batch loss 1.19602895 epoch total loss 1.23020053\n",
      "Trained batch 938 batch loss 1.2650106 epoch total loss 1.2302376\n",
      "Trained batch 939 batch loss 1.20518732 epoch total loss 1.2302109\n",
      "Trained batch 940 batch loss 1.21584809 epoch total loss 1.23019564\n",
      "Trained batch 941 batch loss 1.15202069 epoch total loss 1.23011255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 942 batch loss 1.33435106 epoch total loss 1.23022318\n",
      "Trained batch 943 batch loss 1.2240026 epoch total loss 1.23021662\n",
      "Trained batch 944 batch loss 1.22005677 epoch total loss 1.23020589\n",
      "Trained batch 945 batch loss 1.26430285 epoch total loss 1.23024189\n",
      "Trained batch 946 batch loss 1.1518786 epoch total loss 1.23015904\n",
      "Trained batch 947 batch loss 1.20173383 epoch total loss 1.23012912\n",
      "Trained batch 948 batch loss 1.16826379 epoch total loss 1.2300638\n",
      "Trained batch 949 batch loss 1.15978205 epoch total loss 1.22998977\n",
      "Trained batch 950 batch loss 1.11209548 epoch total loss 1.22986555\n",
      "Trained batch 951 batch loss 1.06740117 epoch total loss 1.22969472\n",
      "Trained batch 952 batch loss 1.14661443 epoch total loss 1.22960746\n",
      "Trained batch 953 batch loss 1.13152254 epoch total loss 1.22950447\n",
      "Trained batch 954 batch loss 1.30552804 epoch total loss 1.22958422\n",
      "Trained batch 955 batch loss 1.19204211 epoch total loss 1.22954488\n",
      "Trained batch 956 batch loss 1.19726634 epoch total loss 1.22951114\n",
      "Trained batch 957 batch loss 1.28893256 epoch total loss 1.22957313\n",
      "Trained batch 958 batch loss 1.19981217 epoch total loss 1.22954214\n",
      "Trained batch 959 batch loss 1.12537956 epoch total loss 1.22943354\n",
      "Trained batch 960 batch loss 1.23318601 epoch total loss 1.22943735\n",
      "Trained batch 961 batch loss 1.14881825 epoch total loss 1.22935343\n",
      "Trained batch 962 batch loss 1.23241305 epoch total loss 1.22935665\n",
      "Trained batch 963 batch loss 1.30250132 epoch total loss 1.22943258\n",
      "Trained batch 964 batch loss 1.25894952 epoch total loss 1.22946322\n",
      "Trained batch 965 batch loss 1.25510013 epoch total loss 1.2294898\n",
      "Trained batch 966 batch loss 1.15119374 epoch total loss 1.22940874\n",
      "Trained batch 967 batch loss 1.17540061 epoch total loss 1.22935295\n",
      "Trained batch 968 batch loss 1.20255828 epoch total loss 1.22932518\n",
      "Trained batch 969 batch loss 1.30483985 epoch total loss 1.22940314\n",
      "Trained batch 970 batch loss 1.26058173 epoch total loss 1.22943532\n",
      "Trained batch 971 batch loss 1.37697244 epoch total loss 1.2295872\n",
      "Trained batch 972 batch loss 1.32425761 epoch total loss 1.22968459\n",
      "Trained batch 973 batch loss 1.37943661 epoch total loss 1.22983849\n",
      "Trained batch 974 batch loss 1.30146766 epoch total loss 1.22991204\n",
      "Trained batch 975 batch loss 1.10411811 epoch total loss 1.22978306\n",
      "Trained batch 976 batch loss 1.22086334 epoch total loss 1.22977388\n",
      "Trained batch 977 batch loss 1.4441824 epoch total loss 1.22999334\n",
      "Trained batch 978 batch loss 1.22398782 epoch total loss 1.22998726\n",
      "Trained batch 979 batch loss 1.2244761 epoch total loss 1.22998154\n",
      "Trained batch 980 batch loss 1.20045 epoch total loss 1.2299515\n",
      "Trained batch 981 batch loss 1.16245365 epoch total loss 1.22988272\n",
      "Trained batch 982 batch loss 1.23037136 epoch total loss 1.22988319\n",
      "Trained batch 983 batch loss 1.13334811 epoch total loss 1.22978485\n",
      "Trained batch 984 batch loss 1.07445443 epoch total loss 1.22962701\n",
      "Trained batch 985 batch loss 1.00726855 epoch total loss 1.22940135\n",
      "Trained batch 986 batch loss 1.15326536 epoch total loss 1.22932422\n",
      "Trained batch 987 batch loss 1.06652927 epoch total loss 1.22915924\n",
      "Trained batch 988 batch loss 1.10902476 epoch total loss 1.22903764\n",
      "Trained batch 989 batch loss 1.09231031 epoch total loss 1.22889936\n",
      "Trained batch 990 batch loss 1.23112023 epoch total loss 1.22890151\n",
      "Trained batch 991 batch loss 1.19276309 epoch total loss 1.22886503\n",
      "Trained batch 992 batch loss 1.16505 epoch total loss 1.22880077\n",
      "Trained batch 993 batch loss 1.0929606 epoch total loss 1.22866404\n",
      "Trained batch 994 batch loss 1.1349864 epoch total loss 1.22856975\n",
      "Trained batch 995 batch loss 1.33575928 epoch total loss 1.22867751\n",
      "Trained batch 996 batch loss 1.19702113 epoch total loss 1.2286458\n",
      "Trained batch 997 batch loss 1.26079464 epoch total loss 1.22867799\n",
      "Trained batch 998 batch loss 1.25304675 epoch total loss 1.22870243\n",
      "Trained batch 999 batch loss 1.09841967 epoch total loss 1.22857201\n",
      "Trained batch 1000 batch loss 1.35453641 epoch total loss 1.2286979\n",
      "Trained batch 1001 batch loss 1.24951112 epoch total loss 1.22871864\n",
      "Trained batch 1002 batch loss 1.34342694 epoch total loss 1.22883308\n",
      "Trained batch 1003 batch loss 1.37795329 epoch total loss 1.22898173\n",
      "Trained batch 1004 batch loss 1.49850261 epoch total loss 1.22925019\n",
      "Trained batch 1005 batch loss 1.38899541 epoch total loss 1.22940922\n",
      "Trained batch 1006 batch loss 1.14991713 epoch total loss 1.22933018\n",
      "Trained batch 1007 batch loss 1.14088488 epoch total loss 1.22924232\n",
      "Trained batch 1008 batch loss 1.14966047 epoch total loss 1.22916341\n",
      "Trained batch 1009 batch loss 1.11124575 epoch total loss 1.22904646\n",
      "Trained batch 1010 batch loss 1.17743576 epoch total loss 1.22899544\n",
      "Trained batch 1011 batch loss 1.06505942 epoch total loss 1.22883332\n",
      "Trained batch 1012 batch loss 1.12293553 epoch total loss 1.22872865\n",
      "Trained batch 1013 batch loss 1.09200335 epoch total loss 1.22859371\n",
      "Trained batch 1014 batch loss 1.00066221 epoch total loss 1.22836888\n",
      "Trained batch 1015 batch loss 1.03396451 epoch total loss 1.22817731\n",
      "Trained batch 1016 batch loss 1.22987294 epoch total loss 1.22817898\n",
      "Trained batch 1017 batch loss 1.27848637 epoch total loss 1.22822845\n",
      "Trained batch 1018 batch loss 0.99273634 epoch total loss 1.22799706\n",
      "Trained batch 1019 batch loss 1.17507267 epoch total loss 1.22794509\n",
      "Trained batch 1020 batch loss 1.0953083 epoch total loss 1.22781503\n",
      "Trained batch 1021 batch loss 1.11454511 epoch total loss 1.22770405\n",
      "Trained batch 1022 batch loss 1.10561311 epoch total loss 1.2275846\n",
      "Trained batch 1023 batch loss 1.0390805 epoch total loss 1.2274003\n",
      "Trained batch 1024 batch loss 1.19458723 epoch total loss 1.22736824\n",
      "Trained batch 1025 batch loss 1.00302589 epoch total loss 1.22714937\n",
      "Trained batch 1026 batch loss 1.00711584 epoch total loss 1.22693491\n",
      "Trained batch 1027 batch loss 1.12456405 epoch total loss 1.22683513\n",
      "Trained batch 1028 batch loss 1.12951 epoch total loss 1.22674048\n",
      "Trained batch 1029 batch loss 1.13846815 epoch total loss 1.22665465\n",
      "Trained batch 1030 batch loss 1.04790664 epoch total loss 1.22648108\n",
      "Trained batch 1031 batch loss 1.15676975 epoch total loss 1.22641349\n",
      "Trained batch 1032 batch loss 1.16150963 epoch total loss 1.22635055\n",
      "Trained batch 1033 batch loss 1.3050195 epoch total loss 1.22642672\n",
      "Trained batch 1034 batch loss 1.38678706 epoch total loss 1.22658181\n",
      "Trained batch 1035 batch loss 1.17319083 epoch total loss 1.22653031\n",
      "Trained batch 1036 batch loss 1.31372225 epoch total loss 1.22661448\n",
      "Trained batch 1037 batch loss 1.4138962 epoch total loss 1.22679508\n",
      "Trained batch 1038 batch loss 1.36506212 epoch total loss 1.22692835\n",
      "Trained batch 1039 batch loss 1.16695821 epoch total loss 1.22687066\n",
      "Trained batch 1040 batch loss 1.09220397 epoch total loss 1.22674119\n",
      "Trained batch 1041 batch loss 1.163414 epoch total loss 1.2266804\n",
      "Trained batch 1042 batch loss 1.20404 epoch total loss 1.22665858\n",
      "Trained batch 1043 batch loss 1.29773855 epoch total loss 1.22672665\n",
      "Trained batch 1044 batch loss 1.32802749 epoch total loss 1.22682369\n",
      "Trained batch 1045 batch loss 1.14304769 epoch total loss 1.22674358\n",
      "Trained batch 1046 batch loss 1.24063623 epoch total loss 1.22675681\n",
      "Trained batch 1047 batch loss 1.3267262 epoch total loss 1.2268523\n",
      "Trained batch 1048 batch loss 1.17680085 epoch total loss 1.22680449\n",
      "Trained batch 1049 batch loss 1.17013013 epoch total loss 1.22675049\n",
      "Trained batch 1050 batch loss 1.12510395 epoch total loss 1.22665381\n",
      "Trained batch 1051 batch loss 1.09183288 epoch total loss 1.22652543\n",
      "Trained batch 1052 batch loss 1.21468198 epoch total loss 1.22651422\n",
      "Trained batch 1053 batch loss 1.15802431 epoch total loss 1.22644925\n",
      "Trained batch 1054 batch loss 1.11350226 epoch total loss 1.22634208\n",
      "Trained batch 1055 batch loss 1.13122153 epoch total loss 1.22625196\n",
      "Trained batch 1056 batch loss 1.15363503 epoch total loss 1.22618318\n",
      "Trained batch 1057 batch loss 1.23458672 epoch total loss 1.22619116\n",
      "Trained batch 1058 batch loss 1.1285671 epoch total loss 1.2260989\n",
      "Trained batch 1059 batch loss 1.21033263 epoch total loss 1.22608399\n",
      "Trained batch 1060 batch loss 1.32308471 epoch total loss 1.22617555\n",
      "Trained batch 1061 batch loss 1.27796769 epoch total loss 1.2262243\n",
      "Trained batch 1062 batch loss 1.20155334 epoch total loss 1.22620106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1063 batch loss 1.33843541 epoch total loss 1.22630668\n",
      "Trained batch 1064 batch loss 1.17465 epoch total loss 1.22625816\n",
      "Trained batch 1065 batch loss 1.18622351 epoch total loss 1.22622061\n",
      "Trained batch 1066 batch loss 1.3638109 epoch total loss 1.22634959\n",
      "Trained batch 1067 batch loss 1.14704645 epoch total loss 1.22627532\n",
      "Trained batch 1068 batch loss 1.12450469 epoch total loss 1.22618008\n",
      "Trained batch 1069 batch loss 1.17946839 epoch total loss 1.22613633\n",
      "Trained batch 1070 batch loss 1.09678578 epoch total loss 1.22601545\n",
      "Trained batch 1071 batch loss 1.09675348 epoch total loss 1.22589481\n",
      "Trained batch 1072 batch loss 1.02149892 epoch total loss 1.22570419\n",
      "Trained batch 1073 batch loss 1.08125961 epoch total loss 1.22556961\n",
      "Trained batch 1074 batch loss 1.0818572 epoch total loss 1.22543573\n",
      "Trained batch 1075 batch loss 1.09680092 epoch total loss 1.22531617\n",
      "Trained batch 1076 batch loss 1.12011433 epoch total loss 1.22521842\n",
      "Trained batch 1077 batch loss 1.05287099 epoch total loss 1.22505832\n",
      "Trained batch 1078 batch loss 1.13069916 epoch total loss 1.22497082\n",
      "Trained batch 1079 batch loss 1.18090487 epoch total loss 1.22493\n",
      "Trained batch 1080 batch loss 1.24809551 epoch total loss 1.22495139\n",
      "Trained batch 1081 batch loss 1.2957561 epoch total loss 1.22501695\n",
      "Trained batch 1082 batch loss 1.31922734 epoch total loss 1.22510397\n",
      "Trained batch 1083 batch loss 1.25912404 epoch total loss 1.22513545\n",
      "Trained batch 1084 batch loss 1.2039423 epoch total loss 1.2251159\n",
      "Trained batch 1085 batch loss 1.32329583 epoch total loss 1.22520638\n",
      "Trained batch 1086 batch loss 1.28515446 epoch total loss 1.22526157\n",
      "Trained batch 1087 batch loss 1.1955539 epoch total loss 1.22523427\n",
      "Trained batch 1088 batch loss 1.35119224 epoch total loss 1.22535\n",
      "Trained batch 1089 batch loss 1.28383589 epoch total loss 1.22540367\n",
      "Trained batch 1090 batch loss 1.50680184 epoch total loss 1.22566187\n",
      "Trained batch 1091 batch loss 1.34918761 epoch total loss 1.22577512\n",
      "Trained batch 1092 batch loss 1.16009474 epoch total loss 1.22571492\n",
      "Trained batch 1093 batch loss 1.24378014 epoch total loss 1.22573149\n",
      "Trained batch 1094 batch loss 1.20052791 epoch total loss 1.22570848\n",
      "Trained batch 1095 batch loss 1.25314021 epoch total loss 1.22573352\n",
      "Trained batch 1096 batch loss 1.3228426 epoch total loss 1.22582221\n",
      "Trained batch 1097 batch loss 1.32990098 epoch total loss 1.2259171\n",
      "Trained batch 1098 batch loss 1.29309225 epoch total loss 1.22597826\n",
      "Trained batch 1099 batch loss 1.35047328 epoch total loss 1.2260915\n",
      "Trained batch 1100 batch loss 1.40992594 epoch total loss 1.22625864\n",
      "Trained batch 1101 batch loss 1.39708972 epoch total loss 1.22641385\n",
      "Trained batch 1102 batch loss 1.41270185 epoch total loss 1.22658288\n",
      "Trained batch 1103 batch loss 1.34747136 epoch total loss 1.22669244\n",
      "Trained batch 1104 batch loss 1.3279916 epoch total loss 1.22678423\n",
      "Trained batch 1105 batch loss 1.36253977 epoch total loss 1.22690701\n",
      "Trained batch 1106 batch loss 1.27314377 epoch total loss 1.22694886\n",
      "Trained batch 1107 batch loss 1.21512413 epoch total loss 1.22693825\n",
      "Trained batch 1108 batch loss 1.16894865 epoch total loss 1.2268858\n",
      "Trained batch 1109 batch loss 1.15267658 epoch total loss 1.22681892\n",
      "Trained batch 1110 batch loss 1.20067918 epoch total loss 1.22679543\n",
      "Trained batch 1111 batch loss 1.32934546 epoch total loss 1.2268877\n",
      "Trained batch 1112 batch loss 1.19451952 epoch total loss 1.22685862\n",
      "Trained batch 1113 batch loss 1.26025963 epoch total loss 1.22688866\n",
      "Trained batch 1114 batch loss 1.09945166 epoch total loss 1.22677433\n",
      "Trained batch 1115 batch loss 1.12275648 epoch total loss 1.22668111\n",
      "Trained batch 1116 batch loss 1.23159885 epoch total loss 1.2266854\n",
      "Trained batch 1117 batch loss 1.14775956 epoch total loss 1.22661471\n",
      "Trained batch 1118 batch loss 1.2492584 epoch total loss 1.22663498\n",
      "Trained batch 1119 batch loss 1.32691288 epoch total loss 1.22672462\n",
      "Trained batch 1120 batch loss 1.22019875 epoch total loss 1.22671878\n",
      "Trained batch 1121 batch loss 1.251791 epoch total loss 1.22674119\n",
      "Trained batch 1122 batch loss 1.19685912 epoch total loss 1.22671461\n",
      "Trained batch 1123 batch loss 1.2209723 epoch total loss 1.22670949\n",
      "Trained batch 1124 batch loss 1.28795576 epoch total loss 1.22676396\n",
      "Trained batch 1125 batch loss 1.19733071 epoch total loss 1.22673786\n",
      "Trained batch 1126 batch loss 1.16334343 epoch total loss 1.22668159\n",
      "Trained batch 1127 batch loss 1.23837328 epoch total loss 1.22669196\n",
      "Trained batch 1128 batch loss 1.30560422 epoch total loss 1.22676194\n",
      "Trained batch 1129 batch loss 1.27848029 epoch total loss 1.22680771\n",
      "Trained batch 1130 batch loss 1.29683471 epoch total loss 1.2268697\n",
      "Trained batch 1131 batch loss 1.33019078 epoch total loss 1.22696114\n",
      "Trained batch 1132 batch loss 1.25453615 epoch total loss 1.22698545\n",
      "Trained batch 1133 batch loss 1.41575766 epoch total loss 1.22715211\n",
      "Trained batch 1134 batch loss 1.27018917 epoch total loss 1.22719\n",
      "Trained batch 1135 batch loss 1.22359705 epoch total loss 1.2271868\n",
      "Trained batch 1136 batch loss 1.2418834 epoch total loss 1.22719979\n",
      "Trained batch 1137 batch loss 1.24117112 epoch total loss 1.22721219\n",
      "Trained batch 1138 batch loss 1.16272426 epoch total loss 1.22715545\n",
      "Trained batch 1139 batch loss 1.15144086 epoch total loss 1.22708905\n",
      "Trained batch 1140 batch loss 1.22745645 epoch total loss 1.22708929\n",
      "Trained batch 1141 batch loss 1.0979656 epoch total loss 1.22697616\n",
      "Trained batch 1142 batch loss 1.15343976 epoch total loss 1.22691178\n",
      "Trained batch 1143 batch loss 1.08315039 epoch total loss 1.22678602\n",
      "Trained batch 1144 batch loss 1.06209326 epoch total loss 1.22664213\n",
      "Trained batch 1145 batch loss 1.19789565 epoch total loss 1.22661698\n",
      "Trained batch 1146 batch loss 1.2467829 epoch total loss 1.22663462\n",
      "Trained batch 1147 batch loss 1.22030044 epoch total loss 1.22662914\n",
      "Trained batch 1148 batch loss 1.2669301 epoch total loss 1.2266643\n",
      "Trained batch 1149 batch loss 1.33975136 epoch total loss 1.22676265\n",
      "Trained batch 1150 batch loss 1.25353217 epoch total loss 1.2267859\n",
      "Trained batch 1151 batch loss 1.21965837 epoch total loss 1.2267797\n",
      "Trained batch 1152 batch loss 1.28046644 epoch total loss 1.22682631\n",
      "Trained batch 1153 batch loss 1.21091 epoch total loss 1.2268126\n",
      "Trained batch 1154 batch loss 1.32461643 epoch total loss 1.22689724\n",
      "Trained batch 1155 batch loss 1.18740928 epoch total loss 1.22686303\n",
      "Trained batch 1156 batch loss 1.22316647 epoch total loss 1.22685981\n",
      "Trained batch 1157 batch loss 1.32570267 epoch total loss 1.22694528\n",
      "Trained batch 1158 batch loss 1.38299704 epoch total loss 1.22708011\n",
      "Trained batch 1159 batch loss 1.04641128 epoch total loss 1.22692418\n",
      "Trained batch 1160 batch loss 0.982380867 epoch total loss 1.22671342\n",
      "Trained batch 1161 batch loss 1.11399841 epoch total loss 1.22661638\n",
      "Trained batch 1162 batch loss 1.18602 epoch total loss 1.22658145\n",
      "Trained batch 1163 batch loss 1.18312335 epoch total loss 1.22654402\n",
      "Trained batch 1164 batch loss 1.1402123 epoch total loss 1.22646987\n",
      "Trained batch 1165 batch loss 1.24508703 epoch total loss 1.22648585\n",
      "Trained batch 1166 batch loss 1.24251473 epoch total loss 1.22649968\n",
      "Trained batch 1167 batch loss 1.33055937 epoch total loss 1.22658885\n",
      "Trained batch 1168 batch loss 1.1899395 epoch total loss 1.22655749\n",
      "Trained batch 1169 batch loss 1.3627522 epoch total loss 1.22667408\n",
      "Trained batch 1170 batch loss 1.46989 epoch total loss 1.22688186\n",
      "Trained batch 1171 batch loss 1.35203433 epoch total loss 1.22698879\n",
      "Trained batch 1172 batch loss 1.09171605 epoch total loss 1.22687328\n",
      "Trained batch 1173 batch loss 1.1510551 epoch total loss 1.22680867\n",
      "Trained batch 1174 batch loss 1.27297616 epoch total loss 1.22684789\n",
      "Trained batch 1175 batch loss 1.25315428 epoch total loss 1.2268703\n",
      "Trained batch 1176 batch loss 1.29191494 epoch total loss 1.22692561\n",
      "Trained batch 1177 batch loss 1.26175714 epoch total loss 1.22695518\n",
      "Trained batch 1178 batch loss 1.28647566 epoch total loss 1.22700572\n",
      "Trained batch 1179 batch loss 1.10669446 epoch total loss 1.22690368\n",
      "Trained batch 1180 batch loss 1.14562869 epoch total loss 1.22683477\n",
      "Trained batch 1181 batch loss 1.12696552 epoch total loss 1.22675025\n",
      "Trained batch 1182 batch loss 1.15128541 epoch total loss 1.22668636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1183 batch loss 1.22169161 epoch total loss 1.22668207\n",
      "Trained batch 1184 batch loss 1.26080668 epoch total loss 1.22671092\n",
      "Trained batch 1185 batch loss 1.34326625 epoch total loss 1.22680926\n",
      "Trained batch 1186 batch loss 1.25844169 epoch total loss 1.22683597\n",
      "Trained batch 1187 batch loss 1.30864596 epoch total loss 1.22690487\n",
      "Trained batch 1188 batch loss 1.26495397 epoch total loss 1.22693694\n",
      "Trained batch 1189 batch loss 1.40962529 epoch total loss 1.2270906\n",
      "Trained batch 1190 batch loss 1.2882036 epoch total loss 1.22714198\n",
      "Trained batch 1191 batch loss 1.35754967 epoch total loss 1.22725141\n",
      "Trained batch 1192 batch loss 1.23436558 epoch total loss 1.22725749\n",
      "Trained batch 1193 batch loss 1.45466018 epoch total loss 1.22744811\n",
      "Trained batch 1194 batch loss 1.50385058 epoch total loss 1.22767961\n",
      "Trained batch 1195 batch loss 1.22422755 epoch total loss 1.22767675\n",
      "Trained batch 1196 batch loss 1.37945676 epoch total loss 1.22780371\n",
      "Trained batch 1197 batch loss 1.2311573 epoch total loss 1.22780657\n",
      "Trained batch 1198 batch loss 1.29239678 epoch total loss 1.22786045\n",
      "Trained batch 1199 batch loss 1.33700013 epoch total loss 1.22795153\n",
      "Trained batch 1200 batch loss 1.16022563 epoch total loss 1.22789514\n",
      "Trained batch 1201 batch loss 1.2209599 epoch total loss 1.2278893\n",
      "Trained batch 1202 batch loss 1.17527437 epoch total loss 1.22784555\n",
      "Trained batch 1203 batch loss 1.27900934 epoch total loss 1.22788811\n",
      "Trained batch 1204 batch loss 1.23140144 epoch total loss 1.22789109\n",
      "Trained batch 1205 batch loss 1.35501671 epoch total loss 1.22799659\n",
      "Trained batch 1206 batch loss 1.26279402 epoch total loss 1.22802544\n",
      "Trained batch 1207 batch loss 1.24045813 epoch total loss 1.22803569\n",
      "Trained batch 1208 batch loss 1.49014103 epoch total loss 1.22825265\n",
      "Trained batch 1209 batch loss 1.08893049 epoch total loss 1.22813749\n",
      "Trained batch 1210 batch loss 1.100842 epoch total loss 1.22803223\n",
      "Trained batch 1211 batch loss 1.04872406 epoch total loss 1.22788417\n",
      "Trained batch 1212 batch loss 1.29560685 epoch total loss 1.22794008\n",
      "Trained batch 1213 batch loss 1.34579933 epoch total loss 1.22803736\n",
      "Trained batch 1214 batch loss 1.12153983 epoch total loss 1.22794962\n",
      "Trained batch 1215 batch loss 1.22812128 epoch total loss 1.22794974\n",
      "Trained batch 1216 batch loss 1.18273735 epoch total loss 1.22791255\n",
      "Trained batch 1217 batch loss 1.18377972 epoch total loss 1.22787642\n",
      "Trained batch 1218 batch loss 1.23108184 epoch total loss 1.22787905\n",
      "Trained batch 1219 batch loss 1.21543205 epoch total loss 1.2278688\n",
      "Trained batch 1220 batch loss 1.19606423 epoch total loss 1.22784269\n",
      "Trained batch 1221 batch loss 1.10929918 epoch total loss 1.22774565\n",
      "Trained batch 1222 batch loss 1.16281724 epoch total loss 1.22769248\n",
      "Trained batch 1223 batch loss 1.20854044 epoch total loss 1.22767675\n",
      "Trained batch 1224 batch loss 1.25508988 epoch total loss 1.22769928\n",
      "Trained batch 1225 batch loss 1.17850089 epoch total loss 1.22765899\n",
      "Trained batch 1226 batch loss 1.2382704 epoch total loss 1.22766769\n",
      "Trained batch 1227 batch loss 1.06353402 epoch total loss 1.22753394\n",
      "Trained batch 1228 batch loss 1.19242859 epoch total loss 1.22750521\n",
      "Trained batch 1229 batch loss 1.21394825 epoch total loss 1.22749424\n",
      "Trained batch 1230 batch loss 1.18492985 epoch total loss 1.22745967\n",
      "Trained batch 1231 batch loss 1.26425087 epoch total loss 1.22748959\n",
      "Trained batch 1232 batch loss 1.21812093 epoch total loss 1.22748196\n",
      "Trained batch 1233 batch loss 1.11655331 epoch total loss 1.22739208\n",
      "Trained batch 1234 batch loss 1.10972238 epoch total loss 1.22729671\n",
      "Trained batch 1235 batch loss 1.23262048 epoch total loss 1.227301\n",
      "Trained batch 1236 batch loss 1.06881773 epoch total loss 1.22717285\n",
      "Trained batch 1237 batch loss 1.15788615 epoch total loss 1.22711682\n",
      "Trained batch 1238 batch loss 1.34526563 epoch total loss 1.22721219\n",
      "Trained batch 1239 batch loss 1.39898479 epoch total loss 1.22735083\n",
      "Trained batch 1240 batch loss 1.1783694 epoch total loss 1.22731125\n",
      "Trained batch 1241 batch loss 1.06754637 epoch total loss 1.22718251\n",
      "Trained batch 1242 batch loss 1.11191893 epoch total loss 1.22708964\n",
      "Trained batch 1243 batch loss 1.10749984 epoch total loss 1.22699356\n",
      "Trained batch 1244 batch loss 1.16145074 epoch total loss 1.22694087\n",
      "Trained batch 1245 batch loss 1.28342056 epoch total loss 1.22698629\n",
      "Trained batch 1246 batch loss 1.23501205 epoch total loss 1.22699273\n",
      "Trained batch 1247 batch loss 1.281672 epoch total loss 1.22703648\n",
      "Trained batch 1248 batch loss 1.15843153 epoch total loss 1.22698152\n",
      "Trained batch 1249 batch loss 1.1684345 epoch total loss 1.22693467\n",
      "Trained batch 1250 batch loss 1.15220702 epoch total loss 1.22687495\n",
      "Trained batch 1251 batch loss 1.29346609 epoch total loss 1.22692811\n",
      "Trained batch 1252 batch loss 1.2598 epoch total loss 1.22695434\n",
      "Trained batch 1253 batch loss 1.29238486 epoch total loss 1.22700655\n",
      "Trained batch 1254 batch loss 1.17358828 epoch total loss 1.226964\n",
      "Trained batch 1255 batch loss 1.1349932 epoch total loss 1.22689068\n",
      "Trained batch 1256 batch loss 1.10761654 epoch total loss 1.22679579\n",
      "Trained batch 1257 batch loss 1.24975705 epoch total loss 1.22681403\n",
      "Trained batch 1258 batch loss 1.25043774 epoch total loss 1.22683287\n",
      "Trained batch 1259 batch loss 1.27700114 epoch total loss 1.22687268\n",
      "Trained batch 1260 batch loss 1.21674824 epoch total loss 1.2268647\n",
      "Trained batch 1261 batch loss 1.17526 epoch total loss 1.22682381\n",
      "Trained batch 1262 batch loss 1.19918287 epoch total loss 1.22680187\n",
      "Trained batch 1263 batch loss 1.16716897 epoch total loss 1.22675467\n",
      "Trained batch 1264 batch loss 1.12764525 epoch total loss 1.22667623\n",
      "Trained batch 1265 batch loss 1.07623982 epoch total loss 1.22655737\n",
      "Trained batch 1266 batch loss 1.17785203 epoch total loss 1.22651887\n",
      "Trained batch 1267 batch loss 1.06390715 epoch total loss 1.2263906\n",
      "Trained batch 1268 batch loss 1.12560105 epoch total loss 1.22631109\n",
      "Trained batch 1269 batch loss 1.11319613 epoch total loss 1.22622192\n",
      "Trained batch 1270 batch loss 1.1285758 epoch total loss 1.22614503\n",
      "Trained batch 1271 batch loss 1.10220575 epoch total loss 1.22604752\n",
      "Trained batch 1272 batch loss 1.38214898 epoch total loss 1.2261703\n",
      "Trained batch 1273 batch loss 1.11977482 epoch total loss 1.22608674\n",
      "Trained batch 1274 batch loss 1.19977915 epoch total loss 1.22606611\n",
      "Trained batch 1275 batch loss 1.28801394 epoch total loss 1.22611463\n",
      "Trained batch 1276 batch loss 1.14435232 epoch total loss 1.22605062\n",
      "Trained batch 1277 batch loss 1.10495043 epoch total loss 1.22595572\n",
      "Trained batch 1278 batch loss 1.34413934 epoch total loss 1.22604823\n",
      "Trained batch 1279 batch loss 1.32141185 epoch total loss 1.22612286\n",
      "Trained batch 1280 batch loss 1.23562527 epoch total loss 1.22613025\n",
      "Trained batch 1281 batch loss 1.28572452 epoch total loss 1.22617674\n",
      "Trained batch 1282 batch loss 1.32749486 epoch total loss 1.22625577\n",
      "Trained batch 1283 batch loss 1.436746 epoch total loss 1.22641993\n",
      "Trained batch 1284 batch loss 1.40065551 epoch total loss 1.22655559\n",
      "Trained batch 1285 batch loss 1.29877687 epoch total loss 1.22661185\n",
      "Trained batch 1286 batch loss 1.23478472 epoch total loss 1.22661817\n",
      "Trained batch 1287 batch loss 1.22950768 epoch total loss 1.22662032\n",
      "Trained batch 1288 batch loss 1.16491532 epoch total loss 1.22657239\n",
      "Trained batch 1289 batch loss 1.31668568 epoch total loss 1.22664237\n",
      "Trained batch 1290 batch loss 1.15905297 epoch total loss 1.22658992\n",
      "Trained batch 1291 batch loss 1.23394012 epoch total loss 1.22659564\n",
      "Trained batch 1292 batch loss 1.33991086 epoch total loss 1.22668338\n",
      "Trained batch 1293 batch loss 1.1709044 epoch total loss 1.22664022\n",
      "Trained batch 1294 batch loss 1.18564403 epoch total loss 1.22660851\n",
      "Trained batch 1295 batch loss 1.11885977 epoch total loss 1.22652531\n",
      "Trained batch 1296 batch loss 1.05814362 epoch total loss 1.22639537\n",
      "Trained batch 1297 batch loss 1.13255894 epoch total loss 1.22632301\n",
      "Trained batch 1298 batch loss 1.18580568 epoch total loss 1.22629189\n",
      "Trained batch 1299 batch loss 1.27523947 epoch total loss 1.22632957\n",
      "Trained batch 1300 batch loss 1.43442667 epoch total loss 1.22648966\n",
      "Trained batch 1301 batch loss 1.33192718 epoch total loss 1.22657061\n",
      "Trained batch 1302 batch loss 1.25162792 epoch total loss 1.22658992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1303 batch loss 1.20821095 epoch total loss 1.22657585\n",
      "Trained batch 1304 batch loss 1.24980593 epoch total loss 1.22659361\n",
      "Trained batch 1305 batch loss 1.32127392 epoch total loss 1.22666609\n",
      "Trained batch 1306 batch loss 1.26539063 epoch total loss 1.22669578\n",
      "Trained batch 1307 batch loss 1.3546263 epoch total loss 1.22679365\n",
      "Trained batch 1308 batch loss 1.29526699 epoch total loss 1.22684598\n",
      "Trained batch 1309 batch loss 1.24160802 epoch total loss 1.2268573\n",
      "Trained batch 1310 batch loss 1.17247629 epoch total loss 1.22681582\n",
      "Trained batch 1311 batch loss 1.19960225 epoch total loss 1.22679496\n",
      "Trained batch 1312 batch loss 1.30597019 epoch total loss 1.2268554\n",
      "Trained batch 1313 batch loss 1.30199718 epoch total loss 1.22691262\n",
      "Trained batch 1314 batch loss 1.38659716 epoch total loss 1.22703421\n",
      "Trained batch 1315 batch loss 1.46349 epoch total loss 1.22721398\n",
      "Trained batch 1316 batch loss 1.24746132 epoch total loss 1.22722936\n",
      "Trained batch 1317 batch loss 1.28884792 epoch total loss 1.22727609\n",
      "Trained batch 1318 batch loss 1.32148921 epoch total loss 1.22734761\n",
      "Trained batch 1319 batch loss 1.35462701 epoch total loss 1.22744405\n",
      "Trained batch 1320 batch loss 1.34766209 epoch total loss 1.22753513\n",
      "Trained batch 1321 batch loss 1.32278192 epoch total loss 1.22760725\n",
      "Trained batch 1322 batch loss 1.12821686 epoch total loss 1.22753203\n",
      "Trained batch 1323 batch loss 1.14530265 epoch total loss 1.22746992\n",
      "Trained batch 1324 batch loss 1.18169308 epoch total loss 1.22743523\n",
      "Trained batch 1325 batch loss 1.20496833 epoch total loss 1.2274183\n",
      "Trained batch 1326 batch loss 1.19065809 epoch total loss 1.22739053\n",
      "Trained batch 1327 batch loss 1.10151196 epoch total loss 1.22729576\n",
      "Trained batch 1328 batch loss 1.13896418 epoch total loss 1.22722924\n",
      "Trained batch 1329 batch loss 1.2098316 epoch total loss 1.22721612\n",
      "Trained batch 1330 batch loss 1.26851749 epoch total loss 1.22724724\n",
      "Trained batch 1331 batch loss 1.21107411 epoch total loss 1.22723508\n",
      "Trained batch 1332 batch loss 1.23428369 epoch total loss 1.22724032\n",
      "Trained batch 1333 batch loss 1.16775608 epoch total loss 1.22719562\n",
      "Trained batch 1334 batch loss 1.2186954 epoch total loss 1.2271893\n",
      "Trained batch 1335 batch loss 1.34532928 epoch total loss 1.22727787\n",
      "Trained batch 1336 batch loss 1.31187916 epoch total loss 1.22734118\n",
      "Trained batch 1337 batch loss 1.1431762 epoch total loss 1.22727823\n",
      "Trained batch 1338 batch loss 1.27250695 epoch total loss 1.22731197\n",
      "Trained batch 1339 batch loss 1.23424363 epoch total loss 1.22731721\n",
      "Trained batch 1340 batch loss 1.13489377 epoch total loss 1.22724819\n",
      "Trained batch 1341 batch loss 1.06643975 epoch total loss 1.22712827\n",
      "Trained batch 1342 batch loss 1.19841373 epoch total loss 1.22710681\n",
      "Trained batch 1343 batch loss 1.2413367 epoch total loss 1.22711742\n",
      "Trained batch 1344 batch loss 1.18351662 epoch total loss 1.22708488\n",
      "Trained batch 1345 batch loss 1.26661766 epoch total loss 1.22711432\n",
      "Trained batch 1346 batch loss 1.29368114 epoch total loss 1.22716379\n",
      "Trained batch 1347 batch loss 1.14148831 epoch total loss 1.22710013\n",
      "Trained batch 1348 batch loss 1.11254609 epoch total loss 1.22701514\n",
      "Trained batch 1349 batch loss 0.986701727 epoch total loss 1.22683704\n",
      "Trained batch 1350 batch loss 1.12002861 epoch total loss 1.22675788\n",
      "Trained batch 1351 batch loss 1.54483497 epoch total loss 1.22699332\n",
      "Trained batch 1352 batch loss 1.36554694 epoch total loss 1.22709584\n",
      "Trained batch 1353 batch loss 1.16027594 epoch total loss 1.22704649\n",
      "Trained batch 1354 batch loss 1.14856791 epoch total loss 1.22698843\n",
      "Trained batch 1355 batch loss 1.20928204 epoch total loss 1.22697532\n",
      "Trained batch 1356 batch loss 1.18209314 epoch total loss 1.2269423\n",
      "Trained batch 1357 batch loss 1.1048162 epoch total loss 1.2268523\n",
      "Trained batch 1358 batch loss 1.29842842 epoch total loss 1.22690511\n",
      "Trained batch 1359 batch loss 1.18419683 epoch total loss 1.22687364\n",
      "Trained batch 1360 batch loss 1.14092326 epoch total loss 1.22681046\n",
      "Trained batch 1361 batch loss 1.27408576 epoch total loss 1.22684515\n",
      "Trained batch 1362 batch loss 1.22612381 epoch total loss 1.22684455\n",
      "Trained batch 1363 batch loss 1.19276011 epoch total loss 1.22681952\n",
      "Trained batch 1364 batch loss 1.12369227 epoch total loss 1.22674394\n",
      "Trained batch 1365 batch loss 1.12180185 epoch total loss 1.22666705\n",
      "Trained batch 1366 batch loss 1.18167329 epoch total loss 1.22663403\n",
      "Trained batch 1367 batch loss 1.18028069 epoch total loss 1.22660017\n",
      "Trained batch 1368 batch loss 1.27843189 epoch total loss 1.22663808\n",
      "Trained batch 1369 batch loss 1.13820982 epoch total loss 1.22657347\n",
      "Trained batch 1370 batch loss 1.13564408 epoch total loss 1.22650707\n",
      "Trained batch 1371 batch loss 1.17048907 epoch total loss 1.2264663\n",
      "Trained batch 1372 batch loss 1.20601606 epoch total loss 1.2264514\n",
      "Trained batch 1373 batch loss 1.18877542 epoch total loss 1.22642386\n",
      "Trained batch 1374 batch loss 1.24099731 epoch total loss 1.22643447\n",
      "Trained batch 1375 batch loss 1.28580856 epoch total loss 1.22647762\n",
      "Trained batch 1376 batch loss 1.26061 epoch total loss 1.22650242\n",
      "Trained batch 1377 batch loss 1.26157653 epoch total loss 1.22652793\n",
      "Trained batch 1378 batch loss 1.2683121 epoch total loss 1.22655821\n",
      "Trained batch 1379 batch loss 1.38613772 epoch total loss 1.22667396\n",
      "Trained batch 1380 batch loss 1.3871969 epoch total loss 1.22679031\n",
      "Trained batch 1381 batch loss 1.26994419 epoch total loss 1.22682154\n",
      "Trained batch 1382 batch loss 1.17098701 epoch total loss 1.22678113\n",
      "Trained batch 1383 batch loss 1.21663094 epoch total loss 1.22677386\n",
      "Trained batch 1384 batch loss 1.08860612 epoch total loss 1.22667396\n",
      "Trained batch 1385 batch loss 1.1017561 epoch total loss 1.22658384\n",
      "Trained batch 1386 batch loss 1.04447067 epoch total loss 1.22645235\n",
      "Trained batch 1387 batch loss 1.24681485 epoch total loss 1.22646713\n",
      "Trained batch 1388 batch loss 1.16413355 epoch total loss 1.22642219\n",
      "Epoch 4 train loss 1.2264221906661987\n",
      "Validated batch 1 batch loss 1.12664723\n",
      "Validated batch 2 batch loss 1.1902777\n",
      "Validated batch 3 batch loss 1.20821607\n",
      "Validated batch 4 batch loss 1.15497077\n",
      "Validated batch 5 batch loss 1.29294121\n",
      "Validated batch 6 batch loss 1.33195496\n",
      "Validated batch 7 batch loss 1.06712723\n",
      "Validated batch 8 batch loss 1.21933138\n",
      "Validated batch 9 batch loss 1.19631624\n",
      "Validated batch 10 batch loss 1.2937088\n",
      "Validated batch 11 batch loss 1.21922064\n",
      "Validated batch 12 batch loss 1.08802319\n",
      "Validated batch 13 batch loss 1.12295461\n",
      "Validated batch 14 batch loss 1.16253293\n",
      "Validated batch 15 batch loss 1.14190209\n",
      "Validated batch 16 batch loss 1.22790062\n",
      "Validated batch 17 batch loss 1.17414546\n",
      "Validated batch 18 batch loss 1.11995101\n",
      "Validated batch 19 batch loss 1.25318551\n",
      "Validated batch 20 batch loss 1.27104473\n",
      "Validated batch 21 batch loss 1.27602017\n",
      "Validated batch 22 batch loss 1.20248556\n",
      "Validated batch 23 batch loss 1.11323714\n",
      "Validated batch 24 batch loss 1.27194238\n",
      "Validated batch 25 batch loss 1.26561749\n",
      "Validated batch 26 batch loss 1.11876678\n",
      "Validated batch 27 batch loss 1.14094806\n",
      "Validated batch 28 batch loss 1.12092161\n",
      "Validated batch 29 batch loss 1.23624611\n",
      "Validated batch 30 batch loss 1.24969542\n",
      "Validated batch 31 batch loss 1.0810461\n",
      "Validated batch 32 batch loss 1.2230109\n",
      "Validated batch 33 batch loss 1.16451478\n",
      "Validated batch 34 batch loss 1.22578704\n",
      "Validated batch 35 batch loss 1.2171\n",
      "Validated batch 36 batch loss 1.2387867\n",
      "Validated batch 37 batch loss 1.14468026\n",
      "Validated batch 38 batch loss 1.14718008\n",
      "Validated batch 39 batch loss 1.20890319\n",
      "Validated batch 40 batch loss 1.20183814\n",
      "Validated batch 41 batch loss 1.23022771\n",
      "Validated batch 42 batch loss 1.28039575\n",
      "Validated batch 43 batch loss 1.46636879\n",
      "Validated batch 44 batch loss 1.24667454\n",
      "Validated batch 45 batch loss 1.22014892\n",
      "Validated batch 46 batch loss 1.10199642\n",
      "Validated batch 47 batch loss 1.13576257\n",
      "Validated batch 48 batch loss 1.22716105\n",
      "Validated batch 49 batch loss 1.13333821\n",
      "Validated batch 50 batch loss 1.19509792\n",
      "Validated batch 51 batch loss 1.18816483\n",
      "Validated batch 52 batch loss 1.22295523\n",
      "Validated batch 53 batch loss 1.26019871\n",
      "Validated batch 54 batch loss 1.26459408\n",
      "Validated batch 55 batch loss 1.23574603\n",
      "Validated batch 56 batch loss 1.18579543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 57 batch loss 1.23791742\n",
      "Validated batch 58 batch loss 1.18681955\n",
      "Validated batch 59 batch loss 1.22635579\n",
      "Validated batch 60 batch loss 1.33178627\n",
      "Validated batch 61 batch loss 1.30468249\n",
      "Validated batch 62 batch loss 1.21441329\n",
      "Validated batch 63 batch loss 1.41246879\n",
      "Validated batch 64 batch loss 1.05808568\n",
      "Validated batch 65 batch loss 1.28041804\n",
      "Validated batch 66 batch loss 1.03699803\n",
      "Validated batch 67 batch loss 1.1860249\n",
      "Validated batch 68 batch loss 1.3063947\n",
      "Validated batch 69 batch loss 1.14084744\n",
      "Validated batch 70 batch loss 1.14204609\n",
      "Validated batch 71 batch loss 1.06869602\n",
      "Validated batch 72 batch loss 1.19656157\n",
      "Validated batch 73 batch loss 1.15313268\n",
      "Validated batch 74 batch loss 1.14469063\n",
      "Validated batch 75 batch loss 1.23614156\n",
      "Validated batch 76 batch loss 1.20319939\n",
      "Validated batch 77 batch loss 1.11905777\n",
      "Validated batch 78 batch loss 1.23540449\n",
      "Validated batch 79 batch loss 1.12603307\n",
      "Validated batch 80 batch loss 1.22600698\n",
      "Validated batch 81 batch loss 1.21973324\n",
      "Validated batch 82 batch loss 1.14152098\n",
      "Validated batch 83 batch loss 1.14886212\n",
      "Validated batch 84 batch loss 1.21305585\n",
      "Validated batch 85 batch loss 1.15497077\n",
      "Validated batch 86 batch loss 1.36731064\n",
      "Validated batch 87 batch loss 1.21501327\n",
      "Validated batch 88 batch loss 1.10997856\n",
      "Validated batch 89 batch loss 1.26577139\n",
      "Validated batch 90 batch loss 1.20388603\n",
      "Validated batch 91 batch loss 1.12777328\n",
      "Validated batch 92 batch loss 1.20333815\n",
      "Validated batch 93 batch loss 1.20938337\n",
      "Validated batch 94 batch loss 1.25657356\n",
      "Validated batch 95 batch loss 1.11054504\n",
      "Validated batch 96 batch loss 1.19241273\n",
      "Validated batch 97 batch loss 1.20346808\n",
      "Validated batch 98 batch loss 1.17930639\n",
      "Validated batch 99 batch loss 1.20975184\n",
      "Validated batch 100 batch loss 1.20430839\n",
      "Validated batch 101 batch loss 1.1471051\n",
      "Validated batch 102 batch loss 1.30975199\n",
      "Validated batch 103 batch loss 1.13707852\n",
      "Validated batch 104 batch loss 1.06753314\n",
      "Validated batch 105 batch loss 1.17812157\n",
      "Validated batch 106 batch loss 1.27533233\n",
      "Validated batch 107 batch loss 1.31142688\n",
      "Validated batch 108 batch loss 1.34939611\n",
      "Validated batch 109 batch loss 1.16333401\n",
      "Validated batch 110 batch loss 1.33125424\n",
      "Validated batch 111 batch loss 1.20534921\n",
      "Validated batch 112 batch loss 1.29358339\n",
      "Validated batch 113 batch loss 1.27509046\n",
      "Validated batch 114 batch loss 0.980494857\n",
      "Validated batch 115 batch loss 1.19403493\n",
      "Validated batch 116 batch loss 1.23459494\n",
      "Validated batch 117 batch loss 1.2181201\n",
      "Validated batch 118 batch loss 1.18384016\n",
      "Validated batch 119 batch loss 1.1964047\n",
      "Validated batch 120 batch loss 1.25307441\n",
      "Validated batch 121 batch loss 1.35202634\n",
      "Validated batch 122 batch loss 1.16165876\n",
      "Validated batch 123 batch loss 1.24193144\n",
      "Validated batch 124 batch loss 1.15478802\n",
      "Validated batch 125 batch loss 1.24135721\n",
      "Validated batch 126 batch loss 1.21980977\n",
      "Validated batch 127 batch loss 1.14766288\n",
      "Validated batch 128 batch loss 1.29290831\n",
      "Validated batch 129 batch loss 1.23751235\n",
      "Validated batch 130 batch loss 1.24871528\n",
      "Validated batch 131 batch loss 1.19323492\n",
      "Validated batch 132 batch loss 1.27130508\n",
      "Validated batch 133 batch loss 1.1522913\n",
      "Validated batch 134 batch loss 1.15027404\n",
      "Validated batch 135 batch loss 1.21290731\n",
      "Validated batch 136 batch loss 1.14165056\n",
      "Validated batch 137 batch loss 1.21633351\n",
      "Validated batch 138 batch loss 1.18380666\n",
      "Validated batch 139 batch loss 1.21203589\n",
      "Validated batch 140 batch loss 1.20377648\n",
      "Validated batch 141 batch loss 1.19950187\n",
      "Validated batch 142 batch loss 1.09911358\n",
      "Validated batch 143 batch loss 1.14607692\n",
      "Validated batch 144 batch loss 1.30679023\n",
      "Validated batch 145 batch loss 1.09841967\n",
      "Validated batch 146 batch loss 1.07860136\n",
      "Validated batch 147 batch loss 1.12510777\n",
      "Validated batch 148 batch loss 1.19683063\n",
      "Validated batch 149 batch loss 1.04617155\n",
      "Validated batch 150 batch loss 1.21577358\n",
      "Validated batch 151 batch loss 1.09399104\n",
      "Validated batch 152 batch loss 1.19481552\n",
      "Validated batch 153 batch loss 1.27632725\n",
      "Validated batch 154 batch loss 1.28917646\n",
      "Validated batch 155 batch loss 1.12388074\n",
      "Validated batch 156 batch loss 1.35133457\n",
      "Validated batch 157 batch loss 1.00211513\n",
      "Validated batch 158 batch loss 1.07357454\n",
      "Validated batch 159 batch loss 1.16796327\n",
      "Validated batch 160 batch loss 1.16913688\n",
      "Validated batch 161 batch loss 1.30567944\n",
      "Validated batch 162 batch loss 1.26737571\n",
      "Validated batch 163 batch loss 1.12529612\n",
      "Validated batch 164 batch loss 1.17170513\n",
      "Validated batch 165 batch loss 1.12843406\n",
      "Validated batch 166 batch loss 1.17900181\n",
      "Validated batch 167 batch loss 1.25019979\n",
      "Validated batch 168 batch loss 1.21050251\n",
      "Validated batch 169 batch loss 1.25899291\n",
      "Validated batch 170 batch loss 1.30243015\n",
      "Validated batch 171 batch loss 1.26505804\n",
      "Validated batch 172 batch loss 1.19358039\n",
      "Validated batch 173 batch loss 1.35997391\n",
      "Validated batch 174 batch loss 1.23462856\n",
      "Validated batch 175 batch loss 1.28677654\n",
      "Validated batch 176 batch loss 1.27111387\n",
      "Validated batch 177 batch loss 1.3535881\n",
      "Validated batch 178 batch loss 1.28119087\n",
      "Validated batch 179 batch loss 1.15584815\n",
      "Validated batch 180 batch loss 1.15592742\n",
      "Validated batch 181 batch loss 1.27583146\n",
      "Validated batch 182 batch loss 1.30647826\n",
      "Validated batch 183 batch loss 1.13543713\n",
      "Validated batch 184 batch loss 1.22814095\n",
      "Validated batch 185 batch loss 1.15240765\n",
      "Epoch 4 val loss 1.2036110162734985\n",
      "Model /aiffel/aiffel/mpii/a/model-epoch-4-loss-1.2036.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.12453365 epoch total loss 1.12453365\n",
      "Trained batch 2 batch loss 1.27259874 epoch total loss 1.1985662\n",
      "Trained batch 3 batch loss 1.24996328 epoch total loss 1.2156986\n",
      "Trained batch 4 batch loss 1.30515122 epoch total loss 1.23806167\n",
      "Trained batch 5 batch loss 1.37735856 epoch total loss 1.265921\n",
      "Trained batch 6 batch loss 1.42692232 epoch total loss 1.29275453\n",
      "Trained batch 7 batch loss 1.45826268 epoch total loss 1.31639862\n",
      "Trained batch 8 batch loss 1.24300671 epoch total loss 1.30722463\n",
      "Trained batch 9 batch loss 1.08036315 epoch total loss 1.28201783\n",
      "Trained batch 10 batch loss 1.08809698 epoch total loss 1.26262569\n",
      "Trained batch 11 batch loss 1.08145106 epoch total loss 1.24615526\n",
      "Trained batch 12 batch loss 1.09370744 epoch total loss 1.23345125\n",
      "Trained batch 13 batch loss 1.12825465 epoch total loss 1.2253592\n",
      "Trained batch 14 batch loss 1.00826538 epoch total loss 1.20985246\n",
      "Trained batch 15 batch loss 1.09658027 epoch total loss 1.20230103\n",
      "Trained batch 16 batch loss 0.951828301 epoch total loss 1.18664646\n",
      "Trained batch 17 batch loss 1.01743388 epoch total loss 1.17669272\n",
      "Trained batch 18 batch loss 1.1246717 epoch total loss 1.17380273\n",
      "Trained batch 19 batch loss 1.26993823 epoch total loss 1.17886245\n",
      "Trained batch 20 batch loss 1.01685858 epoch total loss 1.1707623\n",
      "Trained batch 21 batch loss 1.02783763 epoch total loss 1.16395628\n",
      "Trained batch 22 batch loss 1.05541837 epoch total loss 1.15902281\n",
      "Trained batch 23 batch loss 1.18683052 epoch total loss 1.16023183\n",
      "Trained batch 24 batch loss 1.04428077 epoch total loss 1.15540051\n",
      "Trained batch 25 batch loss 1.16148186 epoch total loss 1.15564382\n",
      "Trained batch 26 batch loss 1.20149302 epoch total loss 1.15740716\n",
      "Trained batch 27 batch loss 1.13704705 epoch total loss 1.15665305\n",
      "Trained batch 28 batch loss 1.04500592 epoch total loss 1.15266573\n",
      "Trained batch 29 batch loss 1.15065563 epoch total loss 1.15259635\n",
      "Trained batch 30 batch loss 1.13576424 epoch total loss 1.15203524\n",
      "Trained batch 31 batch loss 1.18821907 epoch total loss 1.15320241\n",
      "Trained batch 32 batch loss 1.16755748 epoch total loss 1.153651\n",
      "Trained batch 33 batch loss 1.10517812 epoch total loss 1.1521821\n",
      "Trained batch 34 batch loss 1.08647919 epoch total loss 1.15024972\n",
      "Trained batch 35 batch loss 1.17799783 epoch total loss 1.15104246\n",
      "Trained batch 36 batch loss 1.1951375 epoch total loss 1.15226734\n",
      "Trained batch 37 batch loss 1.16194081 epoch total loss 1.15252876\n",
      "Trained batch 38 batch loss 1.20137274 epoch total loss 1.1538142\n",
      "Trained batch 39 batch loss 1.16672957 epoch total loss 1.15414536\n",
      "Trained batch 40 batch loss 1.00323308 epoch total loss 1.15037262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 41 batch loss 1.08725572 epoch total loss 1.14883316\n",
      "Trained batch 42 batch loss 1.10529613 epoch total loss 1.14779663\n",
      "Trained batch 43 batch loss 1.1043961 epoch total loss 1.14678729\n",
      "Trained batch 44 batch loss 1.25496125 epoch total loss 1.14924586\n",
      "Trained batch 45 batch loss 1.20994246 epoch total loss 1.15059471\n",
      "Trained batch 46 batch loss 1.18559468 epoch total loss 1.15135562\n",
      "Trained batch 47 batch loss 1.32161152 epoch total loss 1.15497804\n",
      "Trained batch 48 batch loss 1.09117365 epoch total loss 1.15364885\n",
      "Trained batch 49 batch loss 1.15789104 epoch total loss 1.1537354\n",
      "Trained batch 50 batch loss 1.11540985 epoch total loss 1.15296888\n",
      "Trained batch 51 batch loss 1.20392442 epoch total loss 1.1539681\n",
      "Trained batch 52 batch loss 1.18872774 epoch total loss 1.1546365\n",
      "Trained batch 53 batch loss 1.16682541 epoch total loss 1.15486646\n",
      "Trained batch 54 batch loss 1.14387548 epoch total loss 1.15466297\n",
      "Trained batch 55 batch loss 1.22557473 epoch total loss 1.15595222\n",
      "Trained batch 56 batch loss 1.19175601 epoch total loss 1.15659153\n",
      "Trained batch 57 batch loss 1.2352953 epoch total loss 1.15797234\n",
      "Trained batch 58 batch loss 1.20104909 epoch total loss 1.15871513\n",
      "Trained batch 59 batch loss 1.20679283 epoch total loss 1.15953\n",
      "Trained batch 60 batch loss 1.21886706 epoch total loss 1.16051888\n",
      "Trained batch 61 batch loss 1.29298377 epoch total loss 1.1626904\n",
      "Trained batch 62 batch loss 1.12724388 epoch total loss 1.16211867\n",
      "Trained batch 63 batch loss 0.994901538 epoch total loss 1.15946448\n",
      "Trained batch 64 batch loss 1.18262041 epoch total loss 1.15982628\n",
      "Trained batch 65 batch loss 1.23775184 epoch total loss 1.16102517\n",
      "Trained batch 66 batch loss 1.17199826 epoch total loss 1.16119146\n",
      "Trained batch 67 batch loss 1.09322095 epoch total loss 1.16017699\n",
      "Trained batch 68 batch loss 1.06555164 epoch total loss 1.15878546\n",
      "Trained batch 69 batch loss 0.969549537 epoch total loss 1.15604293\n",
      "Trained batch 70 batch loss 1.07776248 epoch total loss 1.15492451\n",
      "Trained batch 71 batch loss 1.27673233 epoch total loss 1.15664017\n",
      "Trained batch 72 batch loss 1.17071533 epoch total loss 1.15683568\n",
      "Trained batch 73 batch loss 1.19636846 epoch total loss 1.15737712\n",
      "Trained batch 74 batch loss 1.37508082 epoch total loss 1.16031909\n",
      "Trained batch 75 batch loss 1.16319108 epoch total loss 1.16035748\n",
      "Trained batch 76 batch loss 1.12410402 epoch total loss 1.15988052\n",
      "Trained batch 77 batch loss 1.1642406 epoch total loss 1.15993702\n",
      "Trained batch 78 batch loss 1.15489292 epoch total loss 1.15987241\n",
      "Trained batch 79 batch loss 1.16726255 epoch total loss 1.15996587\n",
      "Trained batch 80 batch loss 1.16702616 epoch total loss 1.16005409\n",
      "Trained batch 81 batch loss 0.93535 epoch total loss 1.15728\n",
      "Trained batch 82 batch loss 0.955607831 epoch total loss 1.15482056\n",
      "Trained batch 83 batch loss 1.11443114 epoch total loss 1.15433395\n",
      "Trained batch 84 batch loss 1.29783332 epoch total loss 1.15604234\n",
      "Trained batch 85 batch loss 1.42921197 epoch total loss 1.1592561\n",
      "Trained batch 86 batch loss 1.35805571 epoch total loss 1.16156769\n",
      "Trained batch 87 batch loss 1.2828033 epoch total loss 1.16296124\n",
      "Trained batch 88 batch loss 1.38472223 epoch total loss 1.16548121\n",
      "Trained batch 89 batch loss 1.18104839 epoch total loss 1.16565609\n",
      "Trained batch 90 batch loss 1.37060642 epoch total loss 1.16793334\n",
      "Trained batch 91 batch loss 1.13223946 epoch total loss 1.16754103\n",
      "Trained batch 92 batch loss 1.1712811 epoch total loss 1.16758168\n",
      "Trained batch 93 batch loss 1.19204152 epoch total loss 1.16784465\n",
      "Trained batch 94 batch loss 1.21968198 epoch total loss 1.16839612\n",
      "Trained batch 95 batch loss 1.03781092 epoch total loss 1.16702163\n",
      "Trained batch 96 batch loss 1.00538898 epoch total loss 1.16533792\n",
      "Trained batch 97 batch loss 1.04693341 epoch total loss 1.16411722\n",
      "Trained batch 98 batch loss 1.17760146 epoch total loss 1.1642549\n",
      "Trained batch 99 batch loss 1.02385557 epoch total loss 1.16283667\n",
      "Trained batch 100 batch loss 1.07286346 epoch total loss 1.161937\n",
      "Trained batch 101 batch loss 1.17105496 epoch total loss 1.16202724\n",
      "Trained batch 102 batch loss 1.2141118 epoch total loss 1.16253793\n",
      "Trained batch 103 batch loss 1.15372276 epoch total loss 1.16245234\n",
      "Trained batch 104 batch loss 1.13231373 epoch total loss 1.16216254\n",
      "Trained batch 105 batch loss 1.07580733 epoch total loss 1.16134012\n",
      "Trained batch 106 batch loss 1.29611671 epoch total loss 1.1626116\n",
      "Trained batch 107 batch loss 1.28885198 epoch total loss 1.16379142\n",
      "Trained batch 108 batch loss 1.210747 epoch total loss 1.16422617\n",
      "Trained batch 109 batch loss 1.31766832 epoch total loss 1.16563392\n",
      "Trained batch 110 batch loss 1.26263118 epoch total loss 1.16651571\n",
      "Trained batch 111 batch loss 1.21560454 epoch total loss 1.16695797\n",
      "Trained batch 112 batch loss 1.2303288 epoch total loss 1.16752374\n",
      "Trained batch 113 batch loss 1.17920756 epoch total loss 1.16762722\n",
      "Trained batch 114 batch loss 1.17336845 epoch total loss 1.16767764\n",
      "Trained batch 115 batch loss 1.26107609 epoch total loss 1.16848981\n",
      "Trained batch 116 batch loss 1.22963989 epoch total loss 1.16901696\n",
      "Trained batch 117 batch loss 1.26635933 epoch total loss 1.16984892\n",
      "Trained batch 118 batch loss 1.34870219 epoch total loss 1.17136478\n",
      "Trained batch 119 batch loss 1.24392807 epoch total loss 1.17197454\n",
      "Trained batch 120 batch loss 1.36247015 epoch total loss 1.17356193\n",
      "Trained batch 121 batch loss 1.1909399 epoch total loss 1.17370558\n",
      "Trained batch 122 batch loss 1.18419552 epoch total loss 1.17379153\n",
      "Trained batch 123 batch loss 1.29027247 epoch total loss 1.17473841\n",
      "Trained batch 124 batch loss 1.17951751 epoch total loss 1.17477703\n",
      "Trained batch 125 batch loss 1.20874679 epoch total loss 1.17504871\n",
      "Trained batch 126 batch loss 1.19362783 epoch total loss 1.17519617\n",
      "Trained batch 127 batch loss 1.27505517 epoch total loss 1.17598248\n",
      "Trained batch 128 batch loss 1.2272861 epoch total loss 1.17638326\n",
      "Trained batch 129 batch loss 1.16130912 epoch total loss 1.17626643\n",
      "Trained batch 130 batch loss 1.32272232 epoch total loss 1.17739308\n",
      "Trained batch 131 batch loss 1.36503673 epoch total loss 1.17882538\n",
      "Trained batch 132 batch loss 1.27696347 epoch total loss 1.17956889\n",
      "Trained batch 133 batch loss 1.2595166 epoch total loss 1.18017\n",
      "Trained batch 134 batch loss 1.24789238 epoch total loss 1.18067551\n",
      "Trained batch 135 batch loss 1.29572845 epoch total loss 1.18152773\n",
      "Trained batch 136 batch loss 1.24155068 epoch total loss 1.18196905\n",
      "Trained batch 137 batch loss 1.18203902 epoch total loss 1.18196952\n",
      "Trained batch 138 batch loss 1.19116223 epoch total loss 1.18203616\n",
      "Trained batch 139 batch loss 1.10022974 epoch total loss 1.18144763\n",
      "Trained batch 140 batch loss 1.0318259 epoch total loss 1.18037891\n",
      "Trained batch 141 batch loss 1.05493641 epoch total loss 1.17948925\n",
      "Trained batch 142 batch loss 1.1230123 epoch total loss 1.17909157\n",
      "Trained batch 143 batch loss 0.995657921 epoch total loss 1.17780876\n",
      "Trained batch 144 batch loss 0.930113435 epoch total loss 1.17608869\n",
      "Trained batch 145 batch loss 0.947527647 epoch total loss 1.17451239\n",
      "Trained batch 146 batch loss 0.883317709 epoch total loss 1.1725179\n",
      "Trained batch 147 batch loss 1.07466435 epoch total loss 1.17185211\n",
      "Trained batch 148 batch loss 1.11820841 epoch total loss 1.17148972\n",
      "Trained batch 149 batch loss 1.12834239 epoch total loss 1.17120016\n",
      "Trained batch 150 batch loss 1.18392408 epoch total loss 1.17128503\n",
      "Trained batch 151 batch loss 1.2825892 epoch total loss 1.1720221\n",
      "Trained batch 152 batch loss 1.16732669 epoch total loss 1.17199123\n",
      "Trained batch 153 batch loss 1.34256244 epoch total loss 1.17310607\n",
      "Trained batch 154 batch loss 1.04043794 epoch total loss 1.17224455\n",
      "Trained batch 155 batch loss 1.12533116 epoch total loss 1.171942\n",
      "Trained batch 156 batch loss 0.965560496 epoch total loss 1.17061901\n",
      "Trained batch 157 batch loss 1.00397027 epoch total loss 1.16955745\n",
      "Trained batch 158 batch loss 1.13721442 epoch total loss 1.16935277\n",
      "Trained batch 159 batch loss 1.18257 epoch total loss 1.16943586\n",
      "Trained batch 160 batch loss 1.10343575 epoch total loss 1.16902339\n",
      "Trained batch 161 batch loss 1.19742179 epoch total loss 1.16919982\n",
      "Trained batch 162 batch loss 1.3156687 epoch total loss 1.17010391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 163 batch loss 1.26791167 epoch total loss 1.17070401\n",
      "Trained batch 164 batch loss 1.36444545 epoch total loss 1.17188537\n",
      "Trained batch 165 batch loss 1.19011235 epoch total loss 1.17199576\n",
      "Trained batch 166 batch loss 1.1037457 epoch total loss 1.17158461\n",
      "Trained batch 167 batch loss 1.1113565 epoch total loss 1.171224\n",
      "Trained batch 168 batch loss 1.19087267 epoch total loss 1.17134094\n",
      "Trained batch 169 batch loss 1.259547 epoch total loss 1.17186296\n",
      "Trained batch 170 batch loss 1.3300755 epoch total loss 1.17279363\n",
      "Trained batch 171 batch loss 1.25638056 epoch total loss 1.17328238\n",
      "Trained batch 172 batch loss 1.24672294 epoch total loss 1.17370939\n",
      "Trained batch 173 batch loss 1.12053561 epoch total loss 1.17340195\n",
      "Trained batch 174 batch loss 1.08246267 epoch total loss 1.17287922\n",
      "Trained batch 175 batch loss 1.18847847 epoch total loss 1.17296839\n",
      "Trained batch 176 batch loss 1.23315096 epoch total loss 1.1733104\n",
      "Trained batch 177 batch loss 1.25648475 epoch total loss 1.17378032\n",
      "Trained batch 178 batch loss 1.19097817 epoch total loss 1.17387688\n",
      "Trained batch 179 batch loss 1.24878693 epoch total loss 1.17429543\n",
      "Trained batch 180 batch loss 1.23896468 epoch total loss 1.17465472\n",
      "Trained batch 181 batch loss 1.32473183 epoch total loss 1.17548394\n",
      "Trained batch 182 batch loss 1.26788747 epoch total loss 1.17599165\n",
      "Trained batch 183 batch loss 1.25941753 epoch total loss 1.17644751\n",
      "Trained batch 184 batch loss 1.25356007 epoch total loss 1.17686653\n",
      "Trained batch 185 batch loss 1.26289332 epoch total loss 1.17733157\n",
      "Trained batch 186 batch loss 1.22394633 epoch total loss 1.17758214\n",
      "Trained batch 187 batch loss 1.34233236 epoch total loss 1.17846322\n",
      "Trained batch 188 batch loss 1.23297143 epoch total loss 1.17875314\n",
      "Trained batch 189 batch loss 1.12989891 epoch total loss 1.17849469\n",
      "Trained batch 190 batch loss 1.27546477 epoch total loss 1.17900503\n",
      "Trained batch 191 batch loss 1.15738821 epoch total loss 1.1788919\n",
      "Trained batch 192 batch loss 1.32140422 epoch total loss 1.17963421\n",
      "Trained batch 193 batch loss 1.17433274 epoch total loss 1.17960668\n",
      "Trained batch 194 batch loss 1.0506016 epoch total loss 1.17894173\n",
      "Trained batch 195 batch loss 1.08555865 epoch total loss 1.17846286\n",
      "Trained batch 196 batch loss 1.12624872 epoch total loss 1.17819643\n",
      "Trained batch 197 batch loss 1.23942745 epoch total loss 1.17850721\n",
      "Trained batch 198 batch loss 1.30642629 epoch total loss 1.17915332\n",
      "Trained batch 199 batch loss 1.12263489 epoch total loss 1.17886925\n",
      "Trained batch 200 batch loss 1.11611438 epoch total loss 1.17855549\n",
      "Trained batch 201 batch loss 1.01260304 epoch total loss 1.17772985\n",
      "Trained batch 202 batch loss 1.13701618 epoch total loss 1.17752826\n",
      "Trained batch 203 batch loss 1.13938951 epoch total loss 1.17734039\n",
      "Trained batch 204 batch loss 1.27612329 epoch total loss 1.17782462\n",
      "Trained batch 205 batch loss 1.10488713 epoch total loss 1.1774689\n",
      "Trained batch 206 batch loss 1.28234124 epoch total loss 1.17797804\n",
      "Trained batch 207 batch loss 1.19106555 epoch total loss 1.17804122\n",
      "Trained batch 208 batch loss 1.14879072 epoch total loss 1.17790055\n",
      "Trained batch 209 batch loss 1.18907332 epoch total loss 1.17795408\n",
      "Trained batch 210 batch loss 1.23996615 epoch total loss 1.17824936\n",
      "Trained batch 211 batch loss 1.28414762 epoch total loss 1.17875123\n",
      "Trained batch 212 batch loss 1.27605689 epoch total loss 1.17921019\n",
      "Trained batch 213 batch loss 1.14401579 epoch total loss 1.17904496\n",
      "Trained batch 214 batch loss 1.01577306 epoch total loss 1.17828202\n",
      "Trained batch 215 batch loss 1.06610441 epoch total loss 1.17776024\n",
      "Trained batch 216 batch loss 1.2081778 epoch total loss 1.17790115\n",
      "Trained batch 217 batch loss 1.21026134 epoch total loss 1.17805028\n",
      "Trained batch 218 batch loss 1.27006197 epoch total loss 1.17847228\n",
      "Trained batch 219 batch loss 1.35964215 epoch total loss 1.17929959\n",
      "Trained batch 220 batch loss 1.32971692 epoch total loss 1.17998326\n",
      "Trained batch 221 batch loss 1.35642326 epoch total loss 1.1807816\n",
      "Trained batch 222 batch loss 1.2523396 epoch total loss 1.18110394\n",
      "Trained batch 223 batch loss 1.17252362 epoch total loss 1.18106544\n",
      "Trained batch 224 batch loss 1.25776839 epoch total loss 1.18140793\n",
      "Trained batch 225 batch loss 1.0839355 epoch total loss 1.18097472\n",
      "Trained batch 226 batch loss 1.24630797 epoch total loss 1.1812638\n",
      "Trained batch 227 batch loss 1.26365554 epoch total loss 1.18162668\n",
      "Trained batch 228 batch loss 1.19756079 epoch total loss 1.18169653\n",
      "Trained batch 229 batch loss 1.02725196 epoch total loss 1.18102217\n",
      "Trained batch 230 batch loss 1.01793373 epoch total loss 1.18031311\n",
      "Trained batch 231 batch loss 0.989918292 epoch total loss 1.1794889\n",
      "Trained batch 232 batch loss 1.16681159 epoch total loss 1.1794343\n",
      "Trained batch 233 batch loss 1.38845706 epoch total loss 1.18033135\n",
      "Trained batch 234 batch loss 1.34559703 epoch total loss 1.18103766\n",
      "Trained batch 235 batch loss 1.46408033 epoch total loss 1.18224216\n",
      "Trained batch 236 batch loss 1.35117006 epoch total loss 1.18295789\n",
      "Trained batch 237 batch loss 1.35124123 epoch total loss 1.1836679\n",
      "Trained batch 238 batch loss 1.4151206 epoch total loss 1.18464041\n",
      "Trained batch 239 batch loss 1.29248524 epoch total loss 1.18509161\n",
      "Trained batch 240 batch loss 1.31418502 epoch total loss 1.18562949\n",
      "Trained batch 241 batch loss 1.35688233 epoch total loss 1.18634009\n",
      "Trained batch 242 batch loss 1.2583437 epoch total loss 1.18663752\n",
      "Trained batch 243 batch loss 1.27277422 epoch total loss 1.18699205\n",
      "Trained batch 244 batch loss 1.31191683 epoch total loss 1.18750405\n",
      "Trained batch 245 batch loss 1.2251687 epoch total loss 1.18765771\n",
      "Trained batch 246 batch loss 1.14046896 epoch total loss 1.18746591\n",
      "Trained batch 247 batch loss 1.19729877 epoch total loss 1.18750572\n",
      "Trained batch 248 batch loss 1.24117541 epoch total loss 1.18772209\n",
      "Trained batch 249 batch loss 1.35200882 epoch total loss 1.18838191\n",
      "Trained batch 250 batch loss 1.2846514 epoch total loss 1.18876696\n",
      "Trained batch 251 batch loss 1.16205561 epoch total loss 1.1886605\n",
      "Trained batch 252 batch loss 1.22492218 epoch total loss 1.18880439\n",
      "Trained batch 253 batch loss 1.29643381 epoch total loss 1.18922985\n",
      "Trained batch 254 batch loss 1.23525524 epoch total loss 1.18941104\n",
      "Trained batch 255 batch loss 1.26837957 epoch total loss 1.18972075\n",
      "Trained batch 256 batch loss 1.34588218 epoch total loss 1.19033074\n",
      "Trained batch 257 batch loss 1.39650691 epoch total loss 1.19113302\n",
      "Trained batch 258 batch loss 1.11103976 epoch total loss 1.1908226\n",
      "Trained batch 259 batch loss 1.09799743 epoch total loss 1.19046426\n",
      "Trained batch 260 batch loss 1.1574111 epoch total loss 1.19033706\n",
      "Trained batch 261 batch loss 1.06735 epoch total loss 1.18986583\n",
      "Trained batch 262 batch loss 1.14208603 epoch total loss 1.18968356\n",
      "Trained batch 263 batch loss 1.19882166 epoch total loss 1.18971825\n",
      "Trained batch 264 batch loss 1.26704419 epoch total loss 1.19001126\n",
      "Trained batch 265 batch loss 1.36258101 epoch total loss 1.19066238\n",
      "Trained batch 266 batch loss 1.44311905 epoch total loss 1.19161153\n",
      "Trained batch 267 batch loss 1.2491554 epoch total loss 1.19182694\n",
      "Trained batch 268 batch loss 1.23248529 epoch total loss 1.19197869\n",
      "Trained batch 269 batch loss 1.23148906 epoch total loss 1.19212556\n",
      "Trained batch 270 batch loss 1.12737644 epoch total loss 1.19188571\n",
      "Trained batch 271 batch loss 1.20753706 epoch total loss 1.19194353\n",
      "Trained batch 272 batch loss 1.27743936 epoch total loss 1.19225788\n",
      "Trained batch 273 batch loss 1.21345448 epoch total loss 1.19233537\n",
      "Trained batch 274 batch loss 1.43656981 epoch total loss 1.19322681\n",
      "Trained batch 275 batch loss 1.25722408 epoch total loss 1.19345963\n",
      "Trained batch 276 batch loss 1.4226234 epoch total loss 1.19428992\n",
      "Trained batch 277 batch loss 1.40250587 epoch total loss 1.19504154\n",
      "Trained batch 278 batch loss 1.39244604 epoch total loss 1.19575167\n",
      "Trained batch 279 batch loss 1.36737156 epoch total loss 1.19636679\n",
      "Trained batch 280 batch loss 1.26408839 epoch total loss 1.19660878\n",
      "Trained batch 281 batch loss 1.32092464 epoch total loss 1.19705117\n",
      "Trained batch 282 batch loss 1.24504638 epoch total loss 1.1972214\n",
      "Trained batch 283 batch loss 1.33775043 epoch total loss 1.19771791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 284 batch loss 1.26547861 epoch total loss 1.19795644\n",
      "Trained batch 285 batch loss 1.20530486 epoch total loss 1.19798219\n",
      "Trained batch 286 batch loss 1.33842242 epoch total loss 1.19847322\n",
      "Trained batch 287 batch loss 1.26959538 epoch total loss 1.19872105\n",
      "Trained batch 288 batch loss 1.17352533 epoch total loss 1.19863355\n",
      "Trained batch 289 batch loss 1.21433365 epoch total loss 1.19868779\n",
      "Trained batch 290 batch loss 1.21495175 epoch total loss 1.19874394\n",
      "Trained batch 291 batch loss 1.12394691 epoch total loss 1.1984868\n",
      "Trained batch 292 batch loss 1.07867277 epoch total loss 1.19807649\n",
      "Trained batch 293 batch loss 1.15264702 epoch total loss 1.19792151\n",
      "Trained batch 294 batch loss 1.12139022 epoch total loss 1.19766116\n",
      "Trained batch 295 batch loss 1.33410585 epoch total loss 1.19812369\n",
      "Trained batch 296 batch loss 1.43085814 epoch total loss 1.19891\n",
      "Trained batch 297 batch loss 1.40437579 epoch total loss 1.19960177\n",
      "Trained batch 298 batch loss 1.28317225 epoch total loss 1.19988227\n",
      "Trained batch 299 batch loss 1.33771658 epoch total loss 1.20034325\n",
      "Trained batch 300 batch loss 1.0996387 epoch total loss 1.20000756\n",
      "Trained batch 301 batch loss 1.04173684 epoch total loss 1.19948173\n",
      "Trained batch 302 batch loss 1.00119758 epoch total loss 1.19882512\n",
      "Trained batch 303 batch loss 0.931675851 epoch total loss 1.19794345\n",
      "Trained batch 304 batch loss 1.10411644 epoch total loss 1.19763482\n",
      "Trained batch 305 batch loss 1.15729642 epoch total loss 1.19750261\n",
      "Trained batch 306 batch loss 1.29671621 epoch total loss 1.19782686\n",
      "Trained batch 307 batch loss 1.29476953 epoch total loss 1.19814253\n",
      "Trained batch 308 batch loss 1.22050929 epoch total loss 1.19821525\n",
      "Trained batch 309 batch loss 1.2254734 epoch total loss 1.19830346\n",
      "Trained batch 310 batch loss 1.23642182 epoch total loss 1.19842637\n",
      "Trained batch 311 batch loss 1.28792429 epoch total loss 1.19871414\n",
      "Trained batch 312 batch loss 1.27108693 epoch total loss 1.19894612\n",
      "Trained batch 313 batch loss 1.20317566 epoch total loss 1.19895971\n",
      "Trained batch 314 batch loss 1.2062583 epoch total loss 1.19898295\n",
      "Trained batch 315 batch loss 1.30277407 epoch total loss 1.19931245\n",
      "Trained batch 316 batch loss 1.21844149 epoch total loss 1.19937301\n",
      "Trained batch 317 batch loss 1.2278806 epoch total loss 1.19946289\n",
      "Trained batch 318 batch loss 1.27092052 epoch total loss 1.1996876\n",
      "Trained batch 319 batch loss 1.10586894 epoch total loss 1.19939351\n",
      "Trained batch 320 batch loss 1.10599577 epoch total loss 1.19910169\n",
      "Trained batch 321 batch loss 1.0090009 epoch total loss 1.19850945\n",
      "Trained batch 322 batch loss 1.14961982 epoch total loss 1.19835758\n",
      "Trained batch 323 batch loss 1.2343601 epoch total loss 1.19846916\n",
      "Trained batch 324 batch loss 1.23804307 epoch total loss 1.19859123\n",
      "Trained batch 325 batch loss 1.22464657 epoch total loss 1.19867146\n",
      "Trained batch 326 batch loss 1.41622305 epoch total loss 1.19933879\n",
      "Trained batch 327 batch loss 1.33819318 epoch total loss 1.19976342\n",
      "Trained batch 328 batch loss 1.33533525 epoch total loss 1.20017672\n",
      "Trained batch 329 batch loss 1.3438822 epoch total loss 1.2006135\n",
      "Trained batch 330 batch loss 1.33285236 epoch total loss 1.20101416\n",
      "Trained batch 331 batch loss 1.24262595 epoch total loss 1.20113993\n",
      "Trained batch 332 batch loss 1.17019391 epoch total loss 1.20104671\n",
      "Trained batch 333 batch loss 1.12802362 epoch total loss 1.20082736\n",
      "Trained batch 334 batch loss 1.18643451 epoch total loss 1.20078433\n",
      "Trained batch 335 batch loss 1.16405678 epoch total loss 1.20067465\n",
      "Trained batch 336 batch loss 1.206429 epoch total loss 1.20069182\n",
      "Trained batch 337 batch loss 1.22542012 epoch total loss 1.20076513\n",
      "Trained batch 338 batch loss 1.27372587 epoch total loss 1.20098102\n",
      "Trained batch 339 batch loss 1.18838012 epoch total loss 1.20094383\n",
      "Trained batch 340 batch loss 1.34398818 epoch total loss 1.20136464\n",
      "Trained batch 341 batch loss 1.20499396 epoch total loss 1.20137525\n",
      "Trained batch 342 batch loss 1.06791818 epoch total loss 1.20098507\n",
      "Trained batch 343 batch loss 1.11367321 epoch total loss 1.20073044\n",
      "Trained batch 344 batch loss 1.09159327 epoch total loss 1.20041323\n",
      "Trained batch 345 batch loss 1.20027864 epoch total loss 1.20041287\n",
      "Trained batch 346 batch loss 1.23220718 epoch total loss 1.20050478\n",
      "Trained batch 347 batch loss 1.10891187 epoch total loss 1.20024073\n",
      "Trained batch 348 batch loss 1.09049892 epoch total loss 1.19992542\n",
      "Trained batch 349 batch loss 1.11644816 epoch total loss 1.19968617\n",
      "Trained batch 350 batch loss 1.21955681 epoch total loss 1.19974291\n",
      "Trained batch 351 batch loss 1.22683465 epoch total loss 1.19982016\n",
      "Trained batch 352 batch loss 1.23147976 epoch total loss 1.19991\n",
      "Trained batch 353 batch loss 1.23842084 epoch total loss 1.20001924\n",
      "Trained batch 354 batch loss 1.11886978 epoch total loss 1.19979\n",
      "Trained batch 355 batch loss 1.15389419 epoch total loss 1.19966066\n",
      "Trained batch 356 batch loss 1.19745195 epoch total loss 1.19965446\n",
      "Trained batch 357 batch loss 1.21864474 epoch total loss 1.19970775\n",
      "Trained batch 358 batch loss 1.13697767 epoch total loss 1.19953251\n",
      "Trained batch 359 batch loss 1.10120595 epoch total loss 1.19925857\n",
      "Trained batch 360 batch loss 1.15362251 epoch total loss 1.19913173\n",
      "Trained batch 361 batch loss 1.27209449 epoch total loss 1.19933391\n",
      "Trained batch 362 batch loss 1.11768961 epoch total loss 1.19910836\n",
      "Trained batch 363 batch loss 1.16557527 epoch total loss 1.19901597\n",
      "Trained batch 364 batch loss 1.12060142 epoch total loss 1.19880056\n",
      "Trained batch 365 batch loss 0.95453167 epoch total loss 1.19813132\n",
      "Trained batch 366 batch loss 1.02778077 epoch total loss 1.19766581\n",
      "Trained batch 367 batch loss 0.895036 epoch total loss 1.19684124\n",
      "Trained batch 368 batch loss 1.03507078 epoch total loss 1.19640172\n",
      "Trained batch 369 batch loss 1.08509016 epoch total loss 1.1961\n",
      "Trained batch 370 batch loss 1.13080287 epoch total loss 1.19592357\n",
      "Trained batch 371 batch loss 1.11145329 epoch total loss 1.19569588\n",
      "Trained batch 372 batch loss 1.08952475 epoch total loss 1.19541049\n",
      "Trained batch 373 batch loss 1.06836164 epoch total loss 1.19506979\n",
      "Trained batch 374 batch loss 1.15679741 epoch total loss 1.19496751\n",
      "Trained batch 375 batch loss 0.949037194 epoch total loss 1.19431174\n",
      "Trained batch 376 batch loss 1.04607034 epoch total loss 1.19391751\n",
      "Trained batch 377 batch loss 1.17907655 epoch total loss 1.19387805\n",
      "Trained batch 378 batch loss 1.13530135 epoch total loss 1.1937232\n",
      "Trained batch 379 batch loss 1.13549471 epoch total loss 1.19356954\n",
      "Trained batch 380 batch loss 1.0545907 epoch total loss 1.19320381\n",
      "Trained batch 381 batch loss 1.09117103 epoch total loss 1.19293594\n",
      "Trained batch 382 batch loss 1.18279672 epoch total loss 1.19290948\n",
      "Trained batch 383 batch loss 1.24954367 epoch total loss 1.1930573\n",
      "Trained batch 384 batch loss 1.25429106 epoch total loss 1.1932168\n",
      "Trained batch 385 batch loss 1.28979874 epoch total loss 1.19346762\n",
      "Trained batch 386 batch loss 1.18913627 epoch total loss 1.19345641\n",
      "Trained batch 387 batch loss 1.23933268 epoch total loss 1.19357502\n",
      "Trained batch 388 batch loss 1.16862798 epoch total loss 1.19351065\n",
      "Trained batch 389 batch loss 1.16884637 epoch total loss 1.19344735\n",
      "Trained batch 390 batch loss 1.11334848 epoch total loss 1.19324195\n",
      "Trained batch 391 batch loss 1.2974751 epoch total loss 1.19350851\n",
      "Trained batch 392 batch loss 1.2662648 epoch total loss 1.19369411\n",
      "Trained batch 393 batch loss 1.18895268 epoch total loss 1.19368207\n",
      "Trained batch 394 batch loss 1.00666463 epoch total loss 1.19320738\n",
      "Trained batch 395 batch loss 1.08628917 epoch total loss 1.19293678\n",
      "Trained batch 396 batch loss 0.999231756 epoch total loss 1.19244766\n",
      "Trained batch 397 batch loss 1.13197899 epoch total loss 1.19229531\n",
      "Trained batch 398 batch loss 1.12216294 epoch total loss 1.19211912\n",
      "Trained batch 399 batch loss 1.27268243 epoch total loss 1.19232106\n",
      "Trained batch 400 batch loss 1.15780592 epoch total loss 1.19223475\n",
      "Trained batch 401 batch loss 1.13767648 epoch total loss 1.19209862\n",
      "Trained batch 402 batch loss 1.2105571 epoch total loss 1.19214463\n",
      "Trained batch 403 batch loss 1.33806562 epoch total loss 1.19250667\n",
      "Trained batch 404 batch loss 1.29876733 epoch total loss 1.19276977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 405 batch loss 1.45613563 epoch total loss 1.19342\n",
      "Trained batch 406 batch loss 1.40610886 epoch total loss 1.19394386\n",
      "Trained batch 407 batch loss 1.14744151 epoch total loss 1.19382954\n",
      "Trained batch 408 batch loss 1.10241699 epoch total loss 1.19360554\n",
      "Trained batch 409 batch loss 1.2870568 epoch total loss 1.19383395\n",
      "Trained batch 410 batch loss 1.29823065 epoch total loss 1.19408858\n",
      "Trained batch 411 batch loss 1.33310115 epoch total loss 1.19442677\n",
      "Trained batch 412 batch loss 1.28231037 epoch total loss 1.19464016\n",
      "Trained batch 413 batch loss 1.30773747 epoch total loss 1.19491398\n",
      "Trained batch 414 batch loss 1.24052298 epoch total loss 1.19502413\n",
      "Trained batch 415 batch loss 1.20345867 epoch total loss 1.19504452\n",
      "Trained batch 416 batch loss 1.10002661 epoch total loss 1.19481611\n",
      "Trained batch 417 batch loss 1.16303229 epoch total loss 1.19473982\n",
      "Trained batch 418 batch loss 1.16304851 epoch total loss 1.194664\n",
      "Trained batch 419 batch loss 1.14656758 epoch total loss 1.19454932\n",
      "Trained batch 420 batch loss 1.21703374 epoch total loss 1.19460285\n",
      "Trained batch 421 batch loss 1.23672891 epoch total loss 1.19470286\n",
      "Trained batch 422 batch loss 1.29075527 epoch total loss 1.19493043\n",
      "Trained batch 423 batch loss 1.30676579 epoch total loss 1.19519484\n",
      "Trained batch 424 batch loss 1.24148309 epoch total loss 1.19530404\n",
      "Trained batch 425 batch loss 1.34570467 epoch total loss 1.19565785\n",
      "Trained batch 426 batch loss 1.32414627 epoch total loss 1.19595957\n",
      "Trained batch 427 batch loss 1.24607468 epoch total loss 1.19607687\n",
      "Trained batch 428 batch loss 1.06896734 epoch total loss 1.19577992\n",
      "Trained batch 429 batch loss 1.26394701 epoch total loss 1.19593883\n",
      "Trained batch 430 batch loss 1.09776735 epoch total loss 1.19571054\n",
      "Trained batch 431 batch loss 1.06775737 epoch total loss 1.19541359\n",
      "Trained batch 432 batch loss 1.06245637 epoch total loss 1.19510579\n",
      "Trained batch 433 batch loss 1.23534763 epoch total loss 1.19519877\n",
      "Trained batch 434 batch loss 0.990493238 epoch total loss 1.19472706\n",
      "Trained batch 435 batch loss 1.05974007 epoch total loss 1.19441676\n",
      "Trained batch 436 batch loss 1.19037771 epoch total loss 1.19440746\n",
      "Trained batch 437 batch loss 1.19157672 epoch total loss 1.19440103\n",
      "Trained batch 438 batch loss 1.22511613 epoch total loss 1.19447112\n",
      "Trained batch 439 batch loss 1.22794271 epoch total loss 1.19454741\n",
      "Trained batch 440 batch loss 1.28901386 epoch total loss 1.19476211\n",
      "Trained batch 441 batch loss 1.14770174 epoch total loss 1.19465542\n",
      "Trained batch 442 batch loss 1.08896112 epoch total loss 1.19441628\n",
      "Trained batch 443 batch loss 1.08230853 epoch total loss 1.19416332\n",
      "Trained batch 444 batch loss 1.16123021 epoch total loss 1.19408917\n",
      "Trained batch 445 batch loss 1.13056338 epoch total loss 1.19394636\n",
      "Trained batch 446 batch loss 1.21761155 epoch total loss 1.19399941\n",
      "Trained batch 447 batch loss 1.29868603 epoch total loss 1.19423366\n",
      "Trained batch 448 batch loss 1.19492507 epoch total loss 1.19423521\n",
      "Trained batch 449 batch loss 1.29529071 epoch total loss 1.19446027\n",
      "Trained batch 450 batch loss 1.27335763 epoch total loss 1.19463563\n",
      "Trained batch 451 batch loss 1.3848567 epoch total loss 1.19505739\n",
      "Trained batch 452 batch loss 1.2486217 epoch total loss 1.19517589\n",
      "Trained batch 453 batch loss 1.32554817 epoch total loss 1.19546366\n",
      "Trained batch 454 batch loss 1.25503981 epoch total loss 1.19559491\n",
      "Trained batch 455 batch loss 1.25443339 epoch total loss 1.19572437\n",
      "Trained batch 456 batch loss 1.30164146 epoch total loss 1.19595659\n",
      "Trained batch 457 batch loss 1.07504892 epoch total loss 1.19569206\n",
      "Trained batch 458 batch loss 1.18140805 epoch total loss 1.19566083\n",
      "Trained batch 459 batch loss 1.25300729 epoch total loss 1.19578576\n",
      "Trained batch 460 batch loss 1.1859951 epoch total loss 1.19576442\n",
      "Trained batch 461 batch loss 1.12739348 epoch total loss 1.19561613\n",
      "Trained batch 462 batch loss 1.17847276 epoch total loss 1.19557893\n",
      "Trained batch 463 batch loss 1.0740695 epoch total loss 1.19531655\n",
      "Trained batch 464 batch loss 1.10206306 epoch total loss 1.19511557\n",
      "Trained batch 465 batch loss 0.994006038 epoch total loss 1.19468307\n",
      "Trained batch 466 batch loss 1.01621115 epoch total loss 1.19430017\n",
      "Trained batch 467 batch loss 1.09179401 epoch total loss 1.19408071\n",
      "Trained batch 468 batch loss 1.12075686 epoch total loss 1.19392395\n",
      "Trained batch 469 batch loss 0.982641459 epoch total loss 1.19347346\n",
      "Trained batch 470 batch loss 1.08292 epoch total loss 1.19323838\n",
      "Trained batch 471 batch loss 1.10184324 epoch total loss 1.1930443\n",
      "Trained batch 472 batch loss 1.08397794 epoch total loss 1.19281328\n",
      "Trained batch 473 batch loss 1.27446699 epoch total loss 1.19298589\n",
      "Trained batch 474 batch loss 1.22383916 epoch total loss 1.19305098\n",
      "Trained batch 475 batch loss 1.14987 epoch total loss 1.19296\n",
      "Trained batch 476 batch loss 1.26656675 epoch total loss 1.19311464\n",
      "Trained batch 477 batch loss 1.14881992 epoch total loss 1.19302166\n",
      "Trained batch 478 batch loss 1.02549756 epoch total loss 1.1926713\n",
      "Trained batch 479 batch loss 1.16700733 epoch total loss 1.19261765\n",
      "Trained batch 480 batch loss 1.1590445 epoch total loss 1.19254768\n",
      "Trained batch 481 batch loss 1.27338219 epoch total loss 1.19271576\n",
      "Trained batch 482 batch loss 1.28642702 epoch total loss 1.19291019\n",
      "Trained batch 483 batch loss 1.24511826 epoch total loss 1.19301832\n",
      "Trained batch 484 batch loss 1.29836035 epoch total loss 1.19323587\n",
      "Trained batch 485 batch loss 1.26023066 epoch total loss 1.19337416\n",
      "Trained batch 486 batch loss 1.17863059 epoch total loss 1.19334376\n",
      "Trained batch 487 batch loss 1.31553614 epoch total loss 1.19359469\n",
      "Trained batch 488 batch loss 1.3201617 epoch total loss 1.19385421\n",
      "Trained batch 489 batch loss 1.14984131 epoch total loss 1.19376421\n",
      "Trained batch 490 batch loss 1.11121702 epoch total loss 1.19359565\n",
      "Trained batch 491 batch loss 1.12982142 epoch total loss 1.19346583\n",
      "Trained batch 492 batch loss 1.0435493 epoch total loss 1.19316113\n",
      "Trained batch 493 batch loss 1.13943434 epoch total loss 1.19305205\n",
      "Trained batch 494 batch loss 1.17995906 epoch total loss 1.19302559\n",
      "Trained batch 495 batch loss 1.14941227 epoch total loss 1.19293737\n",
      "Trained batch 496 batch loss 1.21579313 epoch total loss 1.19298351\n",
      "Trained batch 497 batch loss 1.35435319 epoch total loss 1.19330823\n",
      "Trained batch 498 batch loss 1.17707813 epoch total loss 1.19327569\n",
      "Trained batch 499 batch loss 1.23191452 epoch total loss 1.19335318\n",
      "Trained batch 500 batch loss 1.18839455 epoch total loss 1.19334328\n",
      "Trained batch 501 batch loss 1.1212405 epoch total loss 1.19319928\n",
      "Trained batch 502 batch loss 1.03688645 epoch total loss 1.1928879\n",
      "Trained batch 503 batch loss 1.09786677 epoch total loss 1.19269896\n",
      "Trained batch 504 batch loss 0.97112304 epoch total loss 1.19225931\n",
      "Trained batch 505 batch loss 1.36882114 epoch total loss 1.19260895\n",
      "Trained batch 506 batch loss 1.30754709 epoch total loss 1.19283617\n",
      "Trained batch 507 batch loss 1.18817878 epoch total loss 1.19282687\n",
      "Trained batch 508 batch loss 1.10245383 epoch total loss 1.19264901\n",
      "Trained batch 509 batch loss 1.14147 epoch total loss 1.19254851\n",
      "Trained batch 510 batch loss 1.16209221 epoch total loss 1.19248879\n",
      "Trained batch 511 batch loss 1.13558722 epoch total loss 1.19237745\n",
      "Trained batch 512 batch loss 1.04147661 epoch total loss 1.19208276\n",
      "Trained batch 513 batch loss 1.16016161 epoch total loss 1.19202054\n",
      "Trained batch 514 batch loss 1.13843358 epoch total loss 1.19191623\n",
      "Trained batch 515 batch loss 1.24286222 epoch total loss 1.19201517\n",
      "Trained batch 516 batch loss 1.20516515 epoch total loss 1.19204056\n",
      "Trained batch 517 batch loss 1.2413429 epoch total loss 1.19213593\n",
      "Trained batch 518 batch loss 1.05737936 epoch total loss 1.19187582\n",
      "Trained batch 519 batch loss 1.14875126 epoch total loss 1.19179273\n",
      "Trained batch 520 batch loss 1.1514976 epoch total loss 1.19171524\n",
      "Trained batch 521 batch loss 1.0014801 epoch total loss 1.19135\n",
      "Trained batch 522 batch loss 1.02330148 epoch total loss 1.19102812\n",
      "Trained batch 523 batch loss 1.00484467 epoch total loss 1.19067204\n",
      "Trained batch 524 batch loss 1.03695107 epoch total loss 1.19037867\n",
      "Trained batch 525 batch loss 1.19146085 epoch total loss 1.19038069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 526 batch loss 1.16276383 epoch total loss 1.19032824\n",
      "Trained batch 527 batch loss 1.0185045 epoch total loss 1.1900022\n",
      "Trained batch 528 batch loss 1.09666061 epoch total loss 1.18982542\n",
      "Trained batch 529 batch loss 1.23248661 epoch total loss 1.18990612\n",
      "Trained batch 530 batch loss 1.04190445 epoch total loss 1.18962693\n",
      "Trained batch 531 batch loss 1.30015361 epoch total loss 1.18983507\n",
      "Trained batch 532 batch loss 0.933105469 epoch total loss 1.18935251\n",
      "Trained batch 533 batch loss 1.11056578 epoch total loss 1.18920469\n",
      "Trained batch 534 batch loss 1.11095607 epoch total loss 1.18905818\n",
      "Trained batch 535 batch loss 1.15463364 epoch total loss 1.18899393\n",
      "Trained batch 536 batch loss 1.24570692 epoch total loss 1.18909979\n",
      "Trained batch 537 batch loss 1.27289426 epoch total loss 1.18925583\n",
      "Trained batch 538 batch loss 1.42478013 epoch total loss 1.18969369\n",
      "Trained batch 539 batch loss 1.2770412 epoch total loss 1.18985569\n",
      "Trained batch 540 batch loss 1.15473843 epoch total loss 1.18979061\n",
      "Trained batch 541 batch loss 1.19437313 epoch total loss 1.18979919\n",
      "Trained batch 542 batch loss 1.11854327 epoch total loss 1.1896677\n",
      "Trained batch 543 batch loss 1.20936513 epoch total loss 1.18970394\n",
      "Trained batch 544 batch loss 1.22957289 epoch total loss 1.18977714\n",
      "Trained batch 545 batch loss 1.19085073 epoch total loss 1.18977916\n",
      "Trained batch 546 batch loss 1.30952179 epoch total loss 1.18999839\n",
      "Trained batch 547 batch loss 1.17659318 epoch total loss 1.18997383\n",
      "Trained batch 548 batch loss 1.16611743 epoch total loss 1.18993044\n",
      "Trained batch 549 batch loss 1.21895909 epoch total loss 1.18998325\n",
      "Trained batch 550 batch loss 1.16388416 epoch total loss 1.1899358\n",
      "Trained batch 551 batch loss 1.0754801 epoch total loss 1.18972802\n",
      "Trained batch 552 batch loss 1.07256293 epoch total loss 1.18951583\n",
      "Trained batch 553 batch loss 1.17961812 epoch total loss 1.18949795\n",
      "Trained batch 554 batch loss 1.27306783 epoch total loss 1.18964875\n",
      "Trained batch 555 batch loss 1.24778068 epoch total loss 1.18975353\n",
      "Trained batch 556 batch loss 1.19588816 epoch total loss 1.1897645\n",
      "Trained batch 557 batch loss 1.25337243 epoch total loss 1.1898787\n",
      "Trained batch 558 batch loss 1.23699617 epoch total loss 1.18996322\n",
      "Trained batch 559 batch loss 1.14735377 epoch total loss 1.18988693\n",
      "Trained batch 560 batch loss 1.12020731 epoch total loss 1.18976247\n",
      "Trained batch 561 batch loss 1.13471675 epoch total loss 1.18966424\n",
      "Trained batch 562 batch loss 1.05644166 epoch total loss 1.18942726\n",
      "Trained batch 563 batch loss 1.13640642 epoch total loss 1.18933308\n",
      "Trained batch 564 batch loss 1.20825076 epoch total loss 1.1893667\n",
      "Trained batch 565 batch loss 1.10369563 epoch total loss 1.18921506\n",
      "Trained batch 566 batch loss 1.12528086 epoch total loss 1.18910217\n",
      "Trained batch 567 batch loss 1.1423738 epoch total loss 1.1890198\n",
      "Trained batch 568 batch loss 1.22767425 epoch total loss 1.18908775\n",
      "Trained batch 569 batch loss 1.21361184 epoch total loss 1.1891309\n",
      "Trained batch 570 batch loss 1.2631762 epoch total loss 1.18926084\n",
      "Trained batch 571 batch loss 1.37808549 epoch total loss 1.18959153\n",
      "Trained batch 572 batch loss 1.32392454 epoch total loss 1.18982637\n",
      "Trained batch 573 batch loss 1.38632143 epoch total loss 1.19016922\n",
      "Trained batch 574 batch loss 1.3670367 epoch total loss 1.19047749\n",
      "Trained batch 575 batch loss 1.22338 epoch total loss 1.19053471\n",
      "Trained batch 576 batch loss 1.16089392 epoch total loss 1.19048321\n",
      "Trained batch 577 batch loss 1.23177397 epoch total loss 1.19055474\n",
      "Trained batch 578 batch loss 1.24127495 epoch total loss 1.19064248\n",
      "Trained batch 579 batch loss 1.11274362 epoch total loss 1.19050789\n",
      "Trained batch 580 batch loss 1.21194625 epoch total loss 1.19054496\n",
      "Trained batch 581 batch loss 1.16678381 epoch total loss 1.19050407\n",
      "Trained batch 582 batch loss 1.14865208 epoch total loss 1.19043219\n",
      "Trained batch 583 batch loss 1.115798 epoch total loss 1.19030416\n",
      "Trained batch 584 batch loss 1.19232559 epoch total loss 1.19030762\n",
      "Trained batch 585 batch loss 1.12515223 epoch total loss 1.19019616\n",
      "Trained batch 586 batch loss 1.27498662 epoch total loss 1.19034088\n",
      "Trained batch 587 batch loss 1.24401736 epoch total loss 1.19043231\n",
      "Trained batch 588 batch loss 1.1700418 epoch total loss 1.19039762\n",
      "Trained batch 589 batch loss 1.32690072 epoch total loss 1.19062936\n",
      "Trained batch 590 batch loss 1.122383 epoch total loss 1.19051373\n",
      "Trained batch 591 batch loss 1.0733695 epoch total loss 1.19031549\n",
      "Trained batch 592 batch loss 0.999383 epoch total loss 1.1899929\n",
      "Trained batch 593 batch loss 1.1809516 epoch total loss 1.18997777\n",
      "Trained batch 594 batch loss 1.24238038 epoch total loss 1.19006598\n",
      "Trained batch 595 batch loss 1.36068308 epoch total loss 1.19035268\n",
      "Trained batch 596 batch loss 1.25199902 epoch total loss 1.19045615\n",
      "Trained batch 597 batch loss 1.28461027 epoch total loss 1.19061387\n",
      "Trained batch 598 batch loss 1.33008301 epoch total loss 1.19084704\n",
      "Trained batch 599 batch loss 1.22275472 epoch total loss 1.19090033\n",
      "Trained batch 600 batch loss 1.27345526 epoch total loss 1.19103789\n",
      "Trained batch 601 batch loss 1.20888 epoch total loss 1.19106758\n",
      "Trained batch 602 batch loss 1.26696682 epoch total loss 1.1911937\n",
      "Trained batch 603 batch loss 1.23238444 epoch total loss 1.19126189\n",
      "Trained batch 604 batch loss 1.26502228 epoch total loss 1.19138408\n",
      "Trained batch 605 batch loss 1.37015796 epoch total loss 1.1916796\n",
      "Trained batch 606 batch loss 1.22266841 epoch total loss 1.19173062\n",
      "Trained batch 607 batch loss 1.09757257 epoch total loss 1.19157553\n",
      "Trained batch 608 batch loss 1.29380405 epoch total loss 1.19174373\n",
      "Trained batch 609 batch loss 1.28791511 epoch total loss 1.19190168\n",
      "Trained batch 610 batch loss 1.22374165 epoch total loss 1.1919539\n",
      "Trained batch 611 batch loss 1.21219373 epoch total loss 1.19198704\n",
      "Trained batch 612 batch loss 1.3108716 epoch total loss 1.19218123\n",
      "Trained batch 613 batch loss 1.09898627 epoch total loss 1.19202924\n",
      "Trained batch 614 batch loss 1.27485788 epoch total loss 1.19216406\n",
      "Trained batch 615 batch loss 1.2422291 epoch total loss 1.1922456\n",
      "Trained batch 616 batch loss 1.30507064 epoch total loss 1.19242871\n",
      "Trained batch 617 batch loss 1.37970889 epoch total loss 1.19273221\n",
      "Trained batch 618 batch loss 1.23894 epoch total loss 1.19280696\n",
      "Trained batch 619 batch loss 1.24194241 epoch total loss 1.19288635\n",
      "Trained batch 620 batch loss 1.34660125 epoch total loss 1.19313431\n",
      "Trained batch 621 batch loss 1.11951089 epoch total loss 1.19301581\n",
      "Trained batch 622 batch loss 1.24988866 epoch total loss 1.19310725\n",
      "Trained batch 623 batch loss 1.21511841 epoch total loss 1.19314241\n",
      "Trained batch 624 batch loss 1.27064776 epoch total loss 1.19326663\n",
      "Trained batch 625 batch loss 1.20958877 epoch total loss 1.19329274\n",
      "Trained batch 626 batch loss 1.20730233 epoch total loss 1.19331515\n",
      "Trained batch 627 batch loss 1.16768491 epoch total loss 1.19327414\n",
      "Trained batch 628 batch loss 1.23577046 epoch total loss 1.19334185\n",
      "Trained batch 629 batch loss 1.20921826 epoch total loss 1.19336712\n",
      "Trained batch 630 batch loss 1.06193376 epoch total loss 1.19315851\n",
      "Trained batch 631 batch loss 1.1446023 epoch total loss 1.19308162\n",
      "Trained batch 632 batch loss 1.08752406 epoch total loss 1.19291461\n",
      "Trained batch 633 batch loss 1.0885998 epoch total loss 1.19274974\n",
      "Trained batch 634 batch loss 1.08453536 epoch total loss 1.19257915\n",
      "Trained batch 635 batch loss 0.965093732 epoch total loss 1.19222081\n",
      "Trained batch 636 batch loss 1.11207366 epoch total loss 1.1920948\n",
      "Trained batch 637 batch loss 1.02528858 epoch total loss 1.1918329\n",
      "Trained batch 638 batch loss 1.12115252 epoch total loss 1.19172215\n",
      "Trained batch 639 batch loss 1.00929892 epoch total loss 1.19143665\n",
      "Trained batch 640 batch loss 1.12454522 epoch total loss 1.1913321\n",
      "Trained batch 641 batch loss 1.02199316 epoch total loss 1.19106793\n",
      "Trained batch 642 batch loss 1.19037879 epoch total loss 1.19106686\n",
      "Trained batch 643 batch loss 1.35328341 epoch total loss 1.19131911\n",
      "Trained batch 644 batch loss 1.11517394 epoch total loss 1.19120085\n",
      "Trained batch 645 batch loss 1.11497641 epoch total loss 1.19108272\n",
      "Trained batch 646 batch loss 1.37345612 epoch total loss 1.191365\n",
      "Trained batch 647 batch loss 1.3734622 epoch total loss 1.19164658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 648 batch loss 1.23332644 epoch total loss 1.19171083\n",
      "Trained batch 649 batch loss 1.21048248 epoch total loss 1.1917398\n",
      "Trained batch 650 batch loss 1.08848941 epoch total loss 1.19158101\n",
      "Trained batch 651 batch loss 1.24727774 epoch total loss 1.19166648\n",
      "Trained batch 652 batch loss 1.15961945 epoch total loss 1.19161737\n",
      "Trained batch 653 batch loss 1.15887475 epoch total loss 1.19156718\n",
      "Trained batch 654 batch loss 1.2524364 epoch total loss 1.19166028\n",
      "Trained batch 655 batch loss 1.21344209 epoch total loss 1.19169354\n",
      "Trained batch 656 batch loss 1.17185283 epoch total loss 1.19166327\n",
      "Trained batch 657 batch loss 1.38314807 epoch total loss 1.19195473\n",
      "Trained batch 658 batch loss 1.24762368 epoch total loss 1.19203937\n",
      "Trained batch 659 batch loss 1.16839576 epoch total loss 1.19200349\n",
      "Trained batch 660 batch loss 1.1511699 epoch total loss 1.19194162\n",
      "Trained batch 661 batch loss 1.26294231 epoch total loss 1.19204903\n",
      "Trained batch 662 batch loss 1.26572061 epoch total loss 1.19216037\n",
      "Trained batch 663 batch loss 1.23788619 epoch total loss 1.19222939\n",
      "Trained batch 664 batch loss 1.13391244 epoch total loss 1.19214153\n",
      "Trained batch 665 batch loss 1.24785292 epoch total loss 1.19222534\n",
      "Trained batch 666 batch loss 1.25511861 epoch total loss 1.19231975\n",
      "Trained batch 667 batch loss 1.36203146 epoch total loss 1.19257426\n",
      "Trained batch 668 batch loss 1.22169733 epoch total loss 1.19261777\n",
      "Trained batch 669 batch loss 1.25608647 epoch total loss 1.19271266\n",
      "Trained batch 670 batch loss 1.33527064 epoch total loss 1.19292545\n",
      "Trained batch 671 batch loss 1.34839082 epoch total loss 1.1931572\n",
      "Trained batch 672 batch loss 1.06601536 epoch total loss 1.19296801\n",
      "Trained batch 673 batch loss 1.07597125 epoch total loss 1.1927942\n",
      "Trained batch 674 batch loss 1.06792283 epoch total loss 1.19260895\n",
      "Trained batch 675 batch loss 1.12979841 epoch total loss 1.19251597\n",
      "Trained batch 676 batch loss 1.14092779 epoch total loss 1.19243956\n",
      "Trained batch 677 batch loss 1.22187304 epoch total loss 1.19248307\n",
      "Trained batch 678 batch loss 1.2075541 epoch total loss 1.19250536\n",
      "Trained batch 679 batch loss 1.16377 epoch total loss 1.19246304\n",
      "Trained batch 680 batch loss 1.16916 epoch total loss 1.19242883\n",
      "Trained batch 681 batch loss 1.3319242 epoch total loss 1.19263363\n",
      "Trained batch 682 batch loss 1.13907719 epoch total loss 1.19255507\n",
      "Trained batch 683 batch loss 1.34167552 epoch total loss 1.19277346\n",
      "Trained batch 684 batch loss 1.17954779 epoch total loss 1.19275415\n",
      "Trained batch 685 batch loss 1.16187131 epoch total loss 1.19270897\n",
      "Trained batch 686 batch loss 1.07616782 epoch total loss 1.1925391\n",
      "Trained batch 687 batch loss 1.13923717 epoch total loss 1.19246149\n",
      "Trained batch 688 batch loss 1.25902 epoch total loss 1.19255829\n",
      "Trained batch 689 batch loss 1.29354465 epoch total loss 1.1927048\n",
      "Trained batch 690 batch loss 1.26744866 epoch total loss 1.19281316\n",
      "Trained batch 691 batch loss 1.26906657 epoch total loss 1.19292343\n",
      "Trained batch 692 batch loss 1.09838045 epoch total loss 1.19278681\n",
      "Trained batch 693 batch loss 1.17881298 epoch total loss 1.19276679\n",
      "Trained batch 694 batch loss 1.27230561 epoch total loss 1.19288135\n",
      "Trained batch 695 batch loss 1.38685334 epoch total loss 1.19316041\n",
      "Trained batch 696 batch loss 1.19079947 epoch total loss 1.19315696\n",
      "Trained batch 697 batch loss 1.27930975 epoch total loss 1.19328058\n",
      "Trained batch 698 batch loss 1.05994725 epoch total loss 1.19308949\n",
      "Trained batch 699 batch loss 1.12446988 epoch total loss 1.19299138\n",
      "Trained batch 700 batch loss 1.1672343 epoch total loss 1.19295454\n",
      "Trained batch 701 batch loss 1.19653606 epoch total loss 1.19295967\n",
      "Trained batch 702 batch loss 1.15245295 epoch total loss 1.19290197\n",
      "Trained batch 703 batch loss 1.16856062 epoch total loss 1.1928674\n",
      "Trained batch 704 batch loss 1.22418606 epoch total loss 1.19291186\n",
      "Trained batch 705 batch loss 1.20570672 epoch total loss 1.19293\n",
      "Trained batch 706 batch loss 1.13079631 epoch total loss 1.19284201\n",
      "Trained batch 707 batch loss 1.1065197 epoch total loss 1.19271982\n",
      "Trained batch 708 batch loss 1.1115129 epoch total loss 1.19260514\n",
      "Trained batch 709 batch loss 1.16980577 epoch total loss 1.19257295\n",
      "Trained batch 710 batch loss 1.1709497 epoch total loss 1.19254255\n",
      "Trained batch 711 batch loss 1.12946367 epoch total loss 1.19245386\n",
      "Trained batch 712 batch loss 1.23408234 epoch total loss 1.19251227\n",
      "Trained batch 713 batch loss 1.12855649 epoch total loss 1.19242251\n",
      "Trained batch 714 batch loss 1.14630747 epoch total loss 1.1923579\n",
      "Trained batch 715 batch loss 1.11243117 epoch total loss 1.1922462\n",
      "Trained batch 716 batch loss 1.04426575 epoch total loss 1.19203949\n",
      "Trained batch 717 batch loss 1.21028447 epoch total loss 1.19206488\n",
      "Trained batch 718 batch loss 1.28340983 epoch total loss 1.19219208\n",
      "Trained batch 719 batch loss 1.21907246 epoch total loss 1.19222939\n",
      "Trained batch 720 batch loss 1.03969598 epoch total loss 1.19201756\n",
      "Trained batch 721 batch loss 1.21547771 epoch total loss 1.19205\n",
      "Trained batch 722 batch loss 1.0622139 epoch total loss 1.19187021\n",
      "Trained batch 723 batch loss 1.12465239 epoch total loss 1.19177723\n",
      "Trained batch 724 batch loss 1.17282188 epoch total loss 1.191751\n",
      "Trained batch 725 batch loss 1.23174047 epoch total loss 1.1918062\n",
      "Trained batch 726 batch loss 1.09930778 epoch total loss 1.19167876\n",
      "Trained batch 727 batch loss 1.14308643 epoch total loss 1.19161189\n",
      "Trained batch 728 batch loss 1.19483888 epoch total loss 1.1916163\n",
      "Trained batch 729 batch loss 1.13252842 epoch total loss 1.19153523\n",
      "Trained batch 730 batch loss 1.15965641 epoch total loss 1.1914916\n",
      "Trained batch 731 batch loss 1.11139655 epoch total loss 1.19138205\n",
      "Trained batch 732 batch loss 1.14243722 epoch total loss 1.19131517\n",
      "Trained batch 733 batch loss 1.20808315 epoch total loss 1.19133806\n",
      "Trained batch 734 batch loss 1.08145714 epoch total loss 1.19118834\n",
      "Trained batch 735 batch loss 1.14413297 epoch total loss 1.19112432\n",
      "Trained batch 736 batch loss 1.18310809 epoch total loss 1.19111347\n",
      "Trained batch 737 batch loss 1.13942719 epoch total loss 1.19104326\n",
      "Trained batch 738 batch loss 1.07763445 epoch total loss 1.1908896\n",
      "Trained batch 739 batch loss 0.920380473 epoch total loss 1.19052362\n",
      "Trained batch 740 batch loss 1.02639115 epoch total loss 1.19030178\n",
      "Trained batch 741 batch loss 1.20184958 epoch total loss 1.19031739\n",
      "Trained batch 742 batch loss 1.49796379 epoch total loss 1.190732\n",
      "Trained batch 743 batch loss 1.47475123 epoch total loss 1.19111419\n",
      "Trained batch 744 batch loss 1.20162416 epoch total loss 1.19112825\n",
      "Trained batch 745 batch loss 1.10899282 epoch total loss 1.1910181\n",
      "Trained batch 746 batch loss 1.20384586 epoch total loss 1.19103527\n",
      "Trained batch 747 batch loss 1.20960796 epoch total loss 1.19106019\n",
      "Trained batch 748 batch loss 1.23491418 epoch total loss 1.19111884\n",
      "Trained batch 749 batch loss 1.18502796 epoch total loss 1.19111061\n",
      "Trained batch 750 batch loss 1.2660681 epoch total loss 1.19121051\n",
      "Trained batch 751 batch loss 1.27487814 epoch total loss 1.19132197\n",
      "Trained batch 752 batch loss 1.18192899 epoch total loss 1.19130945\n",
      "Trained batch 753 batch loss 1.17326403 epoch total loss 1.19128561\n",
      "Trained batch 754 batch loss 1.15178037 epoch total loss 1.19123316\n",
      "Trained batch 755 batch loss 1.17992973 epoch total loss 1.19121826\n",
      "Trained batch 756 batch loss 1.09669495 epoch total loss 1.19109321\n",
      "Trained batch 757 batch loss 1.18367624 epoch total loss 1.19108331\n",
      "Trained batch 758 batch loss 1.09531832 epoch total loss 1.19095707\n",
      "Trained batch 759 batch loss 1.14791179 epoch total loss 1.19090033\n",
      "Trained batch 760 batch loss 1.29581892 epoch total loss 1.19103837\n",
      "Trained batch 761 batch loss 1.16156197 epoch total loss 1.19099963\n",
      "Trained batch 762 batch loss 1.08161342 epoch total loss 1.1908561\n",
      "Trained batch 763 batch loss 1.08438849 epoch total loss 1.19071651\n",
      "Trained batch 764 batch loss 1.21994948 epoch total loss 1.19075489\n",
      "Trained batch 765 batch loss 1.32471323 epoch total loss 1.19092989\n",
      "Trained batch 766 batch loss 1.26653028 epoch total loss 1.19102859\n",
      "Trained batch 767 batch loss 1.3754245 epoch total loss 1.19126904\n",
      "Trained batch 768 batch loss 1.25181603 epoch total loss 1.19134796\n",
      "Trained batch 769 batch loss 1.21849895 epoch total loss 1.19138324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 770 batch loss 1.21468639 epoch total loss 1.19141352\n",
      "Trained batch 771 batch loss 1.21213782 epoch total loss 1.19144034\n",
      "Trained batch 772 batch loss 1.19340205 epoch total loss 1.19144297\n",
      "Trained batch 773 batch loss 1.15072441 epoch total loss 1.19139028\n",
      "Trained batch 774 batch loss 1.14736199 epoch total loss 1.19133329\n",
      "Trained batch 775 batch loss 1.20912302 epoch total loss 1.19135618\n",
      "Trained batch 776 batch loss 1.20982623 epoch total loss 1.19138\n",
      "Trained batch 777 batch loss 1.2476052 epoch total loss 1.19145238\n",
      "Trained batch 778 batch loss 1.23693991 epoch total loss 1.19151092\n",
      "Trained batch 779 batch loss 1.24581385 epoch total loss 1.19158053\n",
      "Trained batch 780 batch loss 1.27906406 epoch total loss 1.19169271\n",
      "Trained batch 781 batch loss 1.21621716 epoch total loss 1.19172418\n",
      "Trained batch 782 batch loss 1.21061087 epoch total loss 1.19174838\n",
      "Trained batch 783 batch loss 1.10086846 epoch total loss 1.19163227\n",
      "Trained batch 784 batch loss 1.08159494 epoch total loss 1.19149196\n",
      "Trained batch 785 batch loss 1.17831743 epoch total loss 1.19147527\n",
      "Trained batch 786 batch loss 1.13034773 epoch total loss 1.19139743\n",
      "Trained batch 787 batch loss 1.23058033 epoch total loss 1.19144726\n",
      "Trained batch 788 batch loss 1.16641808 epoch total loss 1.19141555\n",
      "Trained batch 789 batch loss 1.02899647 epoch total loss 1.19120967\n",
      "Trained batch 790 batch loss 0.963539 epoch total loss 1.19092155\n",
      "Trained batch 791 batch loss 0.913270533 epoch total loss 1.19057047\n",
      "Trained batch 792 batch loss 1.1916455 epoch total loss 1.1905719\n",
      "Trained batch 793 batch loss 1.27043629 epoch total loss 1.19067264\n",
      "Trained batch 794 batch loss 1.24571323 epoch total loss 1.1907419\n",
      "Trained batch 795 batch loss 1.1760484 epoch total loss 1.19072342\n",
      "Trained batch 796 batch loss 1.18025 epoch total loss 1.19071031\n",
      "Trained batch 797 batch loss 1.09105289 epoch total loss 1.19058526\n",
      "Trained batch 798 batch loss 1.12667835 epoch total loss 1.19050515\n",
      "Trained batch 799 batch loss 1.07929373 epoch total loss 1.19036591\n",
      "Trained batch 800 batch loss 1.19958603 epoch total loss 1.19037747\n",
      "Trained batch 801 batch loss 1.1443193 epoch total loss 1.19032\n",
      "Trained batch 802 batch loss 1.17806911 epoch total loss 1.19030464\n",
      "Trained batch 803 batch loss 1.20653629 epoch total loss 1.1903249\n",
      "Trained batch 804 batch loss 1.17217779 epoch total loss 1.19030225\n",
      "Trained batch 805 batch loss 1.27518129 epoch total loss 1.19040775\n",
      "Trained batch 806 batch loss 1.48507726 epoch total loss 1.19077337\n",
      "Trained batch 807 batch loss 1.49317729 epoch total loss 1.19114816\n",
      "Trained batch 808 batch loss 1.30217278 epoch total loss 1.19128549\n",
      "Trained batch 809 batch loss 1.28750539 epoch total loss 1.19140446\n",
      "Trained batch 810 batch loss 1.30947196 epoch total loss 1.19155014\n",
      "Trained batch 811 batch loss 1.25912976 epoch total loss 1.19163358\n",
      "Trained batch 812 batch loss 0.990421593 epoch total loss 1.19138575\n",
      "Trained batch 813 batch loss 1.16672754 epoch total loss 1.19135547\n",
      "Trained batch 814 batch loss 1.2368691 epoch total loss 1.19141138\n",
      "Trained batch 815 batch loss 1.29923773 epoch total loss 1.1915437\n",
      "Trained batch 816 batch loss 1.19192684 epoch total loss 1.19154418\n",
      "Trained batch 817 batch loss 1.09335816 epoch total loss 1.19142401\n",
      "Trained batch 818 batch loss 1.16128659 epoch total loss 1.19138718\n",
      "Trained batch 819 batch loss 1.08085048 epoch total loss 1.19125223\n",
      "Trained batch 820 batch loss 1.05248094 epoch total loss 1.19108307\n",
      "Trained batch 821 batch loss 1.084589 epoch total loss 1.19095337\n",
      "Trained batch 822 batch loss 1.09275138 epoch total loss 1.19083393\n",
      "Trained batch 823 batch loss 1.15914047 epoch total loss 1.19079542\n",
      "Trained batch 824 batch loss 1.20518434 epoch total loss 1.19081283\n",
      "Trained batch 825 batch loss 1.16360617 epoch total loss 1.19077992\n",
      "Trained batch 826 batch loss 1.12226069 epoch total loss 1.19069695\n",
      "Trained batch 827 batch loss 1.06366217 epoch total loss 1.19054329\n",
      "Trained batch 828 batch loss 1.22121358 epoch total loss 1.19058037\n",
      "Trained batch 829 batch loss 1.22560251 epoch total loss 1.19062257\n",
      "Trained batch 830 batch loss 1.27830839 epoch total loss 1.19072819\n",
      "Trained batch 831 batch loss 1.26112902 epoch total loss 1.19081295\n",
      "Trained batch 832 batch loss 1.23971343 epoch total loss 1.19087172\n",
      "Trained batch 833 batch loss 1.26597154 epoch total loss 1.19096184\n",
      "Trained batch 834 batch loss 1.24083114 epoch total loss 1.19102168\n",
      "Trained batch 835 batch loss 1.22282338 epoch total loss 1.19105983\n",
      "Trained batch 836 batch loss 1.16224384 epoch total loss 1.19102526\n",
      "Trained batch 837 batch loss 1.08912814 epoch total loss 1.19090354\n",
      "Trained batch 838 batch loss 1.22482395 epoch total loss 1.19094408\n",
      "Trained batch 839 batch loss 1.12987638 epoch total loss 1.19087124\n",
      "Trained batch 840 batch loss 1.04488242 epoch total loss 1.19069743\n",
      "Trained batch 841 batch loss 1.14205265 epoch total loss 1.19063962\n",
      "Trained batch 842 batch loss 1.05975127 epoch total loss 1.19048417\n",
      "Trained batch 843 batch loss 1.1192522 epoch total loss 1.19039965\n",
      "Trained batch 844 batch loss 1.11700368 epoch total loss 1.19031262\n",
      "Trained batch 845 batch loss 1.15523887 epoch total loss 1.19027114\n",
      "Trained batch 846 batch loss 1.17732048 epoch total loss 1.19025576\n",
      "Trained batch 847 batch loss 1.32597482 epoch total loss 1.1904161\n",
      "Trained batch 848 batch loss 1.1844492 epoch total loss 1.19040906\n",
      "Trained batch 849 batch loss 1.23912036 epoch total loss 1.1904664\n",
      "Trained batch 850 batch loss 1.16033101 epoch total loss 1.190431\n",
      "Trained batch 851 batch loss 1.23192573 epoch total loss 1.19047976\n",
      "Trained batch 852 batch loss 1.23633671 epoch total loss 1.19053352\n",
      "Trained batch 853 batch loss 1.23462391 epoch total loss 1.19058526\n",
      "Trained batch 854 batch loss 1.16451824 epoch total loss 1.19055474\n",
      "Trained batch 855 batch loss 1.11048162 epoch total loss 1.19046104\n",
      "Trained batch 856 batch loss 1.1740129 epoch total loss 1.19044185\n",
      "Trained batch 857 batch loss 1.27582037 epoch total loss 1.19054139\n",
      "Trained batch 858 batch loss 1.32276273 epoch total loss 1.19069552\n",
      "Trained batch 859 batch loss 1.22693968 epoch total loss 1.19073772\n",
      "Trained batch 860 batch loss 1.17766845 epoch total loss 1.19072247\n",
      "Trained batch 861 batch loss 1.13582027 epoch total loss 1.19065881\n",
      "Trained batch 862 batch loss 1.17442346 epoch total loss 1.19064\n",
      "Trained batch 863 batch loss 1.1048342 epoch total loss 1.19054055\n",
      "Trained batch 864 batch loss 1.14413357 epoch total loss 1.19048691\n",
      "Trained batch 865 batch loss 1.20637929 epoch total loss 1.19050539\n",
      "Trained batch 866 batch loss 1.2641623 epoch total loss 1.19059038\n",
      "Trained batch 867 batch loss 1.26412249 epoch total loss 1.19067526\n",
      "Trained batch 868 batch loss 1.15487528 epoch total loss 1.19063401\n",
      "Trained batch 869 batch loss 1.15191746 epoch total loss 1.19058955\n",
      "Trained batch 870 batch loss 1.13604629 epoch total loss 1.19052672\n",
      "Trained batch 871 batch loss 1.11477494 epoch total loss 1.19043982\n",
      "Trained batch 872 batch loss 1.17144752 epoch total loss 1.19041789\n",
      "Trained batch 873 batch loss 1.11241293 epoch total loss 1.1903286\n",
      "Trained batch 874 batch loss 1.0799284 epoch total loss 1.19020236\n",
      "Trained batch 875 batch loss 1.11367655 epoch total loss 1.19011486\n",
      "Trained batch 876 batch loss 1.10411012 epoch total loss 1.19001663\n",
      "Trained batch 877 batch loss 1.15731895 epoch total loss 1.18997943\n",
      "Trained batch 878 batch loss 1.17576683 epoch total loss 1.18996322\n",
      "Trained batch 879 batch loss 1.0563345 epoch total loss 1.18981111\n",
      "Trained batch 880 batch loss 1.05953217 epoch total loss 1.18966317\n",
      "Trained batch 881 batch loss 1.0883019 epoch total loss 1.18954802\n",
      "Trained batch 882 batch loss 1.15440631 epoch total loss 1.1895082\n",
      "Trained batch 883 batch loss 1.13927639 epoch total loss 1.18945134\n",
      "Trained batch 884 batch loss 1.2240237 epoch total loss 1.18949044\n",
      "Trained batch 885 batch loss 1.18209243 epoch total loss 1.18948209\n",
      "Trained batch 886 batch loss 1.29348326 epoch total loss 1.18959939\n",
      "Trained batch 887 batch loss 1.29821289 epoch total loss 1.18972194\n",
      "Trained batch 888 batch loss 1.27514267 epoch total loss 1.18981814\n",
      "Trained batch 889 batch loss 1.24083519 epoch total loss 1.18987548\n",
      "Trained batch 890 batch loss 1.24001229 epoch total loss 1.18993175\n",
      "Trained batch 891 batch loss 1.1455344 epoch total loss 1.18988192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 892 batch loss 1.13422143 epoch total loss 1.18981957\n",
      "Trained batch 893 batch loss 1.24513543 epoch total loss 1.18988156\n",
      "Trained batch 894 batch loss 1.37942994 epoch total loss 1.19009352\n",
      "Trained batch 895 batch loss 1.30451322 epoch total loss 1.19022143\n",
      "Trained batch 896 batch loss 1.17955589 epoch total loss 1.19020951\n",
      "Trained batch 897 batch loss 1.19097388 epoch total loss 1.19021034\n",
      "Trained batch 898 batch loss 1.15758467 epoch total loss 1.19017398\n",
      "Trained batch 899 batch loss 1.1782769 epoch total loss 1.19016075\n",
      "Trained batch 900 batch loss 1.31061423 epoch total loss 1.19029462\n",
      "Trained batch 901 batch loss 1.2815156 epoch total loss 1.19039583\n",
      "Trained batch 902 batch loss 1.35257435 epoch total loss 1.1905756\n",
      "Trained batch 903 batch loss 1.46285677 epoch total loss 1.1908772\n",
      "Trained batch 904 batch loss 1.2331835 epoch total loss 1.19092393\n",
      "Trained batch 905 batch loss 1.31897295 epoch total loss 1.19106543\n",
      "Trained batch 906 batch loss 1.2238028 epoch total loss 1.19110143\n",
      "Trained batch 907 batch loss 1.21492982 epoch total loss 1.19112778\n",
      "Trained batch 908 batch loss 1.37767243 epoch total loss 1.19133329\n",
      "Trained batch 909 batch loss 1.21213186 epoch total loss 1.19135618\n",
      "Trained batch 910 batch loss 1.08708096 epoch total loss 1.1912415\n",
      "Trained batch 911 batch loss 1.08490634 epoch total loss 1.19112492\n",
      "Trained batch 912 batch loss 1.25831473 epoch total loss 1.19119847\n",
      "Trained batch 913 batch loss 1.22027946 epoch total loss 1.19123042\n",
      "Trained batch 914 batch loss 1.07608652 epoch total loss 1.19110441\n",
      "Trained batch 915 batch loss 1.26406908 epoch total loss 1.19118416\n",
      "Trained batch 916 batch loss 1.08450437 epoch total loss 1.19106758\n",
      "Trained batch 917 batch loss 1.16941166 epoch total loss 1.19104409\n",
      "Trained batch 918 batch loss 1.39882815 epoch total loss 1.19127035\n",
      "Trained batch 919 batch loss 1.39430273 epoch total loss 1.19149125\n",
      "Trained batch 920 batch loss 1.19617581 epoch total loss 1.19149637\n",
      "Trained batch 921 batch loss 1.05039275 epoch total loss 1.19134319\n",
      "Trained batch 922 batch loss 1.01023912 epoch total loss 1.19114673\n",
      "Trained batch 923 batch loss 1.00208843 epoch total loss 1.19094193\n",
      "Trained batch 924 batch loss 1.01762688 epoch total loss 1.19075429\n",
      "Trained batch 925 batch loss 0.993522286 epoch total loss 1.19054103\n",
      "Trained batch 926 batch loss 0.981509447 epoch total loss 1.19031537\n",
      "Trained batch 927 batch loss 1.12065101 epoch total loss 1.19024014\n",
      "Trained batch 928 batch loss 1.15501463 epoch total loss 1.19020224\n",
      "Trained batch 929 batch loss 1.12364173 epoch total loss 1.19013059\n",
      "Trained batch 930 batch loss 1.2458899 epoch total loss 1.19019055\n",
      "Trained batch 931 batch loss 1.16908801 epoch total loss 1.1901679\n",
      "Trained batch 932 batch loss 1.25711095 epoch total loss 1.19023967\n",
      "Trained batch 933 batch loss 1.21394277 epoch total loss 1.19026506\n",
      "Trained batch 934 batch loss 1.08316708 epoch total loss 1.19015038\n",
      "Trained batch 935 batch loss 1.23965073 epoch total loss 1.19020331\n",
      "Trained batch 936 batch loss 1.20893443 epoch total loss 1.19022334\n",
      "Trained batch 937 batch loss 1.21916461 epoch total loss 1.19025421\n",
      "Trained batch 938 batch loss 1.08581781 epoch total loss 1.19014287\n",
      "Trained batch 939 batch loss 1.07227302 epoch total loss 1.19001734\n",
      "Trained batch 940 batch loss 1.24512243 epoch total loss 1.19007599\n",
      "Trained batch 941 batch loss 1.23478007 epoch total loss 1.19012344\n",
      "Trained batch 942 batch loss 1.21651888 epoch total loss 1.19015145\n",
      "Trained batch 943 batch loss 1.28914297 epoch total loss 1.19025648\n",
      "Trained batch 944 batch loss 1.13601518 epoch total loss 1.19019902\n",
      "Trained batch 945 batch loss 1.15286744 epoch total loss 1.19015944\n",
      "Trained batch 946 batch loss 1.19006932 epoch total loss 1.19015932\n",
      "Trained batch 947 batch loss 1.16572046 epoch total loss 1.19013357\n",
      "Trained batch 948 batch loss 1.27520299 epoch total loss 1.19022322\n",
      "Trained batch 949 batch loss 1.27626753 epoch total loss 1.19031394\n",
      "Trained batch 950 batch loss 1.37771964 epoch total loss 1.19051111\n",
      "Trained batch 951 batch loss 1.25704741 epoch total loss 1.1905812\n",
      "Trained batch 952 batch loss 1.25679958 epoch total loss 1.1906507\n",
      "Trained batch 953 batch loss 1.16275799 epoch total loss 1.1906215\n",
      "Trained batch 954 batch loss 1.19981718 epoch total loss 1.19063103\n",
      "Trained batch 955 batch loss 1.17132747 epoch total loss 1.19061089\n",
      "Trained batch 956 batch loss 1.144243 epoch total loss 1.19056249\n",
      "Trained batch 957 batch loss 1.25203753 epoch total loss 1.19062674\n",
      "Trained batch 958 batch loss 1.2136761 epoch total loss 1.19065082\n",
      "Trained batch 959 batch loss 1.17224884 epoch total loss 1.19063151\n",
      "Trained batch 960 batch loss 1.21429968 epoch total loss 1.1906563\n",
      "Trained batch 961 batch loss 1.08953655 epoch total loss 1.19055104\n",
      "Trained batch 962 batch loss 1.01241493 epoch total loss 1.19036591\n",
      "Trained batch 963 batch loss 1.1607058 epoch total loss 1.19033515\n",
      "Trained batch 964 batch loss 1.16157866 epoch total loss 1.19030535\n",
      "Trained batch 965 batch loss 1.17457271 epoch total loss 1.19028902\n",
      "Trained batch 966 batch loss 1.09161568 epoch total loss 1.19018698\n",
      "Trained batch 967 batch loss 1.0621202 epoch total loss 1.19005454\n",
      "Trained batch 968 batch loss 1.08745253 epoch total loss 1.18994844\n",
      "Trained batch 969 batch loss 1.26345563 epoch total loss 1.19002426\n",
      "Trained batch 970 batch loss 1.2904985 epoch total loss 1.19012797\n",
      "Trained batch 971 batch loss 1.22253442 epoch total loss 1.19016123\n",
      "Trained batch 972 batch loss 1.19839883 epoch total loss 1.19016969\n",
      "Trained batch 973 batch loss 0.886781931 epoch total loss 1.18985796\n",
      "Trained batch 974 batch loss 0.850780964 epoch total loss 1.18950987\n",
      "Trained batch 975 batch loss 0.900292039 epoch total loss 1.18921328\n",
      "Trained batch 976 batch loss 1.06780672 epoch total loss 1.18908882\n",
      "Trained batch 977 batch loss 1.25156415 epoch total loss 1.18915272\n",
      "Trained batch 978 batch loss 1.13330495 epoch total loss 1.18909562\n",
      "Trained batch 979 batch loss 1.28499269 epoch total loss 1.18919361\n",
      "Trained batch 980 batch loss 1.34918416 epoch total loss 1.18935692\n",
      "Trained batch 981 batch loss 1.2066226 epoch total loss 1.18937457\n",
      "Trained batch 982 batch loss 1.26279688 epoch total loss 1.18944943\n",
      "Trained batch 983 batch loss 1.29033399 epoch total loss 1.18955195\n",
      "Trained batch 984 batch loss 1.26874387 epoch total loss 1.18963253\n",
      "Trained batch 985 batch loss 1.09440136 epoch total loss 1.18953574\n",
      "Trained batch 986 batch loss 1.16303873 epoch total loss 1.18950891\n",
      "Trained batch 987 batch loss 1.13478506 epoch total loss 1.18945348\n",
      "Trained batch 988 batch loss 1.17222822 epoch total loss 1.18943608\n",
      "Trained batch 989 batch loss 1.11872208 epoch total loss 1.18936467\n",
      "Trained batch 990 batch loss 1.13999796 epoch total loss 1.18931472\n",
      "Trained batch 991 batch loss 1.04588282 epoch total loss 1.18917\n",
      "Trained batch 992 batch loss 0.989507914 epoch total loss 1.18896878\n",
      "Trained batch 993 batch loss 1.19080591 epoch total loss 1.18897057\n",
      "Trained batch 994 batch loss 1.15778935 epoch total loss 1.18893933\n",
      "Trained batch 995 batch loss 1.17757857 epoch total loss 1.18892789\n",
      "Trained batch 996 batch loss 1.18027341 epoch total loss 1.18891931\n",
      "Trained batch 997 batch loss 1.22513545 epoch total loss 1.18895555\n",
      "Trained batch 998 batch loss 1.18425536 epoch total loss 1.18895078\n",
      "Trained batch 999 batch loss 1.12927246 epoch total loss 1.18889105\n",
      "Trained batch 1000 batch loss 0.997782707 epoch total loss 1.1887\n",
      "Trained batch 1001 batch loss 1.18092513 epoch total loss 1.18869221\n",
      "Trained batch 1002 batch loss 1.17531586 epoch total loss 1.18867874\n",
      "Trained batch 1003 batch loss 1.33806419 epoch total loss 1.18882763\n",
      "Trained batch 1004 batch loss 1.20833302 epoch total loss 1.18884718\n",
      "Trained batch 1005 batch loss 1.19684207 epoch total loss 1.18885517\n",
      "Trained batch 1006 batch loss 1.03982377 epoch total loss 1.18870699\n",
      "Trained batch 1007 batch loss 1.04201233 epoch total loss 1.18856132\n",
      "Trained batch 1008 batch loss 1.09495509 epoch total loss 1.18846846\n",
      "Trained batch 1009 batch loss 1.10883737 epoch total loss 1.18838954\n",
      "Trained batch 1010 batch loss 1.26519012 epoch total loss 1.1884656\n",
      "Trained batch 1011 batch loss 1.22907531 epoch total loss 1.18850577\n",
      "Trained batch 1012 batch loss 1.19372714 epoch total loss 1.18851089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1013 batch loss 1.2337873 epoch total loss 1.1885556\n",
      "Trained batch 1014 batch loss 1.20047152 epoch total loss 1.18856728\n",
      "Trained batch 1015 batch loss 1.17097759 epoch total loss 1.18855\n",
      "Trained batch 1016 batch loss 1.1793859 epoch total loss 1.18854105\n",
      "Trained batch 1017 batch loss 1.14651 epoch total loss 1.18849969\n",
      "Trained batch 1018 batch loss 1.20132887 epoch total loss 1.18851233\n",
      "Trained batch 1019 batch loss 1.29516 epoch total loss 1.18861699\n",
      "Trained batch 1020 batch loss 1.27820504 epoch total loss 1.18870473\n",
      "Trained batch 1021 batch loss 1.39061832 epoch total loss 1.1889025\n",
      "Trained batch 1022 batch loss 1.29283309 epoch total loss 1.1890043\n",
      "Trained batch 1023 batch loss 1.16126084 epoch total loss 1.18897712\n",
      "Trained batch 1024 batch loss 1.23778391 epoch total loss 1.18902481\n",
      "Trained batch 1025 batch loss 1.24772108 epoch total loss 1.18908203\n",
      "Trained batch 1026 batch loss 1.1477102 epoch total loss 1.18904173\n",
      "Trained batch 1027 batch loss 1.14179766 epoch total loss 1.18899572\n",
      "Trained batch 1028 batch loss 1.18329644 epoch total loss 1.18899024\n",
      "Trained batch 1029 batch loss 1.29237652 epoch total loss 1.18909073\n",
      "Trained batch 1030 batch loss 1.20124614 epoch total loss 1.18910253\n",
      "Trained batch 1031 batch loss 1.2124064 epoch total loss 1.18912518\n",
      "Trained batch 1032 batch loss 1.15508854 epoch total loss 1.18909216\n",
      "Trained batch 1033 batch loss 1.10959935 epoch total loss 1.18901515\n",
      "Trained batch 1034 batch loss 1.15173066 epoch total loss 1.18897915\n",
      "Trained batch 1035 batch loss 1.08995 epoch total loss 1.18888342\n",
      "Trained batch 1036 batch loss 1.0911938 epoch total loss 1.18878913\n",
      "Trained batch 1037 batch loss 1.0669421 epoch total loss 1.18867159\n",
      "Trained batch 1038 batch loss 1.01215851 epoch total loss 1.1885016\n",
      "Trained batch 1039 batch loss 1.09555078 epoch total loss 1.18841219\n",
      "Trained batch 1040 batch loss 1.14537883 epoch total loss 1.18837082\n",
      "Trained batch 1041 batch loss 1.24838614 epoch total loss 1.18842852\n",
      "Trained batch 1042 batch loss 1.23198545 epoch total loss 1.18847024\n",
      "Trained batch 1043 batch loss 1.24568725 epoch total loss 1.18852508\n",
      "Trained batch 1044 batch loss 1.2403208 epoch total loss 1.18857479\n",
      "Trained batch 1045 batch loss 1.11381614 epoch total loss 1.18850315\n",
      "Trained batch 1046 batch loss 1.2027055 epoch total loss 1.18851686\n",
      "Trained batch 1047 batch loss 1.19411993 epoch total loss 1.1885221\n",
      "Trained batch 1048 batch loss 1.02760279 epoch total loss 1.18836856\n",
      "Trained batch 1049 batch loss 1.1389966 epoch total loss 1.18832159\n",
      "Trained batch 1050 batch loss 1.30636322 epoch total loss 1.188434\n",
      "Trained batch 1051 batch loss 1.25561488 epoch total loss 1.1884979\n",
      "Trained batch 1052 batch loss 1.27896285 epoch total loss 1.18858385\n",
      "Trained batch 1053 batch loss 1.34936643 epoch total loss 1.18873656\n",
      "Trained batch 1054 batch loss 1.38983202 epoch total loss 1.18892741\n",
      "Trained batch 1055 batch loss 1.21453011 epoch total loss 1.18895161\n",
      "Trained batch 1056 batch loss 1.30503559 epoch total loss 1.18906164\n",
      "Trained batch 1057 batch loss 1.12620389 epoch total loss 1.18900216\n",
      "Trained batch 1058 batch loss 1.23326731 epoch total loss 1.189044\n",
      "Trained batch 1059 batch loss 1.16671491 epoch total loss 1.1890229\n",
      "Trained batch 1060 batch loss 1.0989089 epoch total loss 1.1889379\n",
      "Trained batch 1061 batch loss 1.03752875 epoch total loss 1.18879509\n",
      "Trained batch 1062 batch loss 1.06845093 epoch total loss 1.18868184\n",
      "Trained batch 1063 batch loss 1.23696923 epoch total loss 1.18872726\n",
      "Trained batch 1064 batch loss 1.19007289 epoch total loss 1.18872845\n",
      "Trained batch 1065 batch loss 1.21997118 epoch total loss 1.1887579\n",
      "Trained batch 1066 batch loss 1.12375581 epoch total loss 1.18869686\n",
      "Trained batch 1067 batch loss 1.1510148 epoch total loss 1.18866158\n",
      "Trained batch 1068 batch loss 1.2916472 epoch total loss 1.18875802\n",
      "Trained batch 1069 batch loss 1.12206292 epoch total loss 1.18869555\n",
      "Trained batch 1070 batch loss 1.11536 epoch total loss 1.188627\n",
      "Trained batch 1071 batch loss 1.00758767 epoch total loss 1.18845797\n",
      "Trained batch 1072 batch loss 1.06708634 epoch total loss 1.18834484\n",
      "Trained batch 1073 batch loss 1.02875924 epoch total loss 1.18819618\n",
      "Trained batch 1074 batch loss 1.02093554 epoch total loss 1.18804049\n",
      "Trained batch 1075 batch loss 1.14259231 epoch total loss 1.18799818\n",
      "Trained batch 1076 batch loss 1.09940469 epoch total loss 1.1879158\n",
      "Trained batch 1077 batch loss 1.04341209 epoch total loss 1.18778169\n",
      "Trained batch 1078 batch loss 1.16467881 epoch total loss 1.18776023\n",
      "Trained batch 1079 batch loss 1.09018219 epoch total loss 1.18766975\n",
      "Trained batch 1080 batch loss 1.07987738 epoch total loss 1.18757\n",
      "Trained batch 1081 batch loss 1.20408297 epoch total loss 1.18758523\n",
      "Trained batch 1082 batch loss 1.17259431 epoch total loss 1.18757141\n",
      "Trained batch 1083 batch loss 1.26793158 epoch total loss 1.18764567\n",
      "Trained batch 1084 batch loss 1.24682665 epoch total loss 1.18770027\n",
      "Trained batch 1085 batch loss 1.24000311 epoch total loss 1.18774843\n",
      "Trained batch 1086 batch loss 1.21320057 epoch total loss 1.18777192\n",
      "Trained batch 1087 batch loss 1.19794762 epoch total loss 1.18778133\n",
      "Trained batch 1088 batch loss 1.22268534 epoch total loss 1.1878134\n",
      "Trained batch 1089 batch loss 1.31756234 epoch total loss 1.18793249\n",
      "Trained batch 1090 batch loss 1.45851445 epoch total loss 1.18818069\n",
      "Trained batch 1091 batch loss 1.08789802 epoch total loss 1.18808877\n",
      "Trained batch 1092 batch loss 1.18374538 epoch total loss 1.18808472\n",
      "Trained batch 1093 batch loss 1.42017066 epoch total loss 1.18829703\n",
      "Trained batch 1094 batch loss 1.28750086 epoch total loss 1.18838775\n",
      "Trained batch 1095 batch loss 1.19379687 epoch total loss 1.18839276\n",
      "Trained batch 1096 batch loss 1.16323447 epoch total loss 1.18836975\n",
      "Trained batch 1097 batch loss 1.1772244 epoch total loss 1.18835962\n",
      "Trained batch 1098 batch loss 1.13808036 epoch total loss 1.18831384\n",
      "Trained batch 1099 batch loss 1.25167227 epoch total loss 1.18837154\n",
      "Trained batch 1100 batch loss 1.23164105 epoch total loss 1.18841088\n",
      "Trained batch 1101 batch loss 1.19556689 epoch total loss 1.18841732\n",
      "Trained batch 1102 batch loss 1.16784573 epoch total loss 1.18839872\n",
      "Trained batch 1103 batch loss 1.19025183 epoch total loss 1.18840039\n",
      "Trained batch 1104 batch loss 1.11144459 epoch total loss 1.18833077\n",
      "Trained batch 1105 batch loss 1.19273901 epoch total loss 1.1883347\n",
      "Trained batch 1106 batch loss 1.23083341 epoch total loss 1.18837309\n",
      "Trained batch 1107 batch loss 1.13818789 epoch total loss 1.18832779\n",
      "Trained batch 1108 batch loss 1.17621183 epoch total loss 1.18831694\n",
      "Trained batch 1109 batch loss 1.05010867 epoch total loss 1.18819225\n",
      "Trained batch 1110 batch loss 1.1334784 epoch total loss 1.1881429\n",
      "Trained batch 1111 batch loss 1.23467636 epoch total loss 1.18818474\n",
      "Trained batch 1112 batch loss 1.35476685 epoch total loss 1.18833447\n",
      "Trained batch 1113 batch loss 1.42461538 epoch total loss 1.18854678\n",
      "Trained batch 1114 batch loss 1.35659039 epoch total loss 1.18869758\n",
      "Trained batch 1115 batch loss 1.14293897 epoch total loss 1.18865657\n",
      "Trained batch 1116 batch loss 1.21269011 epoch total loss 1.18867803\n",
      "Trained batch 1117 batch loss 1.27362227 epoch total loss 1.1887542\n",
      "Trained batch 1118 batch loss 1.17912066 epoch total loss 1.1887455\n",
      "Trained batch 1119 batch loss 1.2461766 epoch total loss 1.18879688\n",
      "Trained batch 1120 batch loss 1.1819706 epoch total loss 1.1887908\n",
      "Trained batch 1121 batch loss 1.11453867 epoch total loss 1.18872452\n",
      "Trained batch 1122 batch loss 1.13259315 epoch total loss 1.18867445\n",
      "Trained batch 1123 batch loss 1.32898045 epoch total loss 1.18879938\n",
      "Trained batch 1124 batch loss 1.16162729 epoch total loss 1.18877518\n",
      "Trained batch 1125 batch loss 1.1300056 epoch total loss 1.18872297\n",
      "Trained batch 1126 batch loss 1.13899362 epoch total loss 1.18867886\n",
      "Trained batch 1127 batch loss 1.13215733 epoch total loss 1.18862879\n",
      "Trained batch 1128 batch loss 1.10393977 epoch total loss 1.18855357\n",
      "Trained batch 1129 batch loss 1.23932052 epoch total loss 1.18859863\n",
      "Trained batch 1130 batch loss 1.20571721 epoch total loss 1.18861377\n",
      "Trained batch 1131 batch loss 1.05769503 epoch total loss 1.18849802\n",
      "Trained batch 1132 batch loss 0.987899065 epoch total loss 1.18832088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1133 batch loss 0.981220126 epoch total loss 1.18813801\n",
      "Trained batch 1134 batch loss 1.13434052 epoch total loss 1.18809068\n",
      "Trained batch 1135 batch loss 1.18505502 epoch total loss 1.18808794\n",
      "Trained batch 1136 batch loss 1.26403379 epoch total loss 1.18815482\n",
      "Trained batch 1137 batch loss 1.41500974 epoch total loss 1.18835437\n",
      "Trained batch 1138 batch loss 1.37022161 epoch total loss 1.18851423\n",
      "Trained batch 1139 batch loss 1.25614643 epoch total loss 1.1885736\n",
      "Trained batch 1140 batch loss 1.21185708 epoch total loss 1.18859398\n",
      "Trained batch 1141 batch loss 1.17567253 epoch total loss 1.18858266\n",
      "Trained batch 1142 batch loss 1.06635511 epoch total loss 1.18847573\n",
      "Trained batch 1143 batch loss 1.07165325 epoch total loss 1.18837345\n",
      "Trained batch 1144 batch loss 1.26038194 epoch total loss 1.18843639\n",
      "Trained batch 1145 batch loss 1.22084701 epoch total loss 1.18846476\n",
      "Trained batch 1146 batch loss 1.15104592 epoch total loss 1.18843198\n",
      "Trained batch 1147 batch loss 1.12129474 epoch total loss 1.18837357\n",
      "Trained batch 1148 batch loss 1.23403454 epoch total loss 1.18841326\n",
      "Trained batch 1149 batch loss 1.1712631 epoch total loss 1.18839836\n",
      "Trained batch 1150 batch loss 1.17923021 epoch total loss 1.18839037\n",
      "Trained batch 1151 batch loss 1.30987024 epoch total loss 1.18849587\n",
      "Trained batch 1152 batch loss 1.1182816 epoch total loss 1.18843496\n",
      "Trained batch 1153 batch loss 1.23098755 epoch total loss 1.18847179\n",
      "Trained batch 1154 batch loss 1.07169175 epoch total loss 1.18837059\n",
      "Trained batch 1155 batch loss 0.991587639 epoch total loss 1.18820024\n",
      "Trained batch 1156 batch loss 1.0628109 epoch total loss 1.18809175\n",
      "Trained batch 1157 batch loss 1.07370973 epoch total loss 1.18799293\n",
      "Trained batch 1158 batch loss 1.06683445 epoch total loss 1.18788838\n",
      "Trained batch 1159 batch loss 0.998294353 epoch total loss 1.18772471\n",
      "Trained batch 1160 batch loss 1.01135731 epoch total loss 1.18757272\n",
      "Trained batch 1161 batch loss 1.15479684 epoch total loss 1.18754447\n",
      "Trained batch 1162 batch loss 1.12179768 epoch total loss 1.18748796\n",
      "Trained batch 1163 batch loss 1.18649936 epoch total loss 1.18748713\n",
      "Trained batch 1164 batch loss 1.26725054 epoch total loss 1.18755555\n",
      "Trained batch 1165 batch loss 1.03789139 epoch total loss 1.18742704\n",
      "Trained batch 1166 batch loss 1.18909216 epoch total loss 1.18742847\n",
      "Trained batch 1167 batch loss 1.15709162 epoch total loss 1.18740249\n",
      "Trained batch 1168 batch loss 1.10516143 epoch total loss 1.18733203\n",
      "Trained batch 1169 batch loss 1.1306622 epoch total loss 1.18728352\n",
      "Trained batch 1170 batch loss 1.28215921 epoch total loss 1.18736458\n",
      "Trained batch 1171 batch loss 1.05968761 epoch total loss 1.1872555\n",
      "Trained batch 1172 batch loss 1.19314849 epoch total loss 1.18726051\n",
      "Trained batch 1173 batch loss 1.16414285 epoch total loss 1.18724084\n",
      "Trained batch 1174 batch loss 1.10514379 epoch total loss 1.18717086\n",
      "Trained batch 1175 batch loss 1.08919406 epoch total loss 1.18708754\n",
      "Trained batch 1176 batch loss 1.10236192 epoch total loss 1.18701553\n",
      "Trained batch 1177 batch loss 1.19863701 epoch total loss 1.18702543\n",
      "Trained batch 1178 batch loss 1.16551614 epoch total loss 1.18700719\n",
      "Trained batch 1179 batch loss 1.2493732 epoch total loss 1.18706012\n",
      "Trained batch 1180 batch loss 1.20560467 epoch total loss 1.18707573\n",
      "Trained batch 1181 batch loss 1.104877 epoch total loss 1.18700612\n",
      "Trained batch 1182 batch loss 1.12408257 epoch total loss 1.18695283\n",
      "Trained batch 1183 batch loss 1.21750009 epoch total loss 1.1869787\n",
      "Trained batch 1184 batch loss 1.21767604 epoch total loss 1.18700457\n",
      "Trained batch 1185 batch loss 1.14884579 epoch total loss 1.18697238\n",
      "Trained batch 1186 batch loss 1.24884951 epoch total loss 1.18702459\n",
      "Trained batch 1187 batch loss 1.20213008 epoch total loss 1.18703735\n",
      "Trained batch 1188 batch loss 1.18976939 epoch total loss 1.18703961\n",
      "Trained batch 1189 batch loss 1.04680753 epoch total loss 1.18692172\n",
      "Trained batch 1190 batch loss 1.17364717 epoch total loss 1.18691063\n",
      "Trained batch 1191 batch loss 1.13617671 epoch total loss 1.18686807\n",
      "Trained batch 1192 batch loss 1.2447927 epoch total loss 1.18691659\n",
      "Trained batch 1193 batch loss 1.27200913 epoch total loss 1.18698788\n",
      "Trained batch 1194 batch loss 1.40135765 epoch total loss 1.18716741\n",
      "Trained batch 1195 batch loss 1.50155687 epoch total loss 1.1874305\n",
      "Trained batch 1196 batch loss 1.08164799 epoch total loss 1.18734205\n",
      "Trained batch 1197 batch loss 1.1279887 epoch total loss 1.18729246\n",
      "Trained batch 1198 batch loss 1.24885392 epoch total loss 1.18734396\n",
      "Trained batch 1199 batch loss 1.19317818 epoch total loss 1.18734884\n",
      "Trained batch 1200 batch loss 1.1224215 epoch total loss 1.18729472\n",
      "Trained batch 1201 batch loss 1.24438262 epoch total loss 1.18734229\n",
      "Trained batch 1202 batch loss 1.22661114 epoch total loss 1.18737483\n",
      "Trained batch 1203 batch loss 1.2431587 epoch total loss 1.1874212\n",
      "Trained batch 1204 batch loss 1.37382007 epoch total loss 1.18757606\n",
      "Trained batch 1205 batch loss 1.27728879 epoch total loss 1.18765056\n",
      "Trained batch 1206 batch loss 1.3700974 epoch total loss 1.18780184\n",
      "Trained batch 1207 batch loss 1.14014864 epoch total loss 1.18776238\n",
      "Trained batch 1208 batch loss 1.11476362 epoch total loss 1.18770194\n",
      "Trained batch 1209 batch loss 1.24843884 epoch total loss 1.18775213\n",
      "Trained batch 1210 batch loss 1.09114611 epoch total loss 1.18767226\n",
      "Trained batch 1211 batch loss 1.21792495 epoch total loss 1.18769729\n",
      "Trained batch 1212 batch loss 1.1856339 epoch total loss 1.18769562\n",
      "Trained batch 1213 batch loss 1.18666267 epoch total loss 1.18769479\n",
      "Trained batch 1214 batch loss 1.13375783 epoch total loss 1.18765032\n",
      "Trained batch 1215 batch loss 1.14463592 epoch total loss 1.18761492\n",
      "Trained batch 1216 batch loss 1.17785776 epoch total loss 1.18760693\n",
      "Trained batch 1217 batch loss 1.25938439 epoch total loss 1.18766594\n",
      "Trained batch 1218 batch loss 1.22187 epoch total loss 1.18769407\n",
      "Trained batch 1219 batch loss 1.14138579 epoch total loss 1.18765604\n",
      "Trained batch 1220 batch loss 1.04886413 epoch total loss 1.1875422\n",
      "Trained batch 1221 batch loss 1.22963536 epoch total loss 1.18757665\n",
      "Trained batch 1222 batch loss 1.21432984 epoch total loss 1.18759859\n",
      "Trained batch 1223 batch loss 1.25415349 epoch total loss 1.18765306\n",
      "Trained batch 1224 batch loss 1.04918551 epoch total loss 1.18753994\n",
      "Trained batch 1225 batch loss 1.00516212 epoch total loss 1.18739104\n",
      "Trained batch 1226 batch loss 1.17211115 epoch total loss 1.18737853\n",
      "Trained batch 1227 batch loss 1.09031034 epoch total loss 1.18729949\n",
      "Trained batch 1228 batch loss 1.0950489 epoch total loss 1.18722439\n",
      "Trained batch 1229 batch loss 1.10107636 epoch total loss 1.18715429\n",
      "Trained batch 1230 batch loss 1.16120076 epoch total loss 1.18713319\n",
      "Trained batch 1231 batch loss 1.27123332 epoch total loss 1.1872015\n",
      "Trained batch 1232 batch loss 1.26188612 epoch total loss 1.18726206\n",
      "Trained batch 1233 batch loss 1.14547396 epoch total loss 1.1872282\n",
      "Trained batch 1234 batch loss 1.27870238 epoch total loss 1.18730235\n",
      "Trained batch 1235 batch loss 1.43248713 epoch total loss 1.18750083\n",
      "Trained batch 1236 batch loss 1.34306121 epoch total loss 1.18762672\n",
      "Trained batch 1237 batch loss 1.24511707 epoch total loss 1.18767321\n",
      "Trained batch 1238 batch loss 1.06212699 epoch total loss 1.18757176\n",
      "Trained batch 1239 batch loss 1.0196172 epoch total loss 1.18743622\n",
      "Trained batch 1240 batch loss 1.1796906 epoch total loss 1.18743\n",
      "Trained batch 1241 batch loss 1.20226789 epoch total loss 1.18744195\n",
      "Trained batch 1242 batch loss 1.11360836 epoch total loss 1.18738258\n",
      "Trained batch 1243 batch loss 1.28674495 epoch total loss 1.18746245\n",
      "Trained batch 1244 batch loss 1.30384231 epoch total loss 1.18755603\n",
      "Trained batch 1245 batch loss 1.13396692 epoch total loss 1.18751299\n",
      "Trained batch 1246 batch loss 1.21426368 epoch total loss 1.18753433\n",
      "Trained batch 1247 batch loss 1.29458 epoch total loss 1.18762016\n",
      "Trained batch 1248 batch loss 1.15382409 epoch total loss 1.1875931\n",
      "Trained batch 1249 batch loss 1.18489325 epoch total loss 1.18759096\n",
      "Trained batch 1250 batch loss 1.20196211 epoch total loss 1.1876024\n",
      "Trained batch 1251 batch loss 1.28719664 epoch total loss 1.18768203\n",
      "Trained batch 1252 batch loss 1.22381008 epoch total loss 1.18771088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1253 batch loss 1.11752331 epoch total loss 1.18765485\n",
      "Trained batch 1254 batch loss 1.31164873 epoch total loss 1.1877538\n",
      "Trained batch 1255 batch loss 1.45197535 epoch total loss 1.18796432\n",
      "Trained batch 1256 batch loss 1.19603539 epoch total loss 1.18797076\n",
      "Trained batch 1257 batch loss 1.16293883 epoch total loss 1.18795085\n",
      "Trained batch 1258 batch loss 1.13871741 epoch total loss 1.18791175\n",
      "Trained batch 1259 batch loss 1.1683917 epoch total loss 1.18789613\n",
      "Trained batch 1260 batch loss 1.45769155 epoch total loss 1.18811023\n",
      "Trained batch 1261 batch loss 1.35370421 epoch total loss 1.1882416\n",
      "Trained batch 1262 batch loss 1.28990877 epoch total loss 1.18832219\n",
      "Trained batch 1263 batch loss 1.41206586 epoch total loss 1.18849933\n",
      "Trained batch 1264 batch loss 1.25708342 epoch total loss 1.18855357\n",
      "Trained batch 1265 batch loss 1.22775269 epoch total loss 1.18858469\n",
      "Trained batch 1266 batch loss 1.18432868 epoch total loss 1.18858123\n",
      "Trained batch 1267 batch loss 1.26359713 epoch total loss 1.18864048\n",
      "Trained batch 1268 batch loss 1.17020464 epoch total loss 1.18862593\n",
      "Trained batch 1269 batch loss 1.1975081 epoch total loss 1.18863285\n",
      "Trained batch 1270 batch loss 1.18533528 epoch total loss 1.18863022\n",
      "Trained batch 1271 batch loss 1.1336751 epoch total loss 1.18858707\n",
      "Trained batch 1272 batch loss 1.18216312 epoch total loss 1.18858194\n",
      "Trained batch 1273 batch loss 1.13700652 epoch total loss 1.18854141\n",
      "Trained batch 1274 batch loss 1.01698112 epoch total loss 1.18840671\n",
      "Trained batch 1275 batch loss 1.06819701 epoch total loss 1.18831241\n",
      "Trained batch 1276 batch loss 1.17488801 epoch total loss 1.18830192\n",
      "Trained batch 1277 batch loss 1.21281445 epoch total loss 1.18832111\n",
      "Trained batch 1278 batch loss 1.36928201 epoch total loss 1.18846273\n",
      "Trained batch 1279 batch loss 1.32995629 epoch total loss 1.18857336\n",
      "Trained batch 1280 batch loss 1.21002579 epoch total loss 1.18859017\n",
      "Trained batch 1281 batch loss 1.24307466 epoch total loss 1.18863261\n",
      "Trained batch 1282 batch loss 1.06324935 epoch total loss 1.18853486\n",
      "Trained batch 1283 batch loss 0.995348811 epoch total loss 1.18838429\n",
      "Trained batch 1284 batch loss 0.955182493 epoch total loss 1.18820262\n",
      "Trained batch 1285 batch loss 1.11370039 epoch total loss 1.18814468\n",
      "Trained batch 1286 batch loss 1.2553122 epoch total loss 1.1881969\n",
      "Trained batch 1287 batch loss 1.21413159 epoch total loss 1.18821704\n",
      "Trained batch 1288 batch loss 1.23764372 epoch total loss 1.18825543\n",
      "Trained batch 1289 batch loss 1.14440417 epoch total loss 1.18822145\n",
      "Trained batch 1290 batch loss 1.09198976 epoch total loss 1.18814683\n",
      "Trained batch 1291 batch loss 1.16557992 epoch total loss 1.18812931\n",
      "Trained batch 1292 batch loss 1.25937569 epoch total loss 1.1881845\n",
      "Trained batch 1293 batch loss 1.15209246 epoch total loss 1.1881566\n",
      "Trained batch 1294 batch loss 1.15973723 epoch total loss 1.18813467\n",
      "Trained batch 1295 batch loss 1.23972273 epoch total loss 1.18817449\n",
      "Trained batch 1296 batch loss 1.37067854 epoch total loss 1.18831539\n",
      "Trained batch 1297 batch loss 1.2006228 epoch total loss 1.18832493\n",
      "Trained batch 1298 batch loss 1.16737306 epoch total loss 1.18830884\n",
      "Trained batch 1299 batch loss 1.20928192 epoch total loss 1.18832493\n",
      "Trained batch 1300 batch loss 1.07422817 epoch total loss 1.18823707\n",
      "Trained batch 1301 batch loss 1.07283926 epoch total loss 1.1881485\n",
      "Trained batch 1302 batch loss 1.14134836 epoch total loss 1.1881125\n",
      "Trained batch 1303 batch loss 1.09171486 epoch total loss 1.18803847\n",
      "Trained batch 1304 batch loss 1.24416137 epoch total loss 1.1880815\n",
      "Trained batch 1305 batch loss 1.12461329 epoch total loss 1.18803287\n",
      "Trained batch 1306 batch loss 1.08712101 epoch total loss 1.18795562\n",
      "Trained batch 1307 batch loss 1.18089187 epoch total loss 1.18795025\n",
      "Trained batch 1308 batch loss 1.11954796 epoch total loss 1.18789792\n",
      "Trained batch 1309 batch loss 1.15087271 epoch total loss 1.18786967\n",
      "Trained batch 1310 batch loss 1.1146853 epoch total loss 1.18781388\n",
      "Trained batch 1311 batch loss 1.13175631 epoch total loss 1.18777108\n",
      "Trained batch 1312 batch loss 1.16714311 epoch total loss 1.18775535\n",
      "Trained batch 1313 batch loss 1.15130818 epoch total loss 1.18772757\n",
      "Trained batch 1314 batch loss 1.24790883 epoch total loss 1.18777335\n",
      "Trained batch 1315 batch loss 1.10065401 epoch total loss 1.18770719\n",
      "Trained batch 1316 batch loss 1.15179479 epoch total loss 1.18768\n",
      "Trained batch 1317 batch loss 1.21395934 epoch total loss 1.18769991\n",
      "Trained batch 1318 batch loss 1.20896125 epoch total loss 1.18771613\n",
      "Trained batch 1319 batch loss 1.16360283 epoch total loss 1.18769777\n",
      "Trained batch 1320 batch loss 1.2496227 epoch total loss 1.18774474\n",
      "Trained batch 1321 batch loss 1.05721736 epoch total loss 1.18764591\n",
      "Trained batch 1322 batch loss 1.17222857 epoch total loss 1.18763423\n",
      "Trained batch 1323 batch loss 1.1219883 epoch total loss 1.18758464\n",
      "Trained batch 1324 batch loss 1.23009062 epoch total loss 1.18761671\n",
      "Trained batch 1325 batch loss 1.25214839 epoch total loss 1.18766546\n",
      "Trained batch 1326 batch loss 1.17245615 epoch total loss 1.18765402\n",
      "Trained batch 1327 batch loss 1.28982759 epoch total loss 1.18773103\n",
      "Trained batch 1328 batch loss 1.17955756 epoch total loss 1.18772483\n",
      "Trained batch 1329 batch loss 1.30175841 epoch total loss 1.18781066\n",
      "Trained batch 1330 batch loss 1.19749 epoch total loss 1.18781793\n",
      "Trained batch 1331 batch loss 1.21885645 epoch total loss 1.1878413\n",
      "Trained batch 1332 batch loss 1.13956904 epoch total loss 1.18780494\n",
      "Trained batch 1333 batch loss 1.18098283 epoch total loss 1.18779993\n",
      "Trained batch 1334 batch loss 1.22193551 epoch total loss 1.18782544\n",
      "Trained batch 1335 batch loss 1.26757169 epoch total loss 1.18788528\n",
      "Trained batch 1336 batch loss 1.21311891 epoch total loss 1.18790412\n",
      "Trained batch 1337 batch loss 1.11647832 epoch total loss 1.18785071\n",
      "Trained batch 1338 batch loss 1.01121569 epoch total loss 1.18771863\n",
      "Trained batch 1339 batch loss 0.994105458 epoch total loss 1.18757415\n",
      "Trained batch 1340 batch loss 1.04611146 epoch total loss 1.18746853\n",
      "Trained batch 1341 batch loss 1.06263351 epoch total loss 1.18737543\n",
      "Trained batch 1342 batch loss 1.13352823 epoch total loss 1.18733537\n",
      "Trained batch 1343 batch loss 1.02229881 epoch total loss 1.18721247\n",
      "Trained batch 1344 batch loss 1.02589774 epoch total loss 1.18709242\n",
      "Trained batch 1345 batch loss 1.08404052 epoch total loss 1.18701577\n",
      "Trained batch 1346 batch loss 1.12919605 epoch total loss 1.18697286\n",
      "Trained batch 1347 batch loss 1.17295337 epoch total loss 1.18696237\n",
      "Trained batch 1348 batch loss 1.19987142 epoch total loss 1.18697202\n",
      "Trained batch 1349 batch loss 1.2656951 epoch total loss 1.18703032\n",
      "Trained batch 1350 batch loss 1.40451574 epoch total loss 1.18719149\n",
      "Trained batch 1351 batch loss 1.34367335 epoch total loss 1.18730724\n",
      "Trained batch 1352 batch loss 1.24468148 epoch total loss 1.18734968\n",
      "Trained batch 1353 batch loss 1.19817591 epoch total loss 1.18735766\n",
      "Trained batch 1354 batch loss 0.922992051 epoch total loss 1.1871624\n",
      "Trained batch 1355 batch loss 1.007092 epoch total loss 1.18702948\n",
      "Trained batch 1356 batch loss 1.07279646 epoch total loss 1.1869452\n",
      "Trained batch 1357 batch loss 0.983520389 epoch total loss 1.18679523\n",
      "Trained batch 1358 batch loss 0.910777628 epoch total loss 1.18659198\n",
      "Trained batch 1359 batch loss 0.930700779 epoch total loss 1.18640375\n",
      "Trained batch 1360 batch loss 1.01372981 epoch total loss 1.18627667\n",
      "Trained batch 1361 batch loss 1.06966567 epoch total loss 1.18619108\n",
      "Trained batch 1362 batch loss 1.01631415 epoch total loss 1.18606639\n",
      "Trained batch 1363 batch loss 1.19181299 epoch total loss 1.18607056\n",
      "Trained batch 1364 batch loss 1.15438771 epoch total loss 1.18604732\n",
      "Trained batch 1365 batch loss 1.18673658 epoch total loss 1.18604791\n",
      "Trained batch 1366 batch loss 1.17233336 epoch total loss 1.18603778\n",
      "Trained batch 1367 batch loss 1.10068417 epoch total loss 1.18597543\n",
      "Trained batch 1368 batch loss 1.13159966 epoch total loss 1.18593562\n",
      "Trained batch 1369 batch loss 1.11420774 epoch total loss 1.18588328\n",
      "Trained batch 1370 batch loss 1.13030338 epoch total loss 1.18584275\n",
      "Trained batch 1371 batch loss 1.10477519 epoch total loss 1.18578351\n",
      "Trained batch 1372 batch loss 1.25586438 epoch total loss 1.18583465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1373 batch loss 1.25434756 epoch total loss 1.18588459\n",
      "Trained batch 1374 batch loss 1.21049523 epoch total loss 1.18590248\n",
      "Trained batch 1375 batch loss 1.24786389 epoch total loss 1.18594754\n",
      "Trained batch 1376 batch loss 1.25163507 epoch total loss 1.18599522\n",
      "Trained batch 1377 batch loss 1.13333201 epoch total loss 1.18595695\n",
      "Trained batch 1378 batch loss 1.11565387 epoch total loss 1.18590593\n",
      "Trained batch 1379 batch loss 1.04408288 epoch total loss 1.18580306\n",
      "Trained batch 1380 batch loss 1.15003157 epoch total loss 1.18577719\n",
      "Trained batch 1381 batch loss 1.16299236 epoch total loss 1.18576062\n",
      "Trained batch 1382 batch loss 1.35213017 epoch total loss 1.18588102\n",
      "Trained batch 1383 batch loss 1.28150213 epoch total loss 1.18595016\n",
      "Trained batch 1384 batch loss 1.23852706 epoch total loss 1.18598819\n",
      "Trained batch 1385 batch loss 1.16577685 epoch total loss 1.18597353\n",
      "Trained batch 1386 batch loss 1.12062645 epoch total loss 1.18592644\n",
      "Trained batch 1387 batch loss 1.12826252 epoch total loss 1.18588483\n",
      "Trained batch 1388 batch loss 1.00562418 epoch total loss 1.18575501\n",
      "Epoch 5 train loss 1.1857550144195557\n",
      "Validated batch 1 batch loss 1.14439964\n",
      "Validated batch 2 batch loss 1.1636728\n",
      "Validated batch 3 batch loss 1.15012944\n",
      "Validated batch 4 batch loss 1.14127374\n",
      "Validated batch 5 batch loss 1.13781965\n",
      "Validated batch 6 batch loss 1.27538514\n",
      "Validated batch 7 batch loss 1.20017993\n",
      "Validated batch 8 batch loss 1.00118136\n",
      "Validated batch 9 batch loss 1.1282028\n",
      "Validated batch 10 batch loss 1.1874789\n",
      "Validated batch 11 batch loss 1.11034203\n",
      "Validated batch 12 batch loss 1.15219021\n",
      "Validated batch 13 batch loss 1.19974709\n",
      "Validated batch 14 batch loss 1.12309229\n",
      "Validated batch 15 batch loss 1.35156488\n",
      "Validated batch 16 batch loss 1.25354958\n",
      "Validated batch 17 batch loss 1.19428313\n",
      "Validated batch 18 batch loss 1.2928201\n",
      "Validated batch 19 batch loss 1.04776108\n",
      "Validated batch 20 batch loss 1.16659069\n",
      "Validated batch 21 batch loss 1.06088829\n",
      "Validated batch 22 batch loss 1.2231282\n",
      "Validated batch 23 batch loss 1.33086038\n",
      "Validated batch 24 batch loss 1.16041422\n",
      "Validated batch 25 batch loss 1.1085434\n",
      "Validated batch 26 batch loss 1.15850806\n",
      "Validated batch 27 batch loss 1.14450574\n",
      "Validated batch 28 batch loss 1.18110812\n",
      "Validated batch 29 batch loss 1.2406888\n",
      "Validated batch 30 batch loss 1.24438918\n",
      "Validated batch 31 batch loss 1.14866793\n",
      "Validated batch 32 batch loss 1.1940943\n",
      "Validated batch 33 batch loss 1.19772971\n",
      "Validated batch 34 batch loss 1.23479152\n",
      "Validated batch 35 batch loss 1.20597935\n",
      "Validated batch 36 batch loss 1.16400445\n",
      "Validated batch 37 batch loss 1.20020986\n",
      "Validated batch 38 batch loss 1.23556066\n",
      "Validated batch 39 batch loss 1.15531838\n",
      "Validated batch 40 batch loss 1.33055043\n",
      "Validated batch 41 batch loss 1.2887603\n",
      "Validated batch 42 batch loss 1.08637261\n",
      "Validated batch 43 batch loss 1.2940433\n",
      "Validated batch 44 batch loss 1.15375543\n",
      "Validated batch 45 batch loss 1.13423359\n",
      "Validated batch 46 batch loss 1.21472144\n",
      "Validated batch 47 batch loss 1.3179909\n",
      "Validated batch 48 batch loss 1.23685479\n",
      "Validated batch 49 batch loss 1.17089\n",
      "Validated batch 50 batch loss 1.15063143\n",
      "Validated batch 51 batch loss 1.12062216\n",
      "Validated batch 52 batch loss 1.22757578\n",
      "Validated batch 53 batch loss 1.27347445\n",
      "Validated batch 54 batch loss 1.07438862\n",
      "Validated batch 55 batch loss 1.25286269\n",
      "Validated batch 56 batch loss 1.20931\n",
      "Validated batch 57 batch loss 1.24546051\n",
      "Validated batch 58 batch loss 1.27193236\n",
      "Validated batch 59 batch loss 1.23031759\n",
      "Validated batch 60 batch loss 1.13648248\n",
      "Validated batch 61 batch loss 1.17288899\n",
      "Validated batch 62 batch loss 1.21282029\n",
      "Validated batch 63 batch loss 1.20329523\n",
      "Validated batch 64 batch loss 1.23428345\n",
      "Validated batch 65 batch loss 1.2482779\n",
      "Validated batch 66 batch loss 1.49386477\n",
      "Validated batch 67 batch loss 1.27395952\n",
      "Validated batch 68 batch loss 1.22841275\n",
      "Validated batch 69 batch loss 1.08622241\n",
      "Validated batch 70 batch loss 1.13735533\n",
      "Validated batch 71 batch loss 1.12156928\n",
      "Validated batch 72 batch loss 1.2111181\n",
      "Validated batch 73 batch loss 1.11842239\n",
      "Validated batch 74 batch loss 1.1546129\n",
      "Validated batch 75 batch loss 1.25393772\n",
      "Validated batch 76 batch loss 1.2426672\n",
      "Validated batch 77 batch loss 1.31931829\n",
      "Validated batch 78 batch loss 1.26610494\n",
      "Validated batch 79 batch loss 1.19787598\n",
      "Validated batch 80 batch loss 1.24945283\n",
      "Validated batch 81 batch loss 1.38612628\n",
      "Validated batch 82 batch loss 1.27159858\n",
      "Validated batch 83 batch loss 1.33355069\n",
      "Validated batch 84 batch loss 1.3381424\n",
      "Validated batch 85 batch loss 1.28545713\n",
      "Validated batch 86 batch loss 1.30580556\n",
      "Validated batch 87 batch loss 1.14854622\n",
      "Validated batch 88 batch loss 1.20578289\n",
      "Validated batch 89 batch loss 1.28987\n",
      "Validated batch 90 batch loss 1.28615701\n",
      "Validated batch 91 batch loss 1.18967652\n",
      "Validated batch 92 batch loss 1.21036768\n",
      "Validated batch 93 batch loss 1.27792525\n",
      "Validated batch 94 batch loss 1.22909701\n",
      "Validated batch 95 batch loss 1.19147706\n",
      "Validated batch 96 batch loss 1.18893433\n",
      "Validated batch 97 batch loss 1.23289418\n",
      "Validated batch 98 batch loss 1.42421198\n",
      "Validated batch 99 batch loss 1.13199902\n",
      "Validated batch 100 batch loss 1.25126016\n",
      "Validated batch 101 batch loss 1.189448\n",
      "Validated batch 102 batch loss 1.28226674\n",
      "Validated batch 103 batch loss 1.21366978\n",
      "Validated batch 104 batch loss 1.13160264\n",
      "Validated batch 105 batch loss 1.31041074\n",
      "Validated batch 106 batch loss 1.25573421\n",
      "Validated batch 107 batch loss 1.27940512\n",
      "Validated batch 108 batch loss 1.2358911\n",
      "Validated batch 109 batch loss 1.30097497\n",
      "Validated batch 110 batch loss 1.11619377\n",
      "Validated batch 111 batch loss 1.19376493\n",
      "Validated batch 112 batch loss 1.19919074\n",
      "Validated batch 113 batch loss 1.13428307\n",
      "Validated batch 114 batch loss 1.25760889\n",
      "Validated batch 115 batch loss 1.21500731\n",
      "Validated batch 116 batch loss 1.17379427\n",
      "Validated batch 117 batch loss 1.17378\n",
      "Validated batch 118 batch loss 1.27214515\n",
      "Validated batch 119 batch loss 1.13525021\n",
      "Validated batch 120 batch loss 1.25693929\n",
      "Validated batch 121 batch loss 1.41453755\n",
      "Validated batch 122 batch loss 1.0823431\n",
      "Validated batch 123 batch loss 1.22857893\n",
      "Validated batch 124 batch loss 1.19109368\n",
      "Validated batch 125 batch loss 1.25199187\n",
      "Validated batch 126 batch loss 1.26804733\n",
      "Validated batch 127 batch loss 1.13539124\n",
      "Validated batch 128 batch loss 0.950132132\n",
      "Validated batch 129 batch loss 1.22614455\n",
      "Validated batch 130 batch loss 1.17755949\n",
      "Validated batch 131 batch loss 1.17866445\n",
      "Validated batch 132 batch loss 1.27579463\n",
      "Validated batch 133 batch loss 1.09928536\n",
      "Validated batch 134 batch loss 1.2291348\n",
      "Validated batch 135 batch loss 1.29882908\n",
      "Validated batch 136 batch loss 1.26990891\n",
      "Validated batch 137 batch loss 1.2508837\n",
      "Validated batch 138 batch loss 1.07158422\n",
      "Validated batch 139 batch loss 1.23943758\n",
      "Validated batch 140 batch loss 1.21705389\n",
      "Validated batch 141 batch loss 1.14582455\n",
      "Validated batch 142 batch loss 1.1541357\n",
      "Validated batch 143 batch loss 1.22985089\n",
      "Validated batch 144 batch loss 1.19913554\n",
      "Validated batch 145 batch loss 1.19537771\n",
      "Validated batch 146 batch loss 1.21902323\n",
      "Validated batch 147 batch loss 1.10934973\n",
      "Validated batch 148 batch loss 1.35534585\n",
      "Validated batch 149 batch loss 1.14699507\n",
      "Validated batch 150 batch loss 1.0687871\n",
      "Validated batch 151 batch loss 1.17765164\n",
      "Validated batch 152 batch loss 1.37055707\n",
      "Validated batch 153 batch loss 1.27136648\n",
      "Validated batch 154 batch loss 1.38925767\n",
      "Validated batch 155 batch loss 1.16072965\n",
      "Validated batch 156 batch loss 1.37666512\n",
      "Validated batch 157 batch loss 1.16596472\n",
      "Validated batch 158 batch loss 1.28144717\n",
      "Validated batch 159 batch loss 1.25829303\n",
      "Validated batch 160 batch loss 0.949426234\n",
      "Validated batch 161 batch loss 1.0715524\n",
      "Validated batch 162 batch loss 1.19594741\n",
      "Validated batch 163 batch loss 1.22868741\n",
      "Validated batch 164 batch loss 1.23617709\n",
      "Validated batch 165 batch loss 1.12826884\n",
      "Validated batch 166 batch loss 1.19452405\n",
      "Validated batch 167 batch loss 1.23850632\n",
      "Validated batch 168 batch loss 1.24832606\n",
      "Validated batch 169 batch loss 1.32764554\n",
      "Validated batch 170 batch loss 1.26495063\n",
      "Validated batch 171 batch loss 1.17401195\n",
      "Validated batch 172 batch loss 1.2202853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 173 batch loss 1.24863064\n",
      "Validated batch 174 batch loss 1.19209898\n",
      "Validated batch 175 batch loss 1.31927586\n",
      "Validated batch 176 batch loss 1.3856585\n",
      "Validated batch 177 batch loss 1.18892288\n",
      "Validated batch 178 batch loss 1.31269228\n",
      "Validated batch 179 batch loss 1.22387576\n",
      "Validated batch 180 batch loss 1.14088094\n",
      "Validated batch 181 batch loss 1.22210479\n",
      "Validated batch 182 batch loss 1.08010316\n",
      "Validated batch 183 batch loss 1.27810752\n",
      "Validated batch 184 batch loss 1.15118444\n",
      "Validated batch 185 batch loss 1.25828934\n",
      "Epoch 5 val loss 1.2117691040039062\n",
      "Start epoch 6 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.34837627 epoch total loss 1.34837627\n",
      "Trained batch 2 batch loss 1.22110009 epoch total loss 1.28473818\n",
      "Trained batch 3 batch loss 1.08410263 epoch total loss 1.21785963\n",
      "Trained batch 4 batch loss 1.02915418 epoch total loss 1.17068326\n",
      "Trained batch 5 batch loss 1.09732068 epoch total loss 1.15601075\n",
      "Trained batch 6 batch loss 1.11835063 epoch total loss 1.14973402\n",
      "Trained batch 7 batch loss 1.04743063 epoch total loss 1.1351192\n",
      "Trained batch 8 batch loss 1.23073769 epoch total loss 1.1470716\n",
      "Trained batch 9 batch loss 1.09738815 epoch total loss 1.14155126\n",
      "Trained batch 10 batch loss 1.23393345 epoch total loss 1.1507895\n",
      "Trained batch 11 batch loss 1.15747643 epoch total loss 1.15139735\n",
      "Trained batch 12 batch loss 1.1631763 epoch total loss 1.15237892\n",
      "Trained batch 13 batch loss 1.30736 epoch total loss 1.16430056\n",
      "Trained batch 14 batch loss 1.40531993 epoch total loss 1.18151629\n",
      "Trained batch 15 batch loss 1.42681074 epoch total loss 1.19786918\n",
      "Trained batch 16 batch loss 1.33873332 epoch total loss 1.20667326\n",
      "Trained batch 17 batch loss 1.13881075 epoch total loss 1.20268142\n",
      "Trained batch 18 batch loss 1.09452415 epoch total loss 1.19667268\n",
      "Trained batch 19 batch loss 1.27645779 epoch total loss 1.20087194\n",
      "Trained batch 20 batch loss 1.25205564 epoch total loss 1.20343113\n",
      "Trained batch 21 batch loss 1.32829618 epoch total loss 1.20937705\n",
      "Trained batch 22 batch loss 1.32705104 epoch total loss 1.21472597\n",
      "Trained batch 23 batch loss 1.29952 epoch total loss 1.21841264\n",
      "Trained batch 24 batch loss 1.23894536 epoch total loss 1.2192682\n",
      "Trained batch 25 batch loss 1.23065627 epoch total loss 1.2197237\n",
      "Trained batch 26 batch loss 1.24810934 epoch total loss 1.22081542\n",
      "Trained batch 27 batch loss 1.25639045 epoch total loss 1.22213304\n",
      "Trained batch 28 batch loss 1.13194942 epoch total loss 1.21891224\n",
      "Trained batch 29 batch loss 1.17800486 epoch total loss 1.21750164\n",
      "Trained batch 30 batch loss 1.05972779 epoch total loss 1.21224248\n",
      "Trained batch 31 batch loss 1.07429671 epoch total loss 1.20779264\n",
      "Trained batch 32 batch loss 0.961944222 epoch total loss 1.20010984\n",
      "Trained batch 33 batch loss 1.03072894 epoch total loss 1.19497705\n",
      "Trained batch 34 batch loss 1.10637057 epoch total loss 1.19237089\n",
      "Trained batch 35 batch loss 0.889780402 epoch total loss 1.18372548\n",
      "Trained batch 36 batch loss 0.946927726 epoch total loss 1.17714775\n",
      "Trained batch 37 batch loss 0.833299637 epoch total loss 1.16785455\n",
      "Trained batch 38 batch loss 0.960704803 epoch total loss 1.16240323\n",
      "Trained batch 39 batch loss 1.09171033 epoch total loss 1.16059053\n",
      "Trained batch 40 batch loss 1.14513326 epoch total loss 1.16020417\n",
      "Trained batch 41 batch loss 1.10022068 epoch total loss 1.15874112\n",
      "Trained batch 42 batch loss 1.11508751 epoch total loss 1.15770173\n",
      "Trained batch 43 batch loss 1.09891844 epoch total loss 1.15633476\n",
      "Trained batch 44 batch loss 1.16601324 epoch total loss 1.1565547\n",
      "Trained batch 45 batch loss 1.09339499 epoch total loss 1.15515113\n",
      "Trained batch 46 batch loss 1.09829175 epoch total loss 1.15391505\n",
      "Trained batch 47 batch loss 1.21209979 epoch total loss 1.15515304\n",
      "Trained batch 48 batch loss 1.13281703 epoch total loss 1.15468776\n",
      "Trained batch 49 batch loss 1.246387 epoch total loss 1.15655911\n",
      "Trained batch 50 batch loss 1.19827366 epoch total loss 1.15739346\n",
      "Trained batch 51 batch loss 1.2187413 epoch total loss 1.1585964\n",
      "Trained batch 52 batch loss 1.18852174 epoch total loss 1.15917182\n",
      "Trained batch 53 batch loss 1.27601051 epoch total loss 1.16137636\n",
      "Trained batch 54 batch loss 1.19052529 epoch total loss 1.16191614\n",
      "Trained batch 55 batch loss 1.10224843 epoch total loss 1.16083133\n",
      "Trained batch 56 batch loss 1.14026594 epoch total loss 1.16046405\n",
      "Trained batch 57 batch loss 1.08472252 epoch total loss 1.15913522\n",
      "Trained batch 58 batch loss 1.07733774 epoch total loss 1.15772498\n",
      "Trained batch 59 batch loss 1.03033924 epoch total loss 1.15556598\n",
      "Trained batch 60 batch loss 1.2101419 epoch total loss 1.15647554\n",
      "Trained batch 61 batch loss 1.00722742 epoch total loss 1.15402889\n",
      "Trained batch 62 batch loss 1.10804057 epoch total loss 1.15328705\n",
      "Trained batch 63 batch loss 1.08545554 epoch total loss 1.15221047\n",
      "Trained batch 64 batch loss 0.982739747 epoch total loss 1.14956248\n",
      "Trained batch 65 batch loss 1.11089289 epoch total loss 1.14896762\n",
      "Trained batch 66 batch loss 1.1697166 epoch total loss 1.14928198\n",
      "Trained batch 67 batch loss 1.01423955 epoch total loss 1.14726639\n",
      "Trained batch 68 batch loss 1.06912982 epoch total loss 1.14611721\n",
      "Trained batch 69 batch loss 0.995148301 epoch total loss 1.14392924\n",
      "Trained batch 70 batch loss 1.11183 epoch total loss 1.14347076\n",
      "Trained batch 71 batch loss 0.970151186 epoch total loss 1.14102972\n",
      "Trained batch 72 batch loss 0.944705 epoch total loss 1.13830292\n",
      "Trained batch 73 batch loss 1.093413 epoch total loss 1.13768804\n",
      "Trained batch 74 batch loss 0.957133651 epoch total loss 1.13524806\n",
      "Trained batch 75 batch loss 1.05150151 epoch total loss 1.13413131\n",
      "Trained batch 76 batch loss 1.03221333 epoch total loss 1.13279033\n",
      "Trained batch 77 batch loss 1.15966201 epoch total loss 1.13313925\n",
      "Trained batch 78 batch loss 1.00770915 epoch total loss 1.13153112\n",
      "Trained batch 79 batch loss 1.45208418 epoch total loss 1.13558888\n",
      "Trained batch 80 batch loss 1.24907601 epoch total loss 1.13700747\n",
      "Trained batch 81 batch loss 1.00496757 epoch total loss 1.13537729\n",
      "Trained batch 82 batch loss 1.40046799 epoch total loss 1.13861012\n",
      "Trained batch 83 batch loss 1.21413636 epoch total loss 1.13952\n",
      "Trained batch 84 batch loss 1.19598055 epoch total loss 1.14019227\n",
      "Trained batch 85 batch loss 1.29659557 epoch total loss 1.14203227\n",
      "Trained batch 86 batch loss 1.08067107 epoch total loss 1.1413188\n",
      "Trained batch 87 batch loss 1.09233451 epoch total loss 1.14075565\n",
      "Trained batch 88 batch loss 1.06494379 epoch total loss 1.13989413\n",
      "Trained batch 89 batch loss 1.14601326 epoch total loss 1.13996291\n",
      "Trained batch 90 batch loss 1.06908548 epoch total loss 1.1391753\n",
      "Trained batch 91 batch loss 1.11974573 epoch total loss 1.13896179\n",
      "Trained batch 92 batch loss 1.062814 epoch total loss 1.13813412\n",
      "Trained batch 93 batch loss 1.15045786 epoch total loss 1.13826656\n",
      "Trained batch 94 batch loss 1.11088765 epoch total loss 1.13797534\n",
      "Trained batch 95 batch loss 1.08878469 epoch total loss 1.13745749\n",
      "Trained batch 96 batch loss 0.945433557 epoch total loss 1.13545728\n",
      "Trained batch 97 batch loss 0.908061147 epoch total loss 1.13311291\n",
      "Trained batch 98 batch loss 1.11067986 epoch total loss 1.13288403\n",
      "Trained batch 99 batch loss 1.15449393 epoch total loss 1.1331023\n",
      "Trained batch 100 batch loss 1.20175719 epoch total loss 1.13378894\n",
      "Trained batch 101 batch loss 1.11575186 epoch total loss 1.13361037\n",
      "Trained batch 102 batch loss 1.22419429 epoch total loss 1.13449848\n",
      "Trained batch 103 batch loss 1.10487342 epoch total loss 1.13421082\n",
      "Trained batch 104 batch loss 1.01578403 epoch total loss 1.13307214\n",
      "Trained batch 105 batch loss 1.04875541 epoch total loss 1.13226902\n",
      "Trained batch 106 batch loss 1.19783449 epoch total loss 1.1328876\n",
      "Trained batch 107 batch loss 1.1710633 epoch total loss 1.1332444\n",
      "Trained batch 108 batch loss 1.09435856 epoch total loss 1.13288438\n",
      "Trained batch 109 batch loss 1.13462472 epoch total loss 1.13290036\n",
      "Trained batch 110 batch loss 1.19665 epoch total loss 1.13348\n",
      "Trained batch 111 batch loss 1.22249901 epoch total loss 1.13428187\n",
      "Trained batch 112 batch loss 1.02387416 epoch total loss 1.13329613\n",
      "Trained batch 113 batch loss 1.01128304 epoch total loss 1.13221633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 114 batch loss 1.11259878 epoch total loss 1.1320442\n",
      "Trained batch 115 batch loss 1.14312112 epoch total loss 1.13214064\n",
      "Trained batch 116 batch loss 1.30771 epoch total loss 1.13365412\n",
      "Trained batch 117 batch loss 1.12441945 epoch total loss 1.1335752\n",
      "Trained batch 118 batch loss 1.12859297 epoch total loss 1.13353288\n",
      "Trained batch 119 batch loss 1.13858247 epoch total loss 1.13357532\n",
      "Trained batch 120 batch loss 1.12973571 epoch total loss 1.13354325\n",
      "Trained batch 121 batch loss 1.1987468 epoch total loss 1.13408208\n",
      "Trained batch 122 batch loss 1.20323014 epoch total loss 1.13464892\n",
      "Trained batch 123 batch loss 1.18489289 epoch total loss 1.13505745\n",
      "Trained batch 124 batch loss 1.17867303 epoch total loss 1.13540924\n",
      "Trained batch 125 batch loss 1.16923356 epoch total loss 1.13567984\n",
      "Trained batch 126 batch loss 1.04090226 epoch total loss 1.13492763\n",
      "Trained batch 127 batch loss 1.11248505 epoch total loss 1.13475096\n",
      "Trained batch 128 batch loss 1.17569733 epoch total loss 1.1350708\n",
      "Trained batch 129 batch loss 1.1620338 epoch total loss 1.13527977\n",
      "Trained batch 130 batch loss 1.11483192 epoch total loss 1.13512254\n",
      "Trained batch 131 batch loss 1.19573045 epoch total loss 1.13558519\n",
      "Trained batch 132 batch loss 1.04159474 epoch total loss 1.13487315\n",
      "Trained batch 133 batch loss 1.28888083 epoch total loss 1.13603103\n",
      "Trained batch 134 batch loss 1.35272074 epoch total loss 1.13764822\n",
      "Trained batch 135 batch loss 1.0857029 epoch total loss 1.13726342\n",
      "Trained batch 136 batch loss 1.03524792 epoch total loss 1.13651335\n",
      "Trained batch 137 batch loss 1.12682855 epoch total loss 1.13644266\n",
      "Trained batch 138 batch loss 0.996468365 epoch total loss 1.13542843\n",
      "Trained batch 139 batch loss 1.08913541 epoch total loss 1.13509536\n",
      "Trained batch 140 batch loss 1.15208912 epoch total loss 1.13521671\n",
      "Trained batch 141 batch loss 1.2297442 epoch total loss 1.13588715\n",
      "Trained batch 142 batch loss 0.967606068 epoch total loss 1.13470209\n",
      "Trained batch 143 batch loss 1.04922462 epoch total loss 1.13410437\n",
      "Trained batch 144 batch loss 1.10289013 epoch total loss 1.13388765\n",
      "Trained batch 145 batch loss 1.15291977 epoch total loss 1.1340189\n",
      "Trained batch 146 batch loss 1.27353275 epoch total loss 1.13497448\n",
      "Trained batch 147 batch loss 1.23112845 epoch total loss 1.13562858\n",
      "Trained batch 148 batch loss 1.08874214 epoch total loss 1.13531172\n",
      "Trained batch 149 batch loss 0.957867205 epoch total loss 1.13412082\n",
      "Trained batch 150 batch loss 0.799184501 epoch total loss 1.13188791\n",
      "Trained batch 151 batch loss 0.84479326 epoch total loss 1.12998664\n",
      "Trained batch 152 batch loss 1.06226015 epoch total loss 1.12954104\n",
      "Trained batch 153 batch loss 1.39435542 epoch total loss 1.13127172\n",
      "Trained batch 154 batch loss 1.05174637 epoch total loss 1.13075531\n",
      "Trained batch 155 batch loss 1.20641923 epoch total loss 1.13124347\n",
      "Trained batch 156 batch loss 1.31086493 epoch total loss 1.13239491\n",
      "Trained batch 157 batch loss 1.2783159 epoch total loss 1.13332438\n",
      "Trained batch 158 batch loss 1.15486825 epoch total loss 1.13346076\n",
      "Trained batch 159 batch loss 1.17828333 epoch total loss 1.13374257\n",
      "Trained batch 160 batch loss 1.188416 epoch total loss 1.13408434\n",
      "Trained batch 161 batch loss 1.11482477 epoch total loss 1.13396466\n",
      "Trained batch 162 batch loss 1.24615669 epoch total loss 1.13465726\n",
      "Trained batch 163 batch loss 1.2005738 epoch total loss 1.13506162\n",
      "Trained batch 164 batch loss 1.19874334 epoch total loss 1.13544989\n",
      "Trained batch 165 batch loss 1.2870208 epoch total loss 1.13636851\n",
      "Trained batch 166 batch loss 1.20575547 epoch total loss 1.13678646\n",
      "Trained batch 167 batch loss 1.36034131 epoch total loss 1.13812506\n",
      "Trained batch 168 batch loss 1.02981818 epoch total loss 1.13748038\n",
      "Trained batch 169 batch loss 1.01134968 epoch total loss 1.13673413\n",
      "Trained batch 170 batch loss 1.07336605 epoch total loss 1.13636136\n",
      "Trained batch 171 batch loss 1.29380715 epoch total loss 1.13728213\n",
      "Trained batch 172 batch loss 1.08903503 epoch total loss 1.13700151\n",
      "Trained batch 173 batch loss 1.08587515 epoch total loss 1.13670599\n",
      "Trained batch 174 batch loss 1.145818 epoch total loss 1.13675845\n",
      "Trained batch 175 batch loss 1.08920383 epoch total loss 1.13648665\n",
      "Trained batch 176 batch loss 1.2494297 epoch total loss 1.13712835\n",
      "Trained batch 177 batch loss 1.23182249 epoch total loss 1.13766336\n",
      "Trained batch 178 batch loss 1.17642272 epoch total loss 1.13788116\n",
      "Trained batch 179 batch loss 1.23334122 epoch total loss 1.13841438\n",
      "Trained batch 180 batch loss 1.22506535 epoch total loss 1.13889587\n",
      "Trained batch 181 batch loss 1.23722339 epoch total loss 1.13943911\n",
      "Trained batch 182 batch loss 1.20717585 epoch total loss 1.13981128\n",
      "Trained batch 183 batch loss 1.22464371 epoch total loss 1.14027476\n",
      "Trained batch 184 batch loss 1.13302827 epoch total loss 1.14023542\n",
      "Trained batch 185 batch loss 1.2707262 epoch total loss 1.14094079\n",
      "Trained batch 186 batch loss 1.19493508 epoch total loss 1.14123106\n",
      "Trained batch 187 batch loss 1.23638725 epoch total loss 1.14173985\n",
      "Trained batch 188 batch loss 1.21240783 epoch total loss 1.14211571\n",
      "Trained batch 189 batch loss 1.076599 epoch total loss 1.14176905\n",
      "Trained batch 190 batch loss 1.04065156 epoch total loss 1.1412369\n",
      "Trained batch 191 batch loss 0.992379785 epoch total loss 1.14045751\n",
      "Trained batch 192 batch loss 1.20389462 epoch total loss 1.14078796\n",
      "Trained batch 193 batch loss 1.18258798 epoch total loss 1.14100444\n",
      "Trained batch 194 batch loss 1.2241323 epoch total loss 1.141433\n",
      "Trained batch 195 batch loss 1.20537233 epoch total loss 1.14176083\n",
      "Trained batch 196 batch loss 0.961319804 epoch total loss 1.14084029\n",
      "Trained batch 197 batch loss 0.951096833 epoch total loss 1.13987708\n",
      "Trained batch 198 batch loss 1.06238902 epoch total loss 1.13948572\n",
      "Trained batch 199 batch loss 1.2485913 epoch total loss 1.14003408\n",
      "Trained batch 200 batch loss 1.39245868 epoch total loss 1.14129615\n",
      "Trained batch 201 batch loss 1.38085473 epoch total loss 1.142488\n",
      "Trained batch 202 batch loss 1.17736077 epoch total loss 1.14266074\n",
      "Trained batch 203 batch loss 1.12937832 epoch total loss 1.14259529\n",
      "Trained batch 204 batch loss 1.14128113 epoch total loss 1.14258885\n",
      "Trained batch 205 batch loss 1.25373745 epoch total loss 1.14313102\n",
      "Trained batch 206 batch loss 1.23240328 epoch total loss 1.14356434\n",
      "Trained batch 207 batch loss 1.20952475 epoch total loss 1.14388299\n",
      "Trained batch 208 batch loss 1.21333206 epoch total loss 1.1442169\n",
      "Trained batch 209 batch loss 1.22846746 epoch total loss 1.14462006\n",
      "Trained batch 210 batch loss 1.13276446 epoch total loss 1.14456356\n",
      "Trained batch 211 batch loss 1.05593717 epoch total loss 1.14414358\n",
      "Trained batch 212 batch loss 1.06582141 epoch total loss 1.14377415\n",
      "Trained batch 213 batch loss 1.11039424 epoch total loss 1.14361739\n",
      "Trained batch 214 batch loss 1.13818812 epoch total loss 1.143592\n",
      "Trained batch 215 batch loss 1.12659442 epoch total loss 1.14351296\n",
      "Trained batch 216 batch loss 1.04644799 epoch total loss 1.14306355\n",
      "Trained batch 217 batch loss 0.936045885 epoch total loss 1.14210963\n",
      "Trained batch 218 batch loss 1.04908371 epoch total loss 1.14168286\n",
      "Trained batch 219 batch loss 1.08829582 epoch total loss 1.1414392\n",
      "Trained batch 220 batch loss 1.13286781 epoch total loss 1.14140022\n",
      "Trained batch 221 batch loss 1.08557022 epoch total loss 1.14114761\n",
      "Trained batch 222 batch loss 1.14095283 epoch total loss 1.14114666\n",
      "Trained batch 223 batch loss 1.04389787 epoch total loss 1.14071059\n",
      "Trained batch 224 batch loss 1.09456158 epoch total loss 1.1405046\n",
      "Trained batch 225 batch loss 1.12694478 epoch total loss 1.14044428\n",
      "Trained batch 226 batch loss 1.02113295 epoch total loss 1.1399163\n",
      "Trained batch 227 batch loss 1.18164515 epoch total loss 1.14010012\n",
      "Trained batch 228 batch loss 1.17797947 epoch total loss 1.1402663\n",
      "Trained batch 229 batch loss 1.04961252 epoch total loss 1.13987041\n",
      "Trained batch 230 batch loss 1.1028924 epoch total loss 1.13970971\n",
      "Trained batch 231 batch loss 1.10448575 epoch total loss 1.13955724\n",
      "Trained batch 232 batch loss 1.12228489 epoch total loss 1.13948286\n",
      "Trained batch 233 batch loss 1.32175851 epoch total loss 1.14026511\n",
      "Trained batch 234 batch loss 1.15421605 epoch total loss 1.14032459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 235 batch loss 1.09641314 epoch total loss 1.14013779\n",
      "Trained batch 236 batch loss 1.13523936 epoch total loss 1.14011705\n",
      "Trained batch 237 batch loss 1.08268952 epoch total loss 1.13987482\n",
      "Trained batch 238 batch loss 1.08215654 epoch total loss 1.13963223\n",
      "Trained batch 239 batch loss 1.17500353 epoch total loss 1.13978028\n",
      "Trained batch 240 batch loss 1.23863554 epoch total loss 1.14019227\n",
      "Trained batch 241 batch loss 1.1511426 epoch total loss 1.14023781\n",
      "Trained batch 242 batch loss 1.23459458 epoch total loss 1.14062762\n",
      "Trained batch 243 batch loss 1.17997825 epoch total loss 1.14078963\n",
      "Trained batch 244 batch loss 1.18123507 epoch total loss 1.14095545\n",
      "Trained batch 245 batch loss 1.01269674 epoch total loss 1.14043188\n",
      "Trained batch 246 batch loss 1.19482303 epoch total loss 1.14065301\n",
      "Trained batch 247 batch loss 1.09546733 epoch total loss 1.14047\n",
      "Trained batch 248 batch loss 0.940076232 epoch total loss 1.13966203\n",
      "Trained batch 249 batch loss 0.990270734 epoch total loss 1.13906193\n",
      "Trained batch 250 batch loss 1.02235174 epoch total loss 1.1385951\n",
      "Trained batch 251 batch loss 1.15978086 epoch total loss 1.1386795\n",
      "Trained batch 252 batch loss 1.08266354 epoch total loss 1.1384573\n",
      "Trained batch 253 batch loss 1.05060351 epoch total loss 1.13811\n",
      "Trained batch 254 batch loss 1.04365826 epoch total loss 1.13773823\n",
      "Trained batch 255 batch loss 1.03491485 epoch total loss 1.13733494\n",
      "Trained batch 256 batch loss 1.14648128 epoch total loss 1.13737071\n",
      "Trained batch 257 batch loss 1.23618317 epoch total loss 1.13775516\n",
      "Trained batch 258 batch loss 1.21518779 epoch total loss 1.13805521\n",
      "Trained batch 259 batch loss 1.18082738 epoch total loss 1.13822031\n",
      "Trained batch 260 batch loss 1.1725564 epoch total loss 1.13835239\n",
      "Trained batch 261 batch loss 1.09873211 epoch total loss 1.13820052\n",
      "Trained batch 262 batch loss 1.03207409 epoch total loss 1.13779545\n",
      "Trained batch 263 batch loss 1.04283559 epoch total loss 1.13743448\n",
      "Trained batch 264 batch loss 1.07673895 epoch total loss 1.13720465\n",
      "Trained batch 265 batch loss 1.02512 epoch total loss 1.13678157\n",
      "Trained batch 266 batch loss 1.17058361 epoch total loss 1.13690877\n",
      "Trained batch 267 batch loss 1.2389617 epoch total loss 1.13729095\n",
      "Trained batch 268 batch loss 1.22408915 epoch total loss 1.13761485\n",
      "Trained batch 269 batch loss 1.28707516 epoch total loss 1.13817048\n",
      "Trained batch 270 batch loss 1.1563828 epoch total loss 1.13823783\n",
      "Trained batch 271 batch loss 1.11036825 epoch total loss 1.13813508\n",
      "Trained batch 272 batch loss 1.08450794 epoch total loss 1.1379379\n",
      "Trained batch 273 batch loss 1.02240705 epoch total loss 1.13751471\n",
      "Trained batch 274 batch loss 1.03995776 epoch total loss 1.13715863\n",
      "Trained batch 275 batch loss 1.13708973 epoch total loss 1.13715827\n",
      "Trained batch 276 batch loss 1.22765446 epoch total loss 1.13748622\n",
      "Trained batch 277 batch loss 1.23926306 epoch total loss 1.13785362\n",
      "Trained batch 278 batch loss 1.12186384 epoch total loss 1.13779604\n",
      "Trained batch 279 batch loss 1.09816909 epoch total loss 1.13765407\n",
      "Trained batch 280 batch loss 1.16019011 epoch total loss 1.13773453\n",
      "Trained batch 281 batch loss 1.12605882 epoch total loss 1.13769305\n",
      "Trained batch 282 batch loss 0.914673805 epoch total loss 1.13690221\n",
      "Trained batch 283 batch loss 0.914504945 epoch total loss 1.13611627\n",
      "Trained batch 284 batch loss 0.948038042 epoch total loss 1.13545394\n",
      "Trained batch 285 batch loss 1.07794678 epoch total loss 1.13525224\n",
      "Trained batch 286 batch loss 1.03526163 epoch total loss 1.13490248\n",
      "Trained batch 287 batch loss 1.15501785 epoch total loss 1.13497269\n",
      "Trained batch 288 batch loss 1.06193328 epoch total loss 1.13471901\n",
      "Trained batch 289 batch loss 1.15254056 epoch total loss 1.13478065\n",
      "Trained batch 290 batch loss 1.05346823 epoch total loss 1.13450027\n",
      "Trained batch 291 batch loss 1.10303748 epoch total loss 1.13439202\n",
      "Trained batch 292 batch loss 1.19958532 epoch total loss 1.1346153\n",
      "Trained batch 293 batch loss 1.0798794 epoch total loss 1.1344285\n",
      "Trained batch 294 batch loss 1.00373948 epoch total loss 1.13398397\n",
      "Trained batch 295 batch loss 1.03138494 epoch total loss 1.13363612\n",
      "Trained batch 296 batch loss 1.07641709 epoch total loss 1.13344288\n",
      "Trained batch 297 batch loss 1.13362324 epoch total loss 1.13344347\n",
      "Trained batch 298 batch loss 1.2806462 epoch total loss 1.13393748\n",
      "Trained batch 299 batch loss 1.43843937 epoch total loss 1.13495588\n",
      "Trained batch 300 batch loss 1.3565886 epoch total loss 1.13569462\n",
      "Trained batch 301 batch loss 1.1398083 epoch total loss 1.13570833\n",
      "Trained batch 302 batch loss 1.09086895 epoch total loss 1.13555992\n",
      "Trained batch 303 batch loss 1.2212584 epoch total loss 1.13584268\n",
      "Trained batch 304 batch loss 1.10077083 epoch total loss 1.13572729\n",
      "Trained batch 305 batch loss 1.14935136 epoch total loss 1.13577199\n",
      "Trained batch 306 batch loss 1.33842015 epoch total loss 1.1364342\n",
      "Trained batch 307 batch loss 1.14069092 epoch total loss 1.13644803\n",
      "Trained batch 308 batch loss 1.15939808 epoch total loss 1.13652253\n",
      "Trained batch 309 batch loss 1.19079137 epoch total loss 1.13669825\n",
      "Trained batch 310 batch loss 1.28068709 epoch total loss 1.13716269\n",
      "Trained batch 311 batch loss 1.12730896 epoch total loss 1.13713109\n",
      "Trained batch 312 batch loss 1.23581266 epoch total loss 1.13744736\n",
      "Trained batch 313 batch loss 0.996931791 epoch total loss 1.13699841\n",
      "Trained batch 314 batch loss 0.994215488 epoch total loss 1.13654363\n",
      "Trained batch 315 batch loss 1.06570816 epoch total loss 1.13631868\n",
      "Trained batch 316 batch loss 1.18370891 epoch total loss 1.13646877\n",
      "Trained batch 317 batch loss 1.07753301 epoch total loss 1.1362828\n",
      "Trained batch 318 batch loss 1.04369938 epoch total loss 1.13599169\n",
      "Trained batch 319 batch loss 1.10640228 epoch total loss 1.13589895\n",
      "Trained batch 320 batch loss 1.32783425 epoch total loss 1.13649869\n",
      "Trained batch 321 batch loss 1.3035872 epoch total loss 1.13701928\n",
      "Trained batch 322 batch loss 1.11878765 epoch total loss 1.13696265\n",
      "Trained batch 323 batch loss 1.27396107 epoch total loss 1.13738668\n",
      "Trained batch 324 batch loss 1.4102546 epoch total loss 1.13822889\n",
      "Trained batch 325 batch loss 1.30625117 epoch total loss 1.1387459\n",
      "Trained batch 326 batch loss 1.11274755 epoch total loss 1.13866615\n",
      "Trained batch 327 batch loss 1.21107936 epoch total loss 1.13888764\n",
      "Trained batch 328 batch loss 1.35068476 epoch total loss 1.1395334\n",
      "Trained batch 329 batch loss 1.28948379 epoch total loss 1.13998914\n",
      "Trained batch 330 batch loss 1.31841195 epoch total loss 1.14052987\n",
      "Trained batch 331 batch loss 1.25471056 epoch total loss 1.14087474\n",
      "Trained batch 332 batch loss 1.21741092 epoch total loss 1.14110529\n",
      "Trained batch 333 batch loss 1.15884173 epoch total loss 1.14115858\n",
      "Trained batch 334 batch loss 1.1278075 epoch total loss 1.14111853\n",
      "Trained batch 335 batch loss 1.0419867 epoch total loss 1.14082265\n",
      "Trained batch 336 batch loss 1.14585626 epoch total loss 1.14083767\n",
      "Trained batch 337 batch loss 1.23340309 epoch total loss 1.14111233\n",
      "Trained batch 338 batch loss 1.2096225 epoch total loss 1.14131498\n",
      "Trained batch 339 batch loss 1.165663 epoch total loss 1.14138675\n",
      "Trained batch 340 batch loss 1.05971754 epoch total loss 1.14114654\n",
      "Trained batch 341 batch loss 1.01303911 epoch total loss 1.14077091\n",
      "Trained batch 342 batch loss 1.03661084 epoch total loss 1.14046633\n",
      "Trained batch 343 batch loss 1.08904862 epoch total loss 1.14031649\n",
      "Trained batch 344 batch loss 1.12780142 epoch total loss 1.14028013\n",
      "Trained batch 345 batch loss 1.09863842 epoch total loss 1.14015937\n",
      "Trained batch 346 batch loss 1.10679483 epoch total loss 1.14006293\n",
      "Trained batch 347 batch loss 1.12833238 epoch total loss 1.14002907\n",
      "Trained batch 348 batch loss 1.14562774 epoch total loss 1.14004517\n",
      "Trained batch 349 batch loss 1.11671352 epoch total loss 1.13997829\n",
      "Trained batch 350 batch loss 1.09184813 epoch total loss 1.13984084\n",
      "Trained batch 351 batch loss 1.1855793 epoch total loss 1.13997114\n",
      "Trained batch 352 batch loss 1.19973505 epoch total loss 1.14014089\n",
      "Trained batch 353 batch loss 1.17624807 epoch total loss 1.14024317\n",
      "Trained batch 354 batch loss 1.06915355 epoch total loss 1.1400423\n",
      "Trained batch 355 batch loss 1.24564123 epoch total loss 1.14033973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 356 batch loss 1.12417269 epoch total loss 1.14029431\n",
      "Trained batch 357 batch loss 1.21022749 epoch total loss 1.14049029\n",
      "Trained batch 358 batch loss 1.21483326 epoch total loss 1.14069796\n",
      "Trained batch 359 batch loss 1.10917568 epoch total loss 1.1406101\n",
      "Trained batch 360 batch loss 1.08121562 epoch total loss 1.14044511\n",
      "Trained batch 361 batch loss 1.07660317 epoch total loss 1.14026821\n",
      "Trained batch 362 batch loss 1.21876717 epoch total loss 1.14048517\n",
      "Trained batch 363 batch loss 1.25222921 epoch total loss 1.14079297\n",
      "Trained batch 364 batch loss 1.29175556 epoch total loss 1.1412077\n",
      "Trained batch 365 batch loss 1.32391298 epoch total loss 1.14170825\n",
      "Trained batch 366 batch loss 1.23865128 epoch total loss 1.14197314\n",
      "Trained batch 367 batch loss 1.25723302 epoch total loss 1.14228714\n",
      "Trained batch 368 batch loss 1.11702275 epoch total loss 1.14221859\n",
      "Trained batch 369 batch loss 1.08233428 epoch total loss 1.14205623\n",
      "Trained batch 370 batch loss 1.25419617 epoch total loss 1.14235938\n",
      "Trained batch 371 batch loss 1.18360662 epoch total loss 1.14247048\n",
      "Trained batch 372 batch loss 1.12145627 epoch total loss 1.14241409\n",
      "Trained batch 373 batch loss 1.10236228 epoch total loss 1.14230669\n",
      "Trained batch 374 batch loss 1.10642231 epoch total loss 1.14221072\n",
      "Trained batch 375 batch loss 1.20088458 epoch total loss 1.14236724\n",
      "Trained batch 376 batch loss 1.15544701 epoch total loss 1.14240205\n",
      "Trained batch 377 batch loss 1.15921378 epoch total loss 1.14244664\n",
      "Trained batch 378 batch loss 1.28732979 epoch total loss 1.1428299\n",
      "Trained batch 379 batch loss 1.26728916 epoch total loss 1.14315832\n",
      "Trained batch 380 batch loss 1.19814777 epoch total loss 1.14330304\n",
      "Trained batch 381 batch loss 1.28635144 epoch total loss 1.14367843\n",
      "Trained batch 382 batch loss 1.27438509 epoch total loss 1.14402056\n",
      "Trained batch 383 batch loss 1.32151389 epoch total loss 1.14448404\n",
      "Trained batch 384 batch loss 1.07766545 epoch total loss 1.14431\n",
      "Trained batch 385 batch loss 1.07538068 epoch total loss 1.14413095\n",
      "Trained batch 386 batch loss 1.15295839 epoch total loss 1.14415383\n",
      "Trained batch 387 batch loss 1.07904756 epoch total loss 1.14398551\n",
      "Trained batch 388 batch loss 1.00756562 epoch total loss 1.14363396\n",
      "Trained batch 389 batch loss 1.00583231 epoch total loss 1.14327967\n",
      "Trained batch 390 batch loss 1.21787977 epoch total loss 1.143471\n",
      "Trained batch 391 batch loss 1.13957191 epoch total loss 1.14346099\n",
      "Trained batch 392 batch loss 1.04085362 epoch total loss 1.14319921\n",
      "Trained batch 393 batch loss 1.03077698 epoch total loss 1.1429131\n",
      "Trained batch 394 batch loss 1.06329465 epoch total loss 1.14271104\n",
      "Trained batch 395 batch loss 1.00717914 epoch total loss 1.14236796\n",
      "Trained batch 396 batch loss 1.1584785 epoch total loss 1.14240861\n",
      "Trained batch 397 batch loss 1.14662588 epoch total loss 1.14241922\n",
      "Trained batch 398 batch loss 1.23772657 epoch total loss 1.14265871\n",
      "Trained batch 399 batch loss 1.22874284 epoch total loss 1.14287448\n",
      "Trained batch 400 batch loss 1.23461866 epoch total loss 1.14310384\n",
      "Trained batch 401 batch loss 1.15513611 epoch total loss 1.14313388\n",
      "Trained batch 402 batch loss 1.14233255 epoch total loss 1.14313185\n",
      "Trained batch 403 batch loss 1.13396 epoch total loss 1.14310908\n",
      "Trained batch 404 batch loss 1.24640703 epoch total loss 1.14336479\n",
      "Trained batch 405 batch loss 1.11750078 epoch total loss 1.14330089\n",
      "Trained batch 406 batch loss 0.955294609 epoch total loss 1.14283788\n",
      "Trained batch 407 batch loss 0.880911291 epoch total loss 1.14219427\n",
      "Trained batch 408 batch loss 1.05681 epoch total loss 1.14198506\n",
      "Trained batch 409 batch loss 1.20765662 epoch total loss 1.14214563\n",
      "Trained batch 410 batch loss 1.26967788 epoch total loss 1.14245665\n",
      "Trained batch 411 batch loss 1.31188965 epoch total loss 1.14286888\n",
      "Trained batch 412 batch loss 1.29806709 epoch total loss 1.14324558\n",
      "Trained batch 413 batch loss 1.25961912 epoch total loss 1.14352739\n",
      "Trained batch 414 batch loss 1.25792122 epoch total loss 1.14380372\n",
      "Trained batch 415 batch loss 1.26865721 epoch total loss 1.14410448\n",
      "Trained batch 416 batch loss 1.19897175 epoch total loss 1.14423645\n",
      "Trained batch 417 batch loss 1.1914103 epoch total loss 1.14434958\n",
      "Trained batch 418 batch loss 1.15823293 epoch total loss 1.14438272\n",
      "Trained batch 419 batch loss 1.1133002 epoch total loss 1.14430857\n",
      "Trained batch 420 batch loss 1.11436129 epoch total loss 1.14423728\n",
      "Trained batch 421 batch loss 1.24936831 epoch total loss 1.1444869\n",
      "Trained batch 422 batch loss 1.22785 epoch total loss 1.14468443\n",
      "Trained batch 423 batch loss 1.23205423 epoch total loss 1.14489102\n",
      "Trained batch 424 batch loss 1.21969187 epoch total loss 1.14506745\n",
      "Trained batch 425 batch loss 1.18314779 epoch total loss 1.1451571\n",
      "Trained batch 426 batch loss 1.01466739 epoch total loss 1.14485073\n",
      "Trained batch 427 batch loss 0.988699079 epoch total loss 1.14448512\n",
      "Trained batch 428 batch loss 1.11107862 epoch total loss 1.14440703\n",
      "Trained batch 429 batch loss 1.0387125 epoch total loss 1.14416075\n",
      "Trained batch 430 batch loss 1.0490849 epoch total loss 1.14393961\n",
      "Trained batch 431 batch loss 1.10311115 epoch total loss 1.14384484\n",
      "Trained batch 432 batch loss 1.03850043 epoch total loss 1.14360106\n",
      "Trained batch 433 batch loss 1.1585443 epoch total loss 1.14363551\n",
      "Trained batch 434 batch loss 1.16902494 epoch total loss 1.14369404\n",
      "Trained batch 435 batch loss 1.19821 epoch total loss 1.14381933\n",
      "Trained batch 436 batch loss 1.18422139 epoch total loss 1.14391208\n",
      "Trained batch 437 batch loss 1.16894364 epoch total loss 1.14396942\n",
      "Trained batch 438 batch loss 1.05757082 epoch total loss 1.14377213\n",
      "Trained batch 439 batch loss 1.13097405 epoch total loss 1.14374292\n",
      "Trained batch 440 batch loss 1.05928493 epoch total loss 1.14355099\n",
      "Trained batch 441 batch loss 1.05252647 epoch total loss 1.14334464\n",
      "Trained batch 442 batch loss 1.15472746 epoch total loss 1.14337039\n",
      "Trained batch 443 batch loss 1.11002636 epoch total loss 1.14329505\n",
      "Trained batch 444 batch loss 1.19250631 epoch total loss 1.14340591\n",
      "Trained batch 445 batch loss 1.10648632 epoch total loss 1.14332294\n",
      "Trained batch 446 batch loss 1.19182277 epoch total loss 1.14343166\n",
      "Trained batch 447 batch loss 1.00446272 epoch total loss 1.14312077\n",
      "Trained batch 448 batch loss 1.09383154 epoch total loss 1.14301074\n",
      "Trained batch 449 batch loss 1.12589 epoch total loss 1.14297259\n",
      "Trained batch 450 batch loss 1.10758853 epoch total loss 1.14289403\n",
      "Trained batch 451 batch loss 1.05424345 epoch total loss 1.14269745\n",
      "Trained batch 452 batch loss 1.09376276 epoch total loss 1.14258921\n",
      "Trained batch 453 batch loss 1.0883745 epoch total loss 1.14246953\n",
      "Trained batch 454 batch loss 1.24125957 epoch total loss 1.1426872\n",
      "Trained batch 455 batch loss 1.21762598 epoch total loss 1.14285195\n",
      "Trained batch 456 batch loss 1.21613026 epoch total loss 1.14301264\n",
      "Trained batch 457 batch loss 1.19852781 epoch total loss 1.14313412\n",
      "Trained batch 458 batch loss 1.28221548 epoch total loss 1.14343786\n",
      "Trained batch 459 batch loss 1.28860211 epoch total loss 1.14375401\n",
      "Trained batch 460 batch loss 1.23502362 epoch total loss 1.14395249\n",
      "Trained batch 461 batch loss 1.20165229 epoch total loss 1.14407766\n",
      "Trained batch 462 batch loss 1.24256206 epoch total loss 1.1442908\n",
      "Trained batch 463 batch loss 1.16824162 epoch total loss 1.14434242\n",
      "Trained batch 464 batch loss 1.10486841 epoch total loss 1.14425743\n",
      "Trained batch 465 batch loss 1.29646277 epoch total loss 1.14458466\n",
      "Trained batch 466 batch loss 1.28421736 epoch total loss 1.14488435\n",
      "Trained batch 467 batch loss 1.18280888 epoch total loss 1.14496553\n",
      "Trained batch 468 batch loss 1.20031035 epoch total loss 1.14508379\n",
      "Trained batch 469 batch loss 1.14302945 epoch total loss 1.14507937\n",
      "Trained batch 470 batch loss 1.20382476 epoch total loss 1.14520431\n",
      "Trained batch 471 batch loss 1.26937425 epoch total loss 1.14546788\n",
      "Trained batch 472 batch loss 1.25061584 epoch total loss 1.14569068\n",
      "Trained batch 473 batch loss 1.30150199 epoch total loss 1.14602\n",
      "Trained batch 474 batch loss 1.27429068 epoch total loss 1.14629066\n",
      "Trained batch 475 batch loss 1.25290024 epoch total loss 1.14651525\n",
      "Trained batch 476 batch loss 1.01031554 epoch total loss 1.14622903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 477 batch loss 0.977351069 epoch total loss 1.1458751\n",
      "Trained batch 478 batch loss 1.14546871 epoch total loss 1.14587414\n",
      "Trained batch 479 batch loss 1.26398897 epoch total loss 1.14612067\n",
      "Trained batch 480 batch loss 1.39068484 epoch total loss 1.14663017\n",
      "Trained batch 481 batch loss 1.32044518 epoch total loss 1.14699161\n",
      "Trained batch 482 batch loss 1.2378881 epoch total loss 1.1471802\n",
      "Trained batch 483 batch loss 1.19506562 epoch total loss 1.14727938\n",
      "Trained batch 484 batch loss 1.14390194 epoch total loss 1.14727247\n",
      "Trained batch 485 batch loss 1.00477707 epoch total loss 1.14697862\n",
      "Trained batch 486 batch loss 1.20151925 epoch total loss 1.14709079\n",
      "Trained batch 487 batch loss 1.13451457 epoch total loss 1.14706504\n",
      "Trained batch 488 batch loss 1.18720424 epoch total loss 1.1471473\n",
      "Trained batch 489 batch loss 1.03827965 epoch total loss 1.14692461\n",
      "Trained batch 490 batch loss 1.02370846 epoch total loss 1.14667308\n",
      "Trained batch 491 batch loss 0.986553252 epoch total loss 1.14634705\n",
      "Trained batch 492 batch loss 1.03586245 epoch total loss 1.14612246\n",
      "Trained batch 493 batch loss 1.23712122 epoch total loss 1.14630711\n",
      "Trained batch 494 batch loss 1.40766573 epoch total loss 1.14683616\n",
      "Trained batch 495 batch loss 1.42855012 epoch total loss 1.14740515\n",
      "Trained batch 496 batch loss 1.27501655 epoch total loss 1.14766252\n",
      "Trained batch 497 batch loss 1.27297866 epoch total loss 1.14791453\n",
      "Trained batch 498 batch loss 1.38776529 epoch total loss 1.14839625\n",
      "Trained batch 499 batch loss 1.3204118 epoch total loss 1.14874101\n",
      "Trained batch 500 batch loss 1.20574749 epoch total loss 1.14885497\n",
      "Trained batch 501 batch loss 1.17897296 epoch total loss 1.14891505\n",
      "Trained batch 502 batch loss 1.22938156 epoch total loss 1.14907539\n",
      "Trained batch 503 batch loss 1.1834712 epoch total loss 1.1491437\n",
      "Trained batch 504 batch loss 1.26555634 epoch total loss 1.14937472\n",
      "Trained batch 505 batch loss 1.21228099 epoch total loss 1.1494993\n",
      "Trained batch 506 batch loss 1.19895434 epoch total loss 1.14959705\n",
      "Trained batch 507 batch loss 1.16528487 epoch total loss 1.14962804\n",
      "Trained batch 508 batch loss 1.03521061 epoch total loss 1.14940274\n",
      "Trained batch 509 batch loss 1.24822521 epoch total loss 1.14959693\n",
      "Trained batch 510 batch loss 1.15530765 epoch total loss 1.14960814\n",
      "Trained batch 511 batch loss 1.23723578 epoch total loss 1.14977968\n",
      "Trained batch 512 batch loss 1.18386769 epoch total loss 1.1498462\n",
      "Trained batch 513 batch loss 1.11356747 epoch total loss 1.14977551\n",
      "Trained batch 514 batch loss 1.14391172 epoch total loss 1.14976418\n",
      "Trained batch 515 batch loss 1.18265188 epoch total loss 1.14982808\n",
      "Trained batch 516 batch loss 1.07200491 epoch total loss 1.14967728\n",
      "Trained batch 517 batch loss 1.07526147 epoch total loss 1.14953327\n",
      "Trained batch 518 batch loss 1.29861593 epoch total loss 1.14982116\n",
      "Trained batch 519 batch loss 1.24289143 epoch total loss 1.15000057\n",
      "Trained batch 520 batch loss 1.13871586 epoch total loss 1.14997888\n",
      "Trained batch 521 batch loss 1.17502379 epoch total loss 1.15002704\n",
      "Trained batch 522 batch loss 1.22569191 epoch total loss 1.150172\n",
      "Trained batch 523 batch loss 1.17134619 epoch total loss 1.15021241\n",
      "Trained batch 524 batch loss 1.12923467 epoch total loss 1.15017235\n",
      "Trained batch 525 batch loss 1.30624557 epoch total loss 1.15046966\n",
      "Trained batch 526 batch loss 1.30896008 epoch total loss 1.15077102\n",
      "Trained batch 527 batch loss 1.28941453 epoch total loss 1.15103412\n",
      "Trained batch 528 batch loss 1.19903255 epoch total loss 1.15112495\n",
      "Trained batch 529 batch loss 1.16878963 epoch total loss 1.15115833\n",
      "Trained batch 530 batch loss 1.11835766 epoch total loss 1.15109646\n",
      "Trained batch 531 batch loss 1.20524526 epoch total loss 1.15119851\n",
      "Trained batch 532 batch loss 1.1983695 epoch total loss 1.15128708\n",
      "Trained batch 533 batch loss 1.24049044 epoch total loss 1.15145445\n",
      "Trained batch 534 batch loss 1.36213732 epoch total loss 1.15184891\n",
      "Trained batch 535 batch loss 1.23465264 epoch total loss 1.15200377\n",
      "Trained batch 536 batch loss 1.20712698 epoch total loss 1.15210664\n",
      "Trained batch 537 batch loss 1.16669488 epoch total loss 1.15213382\n",
      "Trained batch 538 batch loss 1.20610046 epoch total loss 1.1522342\n",
      "Trained batch 539 batch loss 1.30214167 epoch total loss 1.15251219\n",
      "Trained batch 540 batch loss 1.21879745 epoch total loss 1.15263498\n",
      "Trained batch 541 batch loss 1.18511486 epoch total loss 1.15269506\n",
      "Trained batch 542 batch loss 1.08923948 epoch total loss 1.152578\n",
      "Trained batch 543 batch loss 1.26364136 epoch total loss 1.15278256\n",
      "Trained batch 544 batch loss 1.13839293 epoch total loss 1.15275609\n",
      "Trained batch 545 batch loss 1.14338398 epoch total loss 1.15273881\n",
      "Trained batch 546 batch loss 1.19378364 epoch total loss 1.15281403\n",
      "Trained batch 547 batch loss 1.29422116 epoch total loss 1.1530726\n",
      "Trained batch 548 batch loss 1.21455288 epoch total loss 1.15318477\n",
      "Trained batch 549 batch loss 1.16492188 epoch total loss 1.15320611\n",
      "Trained batch 550 batch loss 1.21514928 epoch total loss 1.15331876\n",
      "Trained batch 551 batch loss 1.21448755 epoch total loss 1.15342975\n",
      "Trained batch 552 batch loss 1.11454058 epoch total loss 1.15335929\n",
      "Trained batch 553 batch loss 1.20499063 epoch total loss 1.15345275\n",
      "Trained batch 554 batch loss 1.17860258 epoch total loss 1.15349817\n",
      "Trained batch 555 batch loss 1.25121117 epoch total loss 1.15367424\n",
      "Trained batch 556 batch loss 1.10242176 epoch total loss 1.15358198\n",
      "Trained batch 557 batch loss 1.0555371 epoch total loss 1.15340602\n",
      "Trained batch 558 batch loss 0.960111439 epoch total loss 1.1530596\n",
      "Trained batch 559 batch loss 0.985762596 epoch total loss 1.15276027\n",
      "Trained batch 560 batch loss 1.02916217 epoch total loss 1.15253961\n",
      "Trained batch 561 batch loss 1.11254168 epoch total loss 1.15246832\n",
      "Trained batch 562 batch loss 1.07157648 epoch total loss 1.15232444\n",
      "Trained batch 563 batch loss 0.965958893 epoch total loss 1.15199339\n",
      "Trained batch 564 batch loss 1.0059644 epoch total loss 1.15173447\n",
      "Trained batch 565 batch loss 1.10342371 epoch total loss 1.15164888\n",
      "Trained batch 566 batch loss 1.40573931 epoch total loss 1.15209794\n",
      "Trained batch 567 batch loss 1.24186826 epoch total loss 1.15225625\n",
      "Trained batch 568 batch loss 1.08026505 epoch total loss 1.15212953\n",
      "Trained batch 569 batch loss 0.972961903 epoch total loss 1.15181458\n",
      "Trained batch 570 batch loss 1.06027782 epoch total loss 1.15165401\n",
      "Trained batch 571 batch loss 1.11591804 epoch total loss 1.15159142\n",
      "Trained batch 572 batch loss 1.17054534 epoch total loss 1.15162456\n",
      "Trained batch 573 batch loss 1.24697888 epoch total loss 1.15179098\n",
      "Trained batch 574 batch loss 1.12379909 epoch total loss 1.15174222\n",
      "Trained batch 575 batch loss 1.19834018 epoch total loss 1.15182328\n",
      "Trained batch 576 batch loss 1.11021399 epoch total loss 1.15175104\n",
      "Trained batch 577 batch loss 1.03750229 epoch total loss 1.15155303\n",
      "Trained batch 578 batch loss 1.18456578 epoch total loss 1.15161014\n",
      "Trained batch 579 batch loss 1.20479989 epoch total loss 1.15170193\n",
      "Trained batch 580 batch loss 1.26364446 epoch total loss 1.15189505\n",
      "Trained batch 581 batch loss 1.14187431 epoch total loss 1.15187776\n",
      "Trained batch 582 batch loss 1.10851347 epoch total loss 1.15180326\n",
      "Trained batch 583 batch loss 1.09497714 epoch total loss 1.15170574\n",
      "Trained batch 584 batch loss 1.03919375 epoch total loss 1.1515131\n",
      "Trained batch 585 batch loss 1.14345431 epoch total loss 1.15149927\n",
      "Trained batch 586 batch loss 1.06968141 epoch total loss 1.15135968\n",
      "Trained batch 587 batch loss 1.24237609 epoch total loss 1.15151477\n",
      "Trained batch 588 batch loss 1.09948277 epoch total loss 1.1514262\n",
      "Trained batch 589 batch loss 1.04266536 epoch total loss 1.15124154\n",
      "Trained batch 590 batch loss 0.981620967 epoch total loss 1.15095413\n",
      "Trained batch 591 batch loss 1.01569271 epoch total loss 1.15072525\n",
      "Trained batch 592 batch loss 1.00710642 epoch total loss 1.15048254\n",
      "Trained batch 593 batch loss 1.10694849 epoch total loss 1.1504091\n",
      "Trained batch 594 batch loss 1.11835277 epoch total loss 1.15035522\n",
      "Trained batch 595 batch loss 1.12675905 epoch total loss 1.15031552\n",
      "Trained batch 596 batch loss 1.30880678 epoch total loss 1.15058136\n",
      "Trained batch 597 batch loss 1.31076574 epoch total loss 1.15084982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 598 batch loss 1.26950336 epoch total loss 1.15104818\n",
      "Trained batch 599 batch loss 1.12217081 epoch total loss 1.151\n",
      "Trained batch 600 batch loss 1.12956178 epoch total loss 1.15096438\n",
      "Trained batch 601 batch loss 1.04366684 epoch total loss 1.1507858\n",
      "Trained batch 602 batch loss 1.26256394 epoch total loss 1.15097153\n",
      "Trained batch 603 batch loss 1.28063881 epoch total loss 1.15118647\n",
      "Trained batch 604 batch loss 1.23934698 epoch total loss 1.15133238\n",
      "Trained batch 605 batch loss 1.23723137 epoch total loss 1.15147448\n",
      "Trained batch 606 batch loss 1.14164102 epoch total loss 1.15145826\n",
      "Trained batch 607 batch loss 1.20241261 epoch total loss 1.15154219\n",
      "Trained batch 608 batch loss 1.12193513 epoch total loss 1.15149343\n",
      "Trained batch 609 batch loss 1.15574932 epoch total loss 1.15150046\n",
      "Trained batch 610 batch loss 1.30187738 epoch total loss 1.15174699\n",
      "Trained batch 611 batch loss 1.24815774 epoch total loss 1.15190482\n",
      "Trained batch 612 batch loss 1.24130309 epoch total loss 1.15205097\n",
      "Trained batch 613 batch loss 1.3443737 epoch total loss 1.15236461\n",
      "Trained batch 614 batch loss 1.24672949 epoch total loss 1.15251827\n",
      "Trained batch 615 batch loss 1.20165205 epoch total loss 1.15259826\n",
      "Trained batch 616 batch loss 1.23681748 epoch total loss 1.15273488\n",
      "Trained batch 617 batch loss 1.20094323 epoch total loss 1.15281308\n",
      "Trained batch 618 batch loss 1.09791291 epoch total loss 1.15272415\n",
      "Trained batch 619 batch loss 1.2436955 epoch total loss 1.15287113\n",
      "Trained batch 620 batch loss 1.28580284 epoch total loss 1.15308559\n",
      "Trained batch 621 batch loss 1.37371838 epoch total loss 1.15344095\n",
      "Trained batch 622 batch loss 1.33123589 epoch total loss 1.1537267\n",
      "Trained batch 623 batch loss 1.36693609 epoch total loss 1.15406895\n",
      "Trained batch 624 batch loss 1.27096939 epoch total loss 1.15425634\n",
      "Trained batch 625 batch loss 1.36585069 epoch total loss 1.1545949\n",
      "Trained batch 626 batch loss 1.33698833 epoch total loss 1.15488625\n",
      "Trained batch 627 batch loss 1.21771216 epoch total loss 1.1549865\n",
      "Trained batch 628 batch loss 1.26038313 epoch total loss 1.15515423\n",
      "Trained batch 629 batch loss 1.24035883 epoch total loss 1.15528977\n",
      "Trained batch 630 batch loss 1.05461884 epoch total loss 1.15512991\n",
      "Trained batch 631 batch loss 1.1261785 epoch total loss 1.15508401\n",
      "Trained batch 632 batch loss 0.992899299 epoch total loss 1.15482748\n",
      "Trained batch 633 batch loss 1.20234632 epoch total loss 1.15490246\n",
      "Trained batch 634 batch loss 1.15305698 epoch total loss 1.1548996\n",
      "Trained batch 635 batch loss 1.20895 epoch total loss 1.15498471\n",
      "Trained batch 636 batch loss 1.19694126 epoch total loss 1.15505075\n",
      "Trained batch 637 batch loss 1.04817367 epoch total loss 1.15488291\n",
      "Trained batch 638 batch loss 0.996533155 epoch total loss 1.15463471\n",
      "Trained batch 639 batch loss 1.05142307 epoch total loss 1.15447319\n",
      "Trained batch 640 batch loss 1.10318899 epoch total loss 1.15439308\n",
      "Trained batch 641 batch loss 1.14845443 epoch total loss 1.15438378\n",
      "Trained batch 642 batch loss 1.29122901 epoch total loss 1.15459692\n",
      "Trained batch 643 batch loss 1.21581066 epoch total loss 1.15469217\n",
      "Trained batch 644 batch loss 1.12383246 epoch total loss 1.15464425\n",
      "Trained batch 645 batch loss 1.08729744 epoch total loss 1.15453982\n",
      "Trained batch 646 batch loss 1.09757471 epoch total loss 1.15445161\n",
      "Trained batch 647 batch loss 1.08391023 epoch total loss 1.15434265\n",
      "Trained batch 648 batch loss 1.25785565 epoch total loss 1.15450239\n",
      "Trained batch 649 batch loss 1.09350634 epoch total loss 1.15440845\n",
      "Trained batch 650 batch loss 1.23066604 epoch total loss 1.15452576\n",
      "Trained batch 651 batch loss 1.08173847 epoch total loss 1.15441394\n",
      "Trained batch 652 batch loss 1.11513734 epoch total loss 1.15435362\n",
      "Trained batch 653 batch loss 1.04387486 epoch total loss 1.15418446\n",
      "Trained batch 654 batch loss 1.15567517 epoch total loss 1.15418673\n",
      "Trained batch 655 batch loss 1.20354605 epoch total loss 1.15426207\n",
      "Trained batch 656 batch loss 1.19009447 epoch total loss 1.15431678\n",
      "Trained batch 657 batch loss 1.28451943 epoch total loss 1.15451503\n",
      "Trained batch 658 batch loss 1.24556804 epoch total loss 1.15465331\n",
      "Trained batch 659 batch loss 1.34157729 epoch total loss 1.15493691\n",
      "Trained batch 660 batch loss 1.33634043 epoch total loss 1.15521181\n",
      "Trained batch 661 batch loss 1.27283788 epoch total loss 1.15538979\n",
      "Trained batch 662 batch loss 1.04538631 epoch total loss 1.15522361\n",
      "Trained batch 663 batch loss 1.17288041 epoch total loss 1.15525019\n",
      "Trained batch 664 batch loss 1.29602027 epoch total loss 1.15546227\n",
      "Trained batch 665 batch loss 1.15254426 epoch total loss 1.15545785\n",
      "Trained batch 666 batch loss 1.15612102 epoch total loss 1.15545881\n",
      "Trained batch 667 batch loss 1.13482952 epoch total loss 1.15542793\n",
      "Trained batch 668 batch loss 1.08574498 epoch total loss 1.15532362\n",
      "Trained batch 669 batch loss 1.11355937 epoch total loss 1.15526116\n",
      "Trained batch 670 batch loss 1.04491746 epoch total loss 1.15509653\n",
      "Trained batch 671 batch loss 1.17916512 epoch total loss 1.15513241\n",
      "Trained batch 672 batch loss 1.05005276 epoch total loss 1.15497601\n",
      "Trained batch 673 batch loss 1.15186214 epoch total loss 1.15497136\n",
      "Trained batch 674 batch loss 1.07776964 epoch total loss 1.1548568\n",
      "Trained batch 675 batch loss 1.1667273 epoch total loss 1.15487444\n",
      "Trained batch 676 batch loss 1.09789431 epoch total loss 1.15479016\n",
      "Trained batch 677 batch loss 1.12169492 epoch total loss 1.15474129\n",
      "Trained batch 678 batch loss 1.17565477 epoch total loss 1.15477216\n",
      "Trained batch 679 batch loss 1.14671993 epoch total loss 1.15476024\n",
      "Trained batch 680 batch loss 1.03544939 epoch total loss 1.15458477\n",
      "Trained batch 681 batch loss 1.17729068 epoch total loss 1.15461814\n",
      "Trained batch 682 batch loss 1.0987159 epoch total loss 1.15453613\n",
      "Trained batch 683 batch loss 1.05800211 epoch total loss 1.15439487\n",
      "Trained batch 684 batch loss 1.05777693 epoch total loss 1.1542536\n",
      "Trained batch 685 batch loss 1.03829741 epoch total loss 1.15408432\n",
      "Trained batch 686 batch loss 1.15544105 epoch total loss 1.15408623\n",
      "Trained batch 687 batch loss 1.16509259 epoch total loss 1.15410233\n",
      "Trained batch 688 batch loss 1.12572193 epoch total loss 1.15406108\n",
      "Trained batch 689 batch loss 1.09911835 epoch total loss 1.15398133\n",
      "Trained batch 690 batch loss 1.32238317 epoch total loss 1.15422535\n",
      "Trained batch 691 batch loss 1.19071412 epoch total loss 1.15427828\n",
      "Trained batch 692 batch loss 1.27205729 epoch total loss 1.15444839\n",
      "Trained batch 693 batch loss 1.29300702 epoch total loss 1.15464842\n",
      "Trained batch 694 batch loss 1.17098486 epoch total loss 1.15467191\n",
      "Trained batch 695 batch loss 1.27564764 epoch total loss 1.15484595\n",
      "Trained batch 696 batch loss 1.18502367 epoch total loss 1.15488923\n",
      "Trained batch 697 batch loss 1.14793849 epoch total loss 1.15487933\n",
      "Trained batch 698 batch loss 1.31288946 epoch total loss 1.15510559\n",
      "Trained batch 699 batch loss 1.22241235 epoch total loss 1.15520191\n",
      "Trained batch 700 batch loss 1.24933827 epoch total loss 1.15533638\n",
      "Trained batch 701 batch loss 1.22179532 epoch total loss 1.15543115\n",
      "Trained batch 702 batch loss 1.14165664 epoch total loss 1.1554116\n",
      "Trained batch 703 batch loss 1.15679765 epoch total loss 1.15541351\n",
      "Trained batch 704 batch loss 1.10730433 epoch total loss 1.1553452\n",
      "Trained batch 705 batch loss 1.13330173 epoch total loss 1.15531397\n",
      "Trained batch 706 batch loss 1.15205956 epoch total loss 1.15530932\n",
      "Trained batch 707 batch loss 1.00566566 epoch total loss 1.15509772\n",
      "Trained batch 708 batch loss 1.11747169 epoch total loss 1.15504456\n",
      "Trained batch 709 batch loss 0.998496711 epoch total loss 1.15482378\n",
      "Trained batch 710 batch loss 0.973806262 epoch total loss 1.15456879\n",
      "Trained batch 711 batch loss 1.13734055 epoch total loss 1.15454459\n",
      "Trained batch 712 batch loss 1.10223222 epoch total loss 1.15447104\n",
      "Trained batch 713 batch loss 1.11662948 epoch total loss 1.15441799\n",
      "Trained batch 714 batch loss 1.15419221 epoch total loss 1.15441763\n",
      "Trained batch 715 batch loss 1.31981325 epoch total loss 1.15464902\n",
      "Trained batch 716 batch loss 1.29140759 epoch total loss 1.15484\n",
      "Trained batch 717 batch loss 1.12704086 epoch total loss 1.15480113\n",
      "Trained batch 718 batch loss 1.00889897 epoch total loss 1.154598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 719 batch loss 1.23188806 epoch total loss 1.15470541\n",
      "Trained batch 720 batch loss 1.2165482 epoch total loss 1.15479136\n",
      "Trained batch 721 batch loss 1.11614168 epoch total loss 1.15473771\n",
      "Trained batch 722 batch loss 1.07949519 epoch total loss 1.15463352\n",
      "Trained batch 723 batch loss 1.31619227 epoch total loss 1.15485692\n",
      "Trained batch 724 batch loss 1.31960571 epoch total loss 1.15508437\n",
      "Trained batch 725 batch loss 1.06997275 epoch total loss 1.15496695\n",
      "Trained batch 726 batch loss 1.07599449 epoch total loss 1.15485823\n",
      "Trained batch 727 batch loss 1.00022686 epoch total loss 1.15464556\n",
      "Trained batch 728 batch loss 0.985135436 epoch total loss 1.15441263\n",
      "Trained batch 729 batch loss 0.984735906 epoch total loss 1.15417993\n",
      "Trained batch 730 batch loss 0.935597241 epoch total loss 1.15388048\n",
      "Trained batch 731 batch loss 1.08918905 epoch total loss 1.15379202\n",
      "Trained batch 732 batch loss 1.10803306 epoch total loss 1.15372944\n",
      "Trained batch 733 batch loss 1.05983067 epoch total loss 1.15360129\n",
      "Trained batch 734 batch loss 1.13403773 epoch total loss 1.15357471\n",
      "Trained batch 735 batch loss 1.08794129 epoch total loss 1.15348542\n",
      "Trained batch 736 batch loss 1.23162413 epoch total loss 1.15359151\n",
      "Trained batch 737 batch loss 1.13267815 epoch total loss 1.15356326\n",
      "Trained batch 738 batch loss 1.10694981 epoch total loss 1.15350008\n",
      "Trained batch 739 batch loss 1.17128849 epoch total loss 1.15352404\n",
      "Trained batch 740 batch loss 1.20893824 epoch total loss 1.1535989\n",
      "Trained batch 741 batch loss 1.16269994 epoch total loss 1.15361118\n",
      "Trained batch 742 batch loss 1.3611989 epoch total loss 1.15389097\n",
      "Trained batch 743 batch loss 1.30481517 epoch total loss 1.1540941\n",
      "Trained batch 744 batch loss 1.31893027 epoch total loss 1.15431571\n",
      "Trained batch 745 batch loss 1.14152479 epoch total loss 1.15429854\n",
      "Trained batch 746 batch loss 1.08176982 epoch total loss 1.15420127\n",
      "Trained batch 747 batch loss 0.898872137 epoch total loss 1.1538595\n",
      "Trained batch 748 batch loss 1.08354104 epoch total loss 1.15376556\n",
      "Trained batch 749 batch loss 1.07539654 epoch total loss 1.15366089\n",
      "Trained batch 750 batch loss 0.845735073 epoch total loss 1.15325034\n",
      "Trained batch 751 batch loss 0.890798509 epoch total loss 1.15290082\n",
      "Trained batch 752 batch loss 0.875753403 epoch total loss 1.15253234\n",
      "Trained batch 753 batch loss 1.02822697 epoch total loss 1.15236723\n",
      "Trained batch 754 batch loss 1.02703691 epoch total loss 1.15220094\n",
      "Trained batch 755 batch loss 1.09233367 epoch total loss 1.15212166\n",
      "Trained batch 756 batch loss 1.06600928 epoch total loss 1.1520077\n",
      "Trained batch 757 batch loss 1.16558886 epoch total loss 1.1520257\n",
      "Trained batch 758 batch loss 1.15713847 epoch total loss 1.15203249\n",
      "Trained batch 759 batch loss 1.17675745 epoch total loss 1.15206504\n",
      "Trained batch 760 batch loss 1.37157845 epoch total loss 1.15235388\n",
      "Trained batch 761 batch loss 1.30112576 epoch total loss 1.15254939\n",
      "Trained batch 762 batch loss 1.2945714 epoch total loss 1.15273571\n",
      "Trained batch 763 batch loss 1.04657984 epoch total loss 1.15259659\n",
      "Trained batch 764 batch loss 1.18058872 epoch total loss 1.15263331\n",
      "Trained batch 765 batch loss 1.12787807 epoch total loss 1.15260088\n",
      "Trained batch 766 batch loss 1.13655758 epoch total loss 1.1525799\n",
      "Trained batch 767 batch loss 1.06865096 epoch total loss 1.15247047\n",
      "Trained batch 768 batch loss 1.00353622 epoch total loss 1.15227664\n",
      "Trained batch 769 batch loss 1.01847732 epoch total loss 1.15210259\n",
      "Trained batch 770 batch loss 1.11784351 epoch total loss 1.15205812\n",
      "Trained batch 771 batch loss 1.02898717 epoch total loss 1.1518985\n",
      "Trained batch 772 batch loss 1.01119 epoch total loss 1.15171623\n",
      "Trained batch 773 batch loss 1.08684444 epoch total loss 1.15163231\n",
      "Trained batch 774 batch loss 1.18971872 epoch total loss 1.15168154\n",
      "Trained batch 775 batch loss 1.20446992 epoch total loss 1.15174961\n",
      "Trained batch 776 batch loss 1.05288815 epoch total loss 1.1516223\n",
      "Trained batch 777 batch loss 1.09774661 epoch total loss 1.15155292\n",
      "Trained batch 778 batch loss 1.21085441 epoch total loss 1.15162909\n",
      "Trained batch 779 batch loss 1.26836836 epoch total loss 1.15177906\n",
      "Trained batch 780 batch loss 1.18913341 epoch total loss 1.15182686\n",
      "Trained batch 781 batch loss 1.15933228 epoch total loss 1.15183651\n",
      "Trained batch 782 batch loss 1.20257497 epoch total loss 1.15190148\n",
      "Trained batch 783 batch loss 1.14825082 epoch total loss 1.15189683\n",
      "Trained batch 784 batch loss 1.05930662 epoch total loss 1.1517787\n",
      "Trained batch 785 batch loss 1.2413168 epoch total loss 1.15189278\n",
      "Trained batch 786 batch loss 1.13032377 epoch total loss 1.15186536\n",
      "Trained batch 787 batch loss 1.31692839 epoch total loss 1.15207505\n",
      "Trained batch 788 batch loss 1.34961939 epoch total loss 1.15232575\n",
      "Trained batch 789 batch loss 1.15977645 epoch total loss 1.15233529\n",
      "Trained batch 790 batch loss 1.09054112 epoch total loss 1.15225697\n",
      "Trained batch 791 batch loss 1.16445935 epoch total loss 1.15227246\n",
      "Trained batch 792 batch loss 1.15184331 epoch total loss 1.15227187\n",
      "Trained batch 793 batch loss 1.15330803 epoch total loss 1.1522733\n",
      "Trained batch 794 batch loss 1.2090205 epoch total loss 1.1523447\n",
      "Trained batch 795 batch loss 1.16638076 epoch total loss 1.15236235\n",
      "Trained batch 796 batch loss 1.24927735 epoch total loss 1.15248418\n",
      "Trained batch 797 batch loss 1.27121878 epoch total loss 1.15263319\n",
      "Trained batch 798 batch loss 1.2114253 epoch total loss 1.15270686\n",
      "Trained batch 799 batch loss 1.26603842 epoch total loss 1.15284872\n",
      "Trained batch 800 batch loss 1.1962415 epoch total loss 1.15290296\n",
      "Trained batch 801 batch loss 1.24020839 epoch total loss 1.15301192\n",
      "Trained batch 802 batch loss 1.07117224 epoch total loss 1.15290987\n",
      "Trained batch 803 batch loss 0.984895945 epoch total loss 1.15270066\n",
      "Trained batch 804 batch loss 0.976800442 epoch total loss 1.15248191\n",
      "Trained batch 805 batch loss 1.21816921 epoch total loss 1.15256345\n",
      "Trained batch 806 batch loss 1.22611403 epoch total loss 1.15265477\n",
      "Trained batch 807 batch loss 1.23989236 epoch total loss 1.15276277\n",
      "Trained batch 808 batch loss 1.19389427 epoch total loss 1.15281379\n",
      "Trained batch 809 batch loss 1.31876659 epoch total loss 1.15301895\n",
      "Trained batch 810 batch loss 1.23384273 epoch total loss 1.15311873\n",
      "Trained batch 811 batch loss 1.18151653 epoch total loss 1.15315366\n",
      "Trained batch 812 batch loss 1.26569676 epoch total loss 1.1532923\n",
      "Trained batch 813 batch loss 1.30563641 epoch total loss 1.1534797\n",
      "Trained batch 814 batch loss 1.15769351 epoch total loss 1.15348494\n",
      "Trained batch 815 batch loss 1.01275766 epoch total loss 1.15331221\n",
      "Trained batch 816 batch loss 1.15235138 epoch total loss 1.15331101\n",
      "Trained batch 817 batch loss 0.917787611 epoch total loss 1.15302277\n",
      "Trained batch 818 batch loss 1.05143034 epoch total loss 1.15289855\n",
      "Trained batch 819 batch loss 1.08240592 epoch total loss 1.15281248\n",
      "Trained batch 820 batch loss 1.19080532 epoch total loss 1.15285885\n",
      "Trained batch 821 batch loss 1.20950055 epoch total loss 1.15292776\n",
      "Trained batch 822 batch loss 1.25266528 epoch total loss 1.15304911\n",
      "Trained batch 823 batch loss 1.21237469 epoch total loss 1.15312123\n",
      "Trained batch 824 batch loss 1.19418228 epoch total loss 1.15317106\n",
      "Trained batch 825 batch loss 1.19612896 epoch total loss 1.15322316\n",
      "Trained batch 826 batch loss 1.17561674 epoch total loss 1.15325022\n",
      "Trained batch 827 batch loss 1.16052771 epoch total loss 1.15325904\n",
      "Trained batch 828 batch loss 1.17057252 epoch total loss 1.1532799\n",
      "Trained batch 829 batch loss 1.04158366 epoch total loss 1.15314519\n",
      "Trained batch 830 batch loss 1.15394664 epoch total loss 1.15314615\n",
      "Trained batch 831 batch loss 1.08191299 epoch total loss 1.15306044\n",
      "Trained batch 832 batch loss 1.06510544 epoch total loss 1.1529547\n",
      "Trained batch 833 batch loss 1.06404376 epoch total loss 1.15284789\n",
      "Trained batch 834 batch loss 0.906182766 epoch total loss 1.15255213\n",
      "Trained batch 835 batch loss 0.986583591 epoch total loss 1.15235341\n",
      "Trained batch 836 batch loss 1.02347159 epoch total loss 1.15219927\n",
      "Trained batch 837 batch loss 1.07771838 epoch total loss 1.15211022\n",
      "Trained batch 838 batch loss 1.00677919 epoch total loss 1.15193677\n",
      "Trained batch 839 batch loss 1.08018601 epoch total loss 1.1518513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 840 batch loss 1.07424855 epoch total loss 1.15175891\n",
      "Trained batch 841 batch loss 1.06207418 epoch total loss 1.15165222\n",
      "Trained batch 842 batch loss 1.23716283 epoch total loss 1.15175378\n",
      "Trained batch 843 batch loss 1.2411865 epoch total loss 1.15186\n",
      "Trained batch 844 batch loss 1.26799536 epoch total loss 1.15199757\n",
      "Trained batch 845 batch loss 1.20137382 epoch total loss 1.15205598\n",
      "Trained batch 846 batch loss 1.25505805 epoch total loss 1.15217769\n",
      "Trained batch 847 batch loss 1.24834156 epoch total loss 1.1522913\n",
      "Trained batch 848 batch loss 1.10313642 epoch total loss 1.15223336\n",
      "Trained batch 849 batch loss 1.16105711 epoch total loss 1.15224373\n",
      "Trained batch 850 batch loss 1.28487635 epoch total loss 1.15239978\n",
      "Trained batch 851 batch loss 1.1296072 epoch total loss 1.15237296\n",
      "Trained batch 852 batch loss 0.987352252 epoch total loss 1.15217924\n",
      "Trained batch 853 batch loss 1.07848883 epoch total loss 1.15209293\n",
      "Trained batch 854 batch loss 1.2001524 epoch total loss 1.15214908\n",
      "Trained batch 855 batch loss 1.29969764 epoch total loss 1.1523217\n",
      "Trained batch 856 batch loss 1.16898119 epoch total loss 1.15234113\n",
      "Trained batch 857 batch loss 1.41511846 epoch total loss 1.15264773\n",
      "Trained batch 858 batch loss 1.20962024 epoch total loss 1.15271413\n",
      "Trained batch 859 batch loss 1.19871032 epoch total loss 1.15276778\n",
      "Trained batch 860 batch loss 1.25996172 epoch total loss 1.15289235\n",
      "Trained batch 861 batch loss 1.15707433 epoch total loss 1.15289724\n",
      "Trained batch 862 batch loss 1.2622366 epoch total loss 1.15302408\n",
      "Trained batch 863 batch loss 1.29188728 epoch total loss 1.15318489\n",
      "Trained batch 864 batch loss 1.19751406 epoch total loss 1.15323627\n",
      "Trained batch 865 batch loss 1.34429872 epoch total loss 1.15345716\n",
      "Trained batch 866 batch loss 1.23436689 epoch total loss 1.15355062\n",
      "Trained batch 867 batch loss 1.16727161 epoch total loss 1.15356648\n",
      "Trained batch 868 batch loss 1.13780427 epoch total loss 1.15354824\n",
      "Trained batch 869 batch loss 1.1049155 epoch total loss 1.15349233\n",
      "Trained batch 870 batch loss 1.11292827 epoch total loss 1.15344572\n",
      "Trained batch 871 batch loss 1.11446679 epoch total loss 1.1534009\n",
      "Trained batch 872 batch loss 1.1554184 epoch total loss 1.15340316\n",
      "Trained batch 873 batch loss 1.15956569 epoch total loss 1.1534102\n",
      "Trained batch 874 batch loss 1.27959728 epoch total loss 1.15355456\n",
      "Trained batch 875 batch loss 1.39585114 epoch total loss 1.1538316\n",
      "Trained batch 876 batch loss 1.18706143 epoch total loss 1.15386951\n",
      "Trained batch 877 batch loss 1.17367136 epoch total loss 1.15389204\n",
      "Trained batch 878 batch loss 1.29547811 epoch total loss 1.15405333\n",
      "Trained batch 879 batch loss 1.22362864 epoch total loss 1.15413249\n",
      "Trained batch 880 batch loss 1.20824265 epoch total loss 1.154194\n",
      "Trained batch 881 batch loss 1.19340754 epoch total loss 1.15423846\n",
      "Trained batch 882 batch loss 1.16305232 epoch total loss 1.15424848\n",
      "Trained batch 883 batch loss 1.1222564 epoch total loss 1.15421224\n",
      "Trained batch 884 batch loss 1.16901767 epoch total loss 1.15422893\n",
      "Trained batch 885 batch loss 1.19781876 epoch total loss 1.15427816\n",
      "Trained batch 886 batch loss 1.10273659 epoch total loss 1.15422\n",
      "Trained batch 887 batch loss 1.09658504 epoch total loss 1.15415502\n",
      "Trained batch 888 batch loss 1.13824463 epoch total loss 1.15413713\n",
      "Trained batch 889 batch loss 0.987524748 epoch total loss 1.15394974\n",
      "Trained batch 890 batch loss 1.21871328 epoch total loss 1.15402257\n",
      "Trained batch 891 batch loss 1.25968635 epoch total loss 1.15414107\n",
      "Trained batch 892 batch loss 1.12735152 epoch total loss 1.15411103\n",
      "Trained batch 893 batch loss 1.21740651 epoch total loss 1.15418196\n",
      "Trained batch 894 batch loss 1.24511504 epoch total loss 1.15428364\n",
      "Trained batch 895 batch loss 1.17938793 epoch total loss 1.15431178\n",
      "Trained batch 896 batch loss 1.07883477 epoch total loss 1.1542275\n",
      "Trained batch 897 batch loss 1.07412028 epoch total loss 1.15413821\n",
      "Trained batch 898 batch loss 1.20570946 epoch total loss 1.15419555\n",
      "Trained batch 899 batch loss 1.12570846 epoch total loss 1.15416396\n",
      "Trained batch 900 batch loss 1.06377292 epoch total loss 1.15406346\n",
      "Trained batch 901 batch loss 1.09624743 epoch total loss 1.15399921\n",
      "Trained batch 902 batch loss 1.29087222 epoch total loss 1.15415096\n",
      "Trained batch 903 batch loss 1.38639569 epoch total loss 1.1544081\n",
      "Trained batch 904 batch loss 1.28791952 epoch total loss 1.15455592\n",
      "Trained batch 905 batch loss 1.30305219 epoch total loss 1.15472\n",
      "Trained batch 906 batch loss 1.33102453 epoch total loss 1.15491462\n",
      "Trained batch 907 batch loss 1.07021725 epoch total loss 1.15482128\n",
      "Trained batch 908 batch loss 0.995674193 epoch total loss 1.15464604\n",
      "Trained batch 909 batch loss 0.986702502 epoch total loss 1.15446126\n",
      "Trained batch 910 batch loss 0.9049716 epoch total loss 1.1541872\n",
      "Trained batch 911 batch loss 1.23307037 epoch total loss 1.15427375\n",
      "Trained batch 912 batch loss 1.22719896 epoch total loss 1.15435362\n",
      "Trained batch 913 batch loss 1.20212591 epoch total loss 1.15440595\n",
      "Trained batch 914 batch loss 1.01069331 epoch total loss 1.15424883\n",
      "Trained batch 915 batch loss 1.08010507 epoch total loss 1.15416777\n",
      "Trained batch 916 batch loss 1.23032892 epoch total loss 1.15425086\n",
      "Trained batch 917 batch loss 1.27899325 epoch total loss 1.154387\n",
      "Trained batch 918 batch loss 1.27219069 epoch total loss 1.15451539\n",
      "Trained batch 919 batch loss 1.26920319 epoch total loss 1.15464008\n",
      "Trained batch 920 batch loss 1.24320745 epoch total loss 1.15473628\n",
      "Trained batch 921 batch loss 1.23544407 epoch total loss 1.15482402\n",
      "Trained batch 922 batch loss 1.12799656 epoch total loss 1.15479493\n",
      "Trained batch 923 batch loss 1.24875855 epoch total loss 1.15489674\n",
      "Trained batch 924 batch loss 1.1297574 epoch total loss 1.15486956\n",
      "Trained batch 925 batch loss 1.16784835 epoch total loss 1.15488362\n",
      "Trained batch 926 batch loss 1.14300299 epoch total loss 1.15487075\n",
      "Trained batch 927 batch loss 1.09180784 epoch total loss 1.15480268\n",
      "Trained batch 928 batch loss 1.22220588 epoch total loss 1.15487528\n",
      "Trained batch 929 batch loss 1.16352129 epoch total loss 1.15488458\n",
      "Trained batch 930 batch loss 1.25685382 epoch total loss 1.15499425\n",
      "Trained batch 931 batch loss 1.23804426 epoch total loss 1.15508342\n",
      "Trained batch 932 batch loss 1.20630193 epoch total loss 1.15513837\n",
      "Trained batch 933 batch loss 1.29269338 epoch total loss 1.15528584\n",
      "Trained batch 934 batch loss 1.18047726 epoch total loss 1.15531278\n",
      "Trained batch 935 batch loss 1.13645637 epoch total loss 1.15529263\n",
      "Trained batch 936 batch loss 1.1475929 epoch total loss 1.1552844\n",
      "Trained batch 937 batch loss 1.22186255 epoch total loss 1.15535533\n",
      "Trained batch 938 batch loss 1.11852646 epoch total loss 1.15531611\n",
      "Trained batch 939 batch loss 1.03477 epoch total loss 1.15518773\n",
      "Trained batch 940 batch loss 1.11592638 epoch total loss 1.155146\n",
      "Trained batch 941 batch loss 1.15695679 epoch total loss 1.15514803\n",
      "Trained batch 942 batch loss 1.14624584 epoch total loss 1.15513849\n",
      "Trained batch 943 batch loss 1.32476306 epoch total loss 1.15531838\n",
      "Trained batch 944 batch loss 1.23953652 epoch total loss 1.15540755\n",
      "Trained batch 945 batch loss 1.36485314 epoch total loss 1.15562916\n",
      "Trained batch 946 batch loss 1.2011224 epoch total loss 1.15567732\n",
      "Trained batch 947 batch loss 1.25142 epoch total loss 1.15577853\n",
      "Trained batch 948 batch loss 1.28208613 epoch total loss 1.15591168\n",
      "Trained batch 949 batch loss 1.19172549 epoch total loss 1.15594947\n",
      "Trained batch 950 batch loss 1.11611891 epoch total loss 1.15590751\n",
      "Trained batch 951 batch loss 1.17260504 epoch total loss 1.15592515\n",
      "Trained batch 952 batch loss 1.04748285 epoch total loss 1.15581119\n",
      "Trained batch 953 batch loss 1.04069066 epoch total loss 1.15569043\n",
      "Trained batch 954 batch loss 1.09255528 epoch total loss 1.15562415\n",
      "Trained batch 955 batch loss 1.14750981 epoch total loss 1.15561557\n",
      "Trained batch 956 batch loss 1.16111779 epoch total loss 1.15562141\n",
      "Trained batch 957 batch loss 1.08498931 epoch total loss 1.1555475\n",
      "Trained batch 958 batch loss 1.25931966 epoch total loss 1.15565586\n",
      "Trained batch 959 batch loss 1.10818434 epoch total loss 1.15560627\n",
      "Trained batch 960 batch loss 1.05690384 epoch total loss 1.15550351\n",
      "Trained batch 961 batch loss 1.12393153 epoch total loss 1.15547061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 962 batch loss 1.050668 epoch total loss 1.15536165\n",
      "Trained batch 963 batch loss 1.06410456 epoch total loss 1.15526688\n",
      "Trained batch 964 batch loss 1.05371642 epoch total loss 1.1551615\n",
      "Trained batch 965 batch loss 1.19662929 epoch total loss 1.15520453\n",
      "Trained batch 966 batch loss 1.24938726 epoch total loss 1.15530205\n",
      "Trained batch 967 batch loss 1.23258376 epoch total loss 1.15538192\n",
      "Trained batch 968 batch loss 1.13623559 epoch total loss 1.15536213\n",
      "Trained batch 969 batch loss 1.09279478 epoch total loss 1.15529752\n",
      "Trained batch 970 batch loss 1.14988601 epoch total loss 1.15529191\n",
      "Trained batch 971 batch loss 1.10586 epoch total loss 1.15524101\n",
      "Trained batch 972 batch loss 1.12 epoch total loss 1.15520477\n",
      "Trained batch 973 batch loss 1.13428485 epoch total loss 1.1551832\n",
      "Trained batch 974 batch loss 1.21086764 epoch total loss 1.1552403\n",
      "Trained batch 975 batch loss 1.0809418 epoch total loss 1.15516412\n",
      "Trained batch 976 batch loss 1.14588869 epoch total loss 1.15515459\n",
      "Trained batch 977 batch loss 1.24121964 epoch total loss 1.15524268\n",
      "Trained batch 978 batch loss 1.13346 epoch total loss 1.15522039\n",
      "Trained batch 979 batch loss 1.2373296 epoch total loss 1.15530419\n",
      "Trained batch 980 batch loss 1.20320892 epoch total loss 1.15535319\n",
      "Trained batch 981 batch loss 1.41301882 epoch total loss 1.15561581\n",
      "Trained batch 982 batch loss 1.16524243 epoch total loss 1.15562558\n",
      "Trained batch 983 batch loss 1.21478748 epoch total loss 1.1556859\n",
      "Trained batch 984 batch loss 1.34197485 epoch total loss 1.15587509\n",
      "Trained batch 985 batch loss 1.25197065 epoch total loss 1.1559726\n",
      "Trained batch 986 batch loss 1.1395961 epoch total loss 1.15595615\n",
      "Trained batch 987 batch loss 1.13687599 epoch total loss 1.15593672\n",
      "Trained batch 988 batch loss 1.1453 epoch total loss 1.15592587\n",
      "Trained batch 989 batch loss 1.09551144 epoch total loss 1.15586483\n",
      "Trained batch 990 batch loss 1.16547585 epoch total loss 1.15587449\n",
      "Trained batch 991 batch loss 1.17409706 epoch total loss 1.15589285\n",
      "Trained batch 992 batch loss 1.23774409 epoch total loss 1.15597546\n",
      "Trained batch 993 batch loss 1.12124109 epoch total loss 1.15594041\n",
      "Trained batch 994 batch loss 1.11451149 epoch total loss 1.15589881\n",
      "Trained batch 995 batch loss 1.13890505 epoch total loss 1.15588176\n",
      "Trained batch 996 batch loss 1.19893694 epoch total loss 1.15592492\n",
      "Trained batch 997 batch loss 1.10331941 epoch total loss 1.15587211\n",
      "Trained batch 998 batch loss 1.10831726 epoch total loss 1.15582442\n",
      "Trained batch 999 batch loss 1.04297304 epoch total loss 1.15571153\n",
      "Trained batch 1000 batch loss 1.15963817 epoch total loss 1.15571547\n",
      "Trained batch 1001 batch loss 1.17785859 epoch total loss 1.15573752\n",
      "Trained batch 1002 batch loss 1.17339981 epoch total loss 1.15575516\n",
      "Trained batch 1003 batch loss 1.22145963 epoch total loss 1.15582061\n",
      "Trained batch 1004 batch loss 1.18240571 epoch total loss 1.15584707\n",
      "Trained batch 1005 batch loss 1.07973802 epoch total loss 1.15577126\n",
      "Trained batch 1006 batch loss 1.19148612 epoch total loss 1.1558069\n",
      "Trained batch 1007 batch loss 1.18406177 epoch total loss 1.15583491\n",
      "Trained batch 1008 batch loss 1.05791807 epoch total loss 1.15573776\n",
      "Trained batch 1009 batch loss 1.11262417 epoch total loss 1.15569508\n",
      "Trained batch 1010 batch loss 0.983770669 epoch total loss 1.15552485\n",
      "Trained batch 1011 batch loss 1.21690369 epoch total loss 1.15558553\n",
      "Trained batch 1012 batch loss 1.21038139 epoch total loss 1.15563965\n",
      "Trained batch 1013 batch loss 1.16006541 epoch total loss 1.15564394\n",
      "Trained batch 1014 batch loss 1.12390852 epoch total loss 1.15561271\n",
      "Trained batch 1015 batch loss 1.01221681 epoch total loss 1.15547144\n",
      "Trained batch 1016 batch loss 0.969367266 epoch total loss 1.15528822\n",
      "Trained batch 1017 batch loss 0.904716194 epoch total loss 1.15504181\n",
      "Trained batch 1018 batch loss 0.883930445 epoch total loss 1.1547755\n",
      "Trained batch 1019 batch loss 1.02573943 epoch total loss 1.15464878\n",
      "Trained batch 1020 batch loss 1.22703886 epoch total loss 1.15471983\n",
      "Trained batch 1021 batch loss 1.08314228 epoch total loss 1.15464973\n",
      "Trained batch 1022 batch loss 1.14845 epoch total loss 1.15464365\n",
      "Trained batch 1023 batch loss 1.16672778 epoch total loss 1.15465546\n",
      "Trained batch 1024 batch loss 1.04786313 epoch total loss 1.15455115\n",
      "Trained batch 1025 batch loss 0.977758586 epoch total loss 1.15437865\n",
      "Trained batch 1026 batch loss 1.00257015 epoch total loss 1.15423071\n",
      "Trained batch 1027 batch loss 1.07036579 epoch total loss 1.15414906\n",
      "Trained batch 1028 batch loss 1.09555745 epoch total loss 1.15409207\n",
      "Trained batch 1029 batch loss 1.06011677 epoch total loss 1.15400064\n",
      "Trained batch 1030 batch loss 0.964426875 epoch total loss 1.1538167\n",
      "Trained batch 1031 batch loss 0.915387928 epoch total loss 1.15358543\n",
      "Trained batch 1032 batch loss 1.10422122 epoch total loss 1.15353763\n",
      "Trained batch 1033 batch loss 1.19703078 epoch total loss 1.15357971\n",
      "Trained batch 1034 batch loss 1.36234355 epoch total loss 1.15378153\n",
      "Trained batch 1035 batch loss 1.06382227 epoch total loss 1.15369463\n",
      "Trained batch 1036 batch loss 1.06395209 epoch total loss 1.15360808\n",
      "Trained batch 1037 batch loss 1.19057214 epoch total loss 1.15364373\n",
      "Trained batch 1038 batch loss 1.03012204 epoch total loss 1.15352476\n",
      "Trained batch 1039 batch loss 1.13992703 epoch total loss 1.15351152\n",
      "Trained batch 1040 batch loss 1.22377944 epoch total loss 1.15357912\n",
      "Trained batch 1041 batch loss 1.16600215 epoch total loss 1.15359104\n",
      "Trained batch 1042 batch loss 1.23944592 epoch total loss 1.15367353\n",
      "Trained batch 1043 batch loss 1.26834989 epoch total loss 1.15378344\n",
      "Trained batch 1044 batch loss 1.21406949 epoch total loss 1.15384126\n",
      "Trained batch 1045 batch loss 1.1254077 epoch total loss 1.15381396\n",
      "Trained batch 1046 batch loss 1.11268187 epoch total loss 1.15377462\n",
      "Trained batch 1047 batch loss 1.10941219 epoch total loss 1.15373218\n",
      "Trained batch 1048 batch loss 1.11955607 epoch total loss 1.15369952\n",
      "Trained batch 1049 batch loss 1.22367799 epoch total loss 1.15376627\n",
      "Trained batch 1050 batch loss 1.13223135 epoch total loss 1.15374565\n",
      "Trained batch 1051 batch loss 1.17097354 epoch total loss 1.1537621\n",
      "Trained batch 1052 batch loss 1.09604406 epoch total loss 1.15370727\n",
      "Trained batch 1053 batch loss 1.13392913 epoch total loss 1.15368855\n",
      "Trained batch 1054 batch loss 1.04103112 epoch total loss 1.15358162\n",
      "Trained batch 1055 batch loss 1.15990114 epoch total loss 1.15358758\n",
      "Trained batch 1056 batch loss 0.996138 epoch total loss 1.15343845\n",
      "Trained batch 1057 batch loss 1.18780422 epoch total loss 1.15347087\n",
      "Trained batch 1058 batch loss 1.1243608 epoch total loss 1.15344346\n",
      "Trained batch 1059 batch loss 1.17509747 epoch total loss 1.15346384\n",
      "Trained batch 1060 batch loss 1.17077935 epoch total loss 1.15348017\n",
      "Trained batch 1061 batch loss 1.11616564 epoch total loss 1.15344501\n",
      "Trained batch 1062 batch loss 1.01642072 epoch total loss 1.15331602\n",
      "Trained batch 1063 batch loss 1.11074138 epoch total loss 1.15327597\n",
      "Trained batch 1064 batch loss 1.09100568 epoch total loss 1.15321755\n",
      "Trained batch 1065 batch loss 1.20890975 epoch total loss 1.15326977\n",
      "Trained batch 1066 batch loss 0.98822248 epoch total loss 1.15311503\n",
      "Trained batch 1067 batch loss 0.956682 epoch total loss 1.15293086\n",
      "Trained batch 1068 batch loss 0.914601326 epoch total loss 1.1527077\n",
      "Trained batch 1069 batch loss 1.09013438 epoch total loss 1.15264904\n",
      "Trained batch 1070 batch loss 1.13287318 epoch total loss 1.15263057\n",
      "Trained batch 1071 batch loss 1.25847936 epoch total loss 1.15272939\n",
      "Trained batch 1072 batch loss 1.39227366 epoch total loss 1.15295279\n",
      "Trained batch 1073 batch loss 1.353899 epoch total loss 1.15314007\n",
      "Trained batch 1074 batch loss 1.16092086 epoch total loss 1.15314734\n",
      "Trained batch 1075 batch loss 1.23190689 epoch total loss 1.15322065\n",
      "Trained batch 1076 batch loss 1.07726419 epoch total loss 1.15315008\n",
      "Trained batch 1077 batch loss 1.04498792 epoch total loss 1.15304971\n",
      "Trained batch 1078 batch loss 1.15051484 epoch total loss 1.15304732\n",
      "Trained batch 1079 batch loss 1.13208842 epoch total loss 1.15302789\n",
      "Trained batch 1080 batch loss 1.21271276 epoch total loss 1.15308321\n",
      "Trained batch 1081 batch loss 1.14929044 epoch total loss 1.15307963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1082 batch loss 1.14805424 epoch total loss 1.1530751\n",
      "Trained batch 1083 batch loss 1.20089364 epoch total loss 1.15311921\n",
      "Trained batch 1084 batch loss 1.18288445 epoch total loss 1.15314662\n",
      "Trained batch 1085 batch loss 1.18905008 epoch total loss 1.15317976\n",
      "Trained batch 1086 batch loss 1.16050661 epoch total loss 1.15318656\n",
      "Trained batch 1087 batch loss 1.10754621 epoch total loss 1.1531446\n",
      "Trained batch 1088 batch loss 1.10076463 epoch total loss 1.15309632\n",
      "Trained batch 1089 batch loss 0.951701045 epoch total loss 1.15291142\n",
      "Trained batch 1090 batch loss 0.980315864 epoch total loss 1.15275311\n",
      "Trained batch 1091 batch loss 1.08246994 epoch total loss 1.15268874\n",
      "Trained batch 1092 batch loss 1.06492651 epoch total loss 1.15260839\n",
      "Trained batch 1093 batch loss 1.05887151 epoch total loss 1.15252256\n",
      "Trained batch 1094 batch loss 1.0860374 epoch total loss 1.15246177\n",
      "Trained batch 1095 batch loss 1.0174309 epoch total loss 1.1523385\n",
      "Trained batch 1096 batch loss 0.970411658 epoch total loss 1.15217257\n",
      "Trained batch 1097 batch loss 1.07818604 epoch total loss 1.15210521\n",
      "Trained batch 1098 batch loss 1.13206518 epoch total loss 1.15208697\n",
      "Trained batch 1099 batch loss 1.19513929 epoch total loss 1.15212619\n",
      "Trained batch 1100 batch loss 1.30513668 epoch total loss 1.15226531\n",
      "Trained batch 1101 batch loss 1.16217101 epoch total loss 1.15227437\n",
      "Trained batch 1102 batch loss 1.15597594 epoch total loss 1.15227771\n",
      "Trained batch 1103 batch loss 1.14201081 epoch total loss 1.15226841\n",
      "Trained batch 1104 batch loss 1.11331916 epoch total loss 1.15223312\n",
      "Trained batch 1105 batch loss 1.07715654 epoch total loss 1.15216517\n",
      "Trained batch 1106 batch loss 1.11848569 epoch total loss 1.15213466\n",
      "Trained batch 1107 batch loss 0.976901233 epoch total loss 1.15197647\n",
      "Trained batch 1108 batch loss 1.1704601 epoch total loss 1.15199304\n",
      "Trained batch 1109 batch loss 1.15199435 epoch total loss 1.15199304\n",
      "Trained batch 1110 batch loss 1.28452468 epoch total loss 1.15211248\n",
      "Trained batch 1111 batch loss 1.24939728 epoch total loss 1.1522\n",
      "Trained batch 1112 batch loss 1.21993577 epoch total loss 1.15226102\n",
      "Trained batch 1113 batch loss 1.18476915 epoch total loss 1.15229023\n",
      "Trained batch 1114 batch loss 1.19403148 epoch total loss 1.15232778\n",
      "Trained batch 1115 batch loss 1.16254497 epoch total loss 1.15233696\n",
      "Trained batch 1116 batch loss 1.25254548 epoch total loss 1.15242672\n",
      "Trained batch 1117 batch loss 1.26529408 epoch total loss 1.15252781\n",
      "Trained batch 1118 batch loss 1.29136026 epoch total loss 1.15265203\n",
      "Trained batch 1119 batch loss 1.20522928 epoch total loss 1.15269899\n",
      "Trained batch 1120 batch loss 1.15784204 epoch total loss 1.15270352\n",
      "Trained batch 1121 batch loss 1.09239447 epoch total loss 1.15264976\n",
      "Trained batch 1122 batch loss 1.19546342 epoch total loss 1.15268791\n",
      "Trained batch 1123 batch loss 1.17150259 epoch total loss 1.1527046\n",
      "Trained batch 1124 batch loss 1.17484069 epoch total loss 1.15272427\n",
      "Trained batch 1125 batch loss 1.24621034 epoch total loss 1.15280735\n",
      "Trained batch 1126 batch loss 1.14107037 epoch total loss 1.15279698\n",
      "Trained batch 1127 batch loss 1.03478098 epoch total loss 1.15269232\n",
      "Trained batch 1128 batch loss 1.18674874 epoch total loss 1.15272248\n",
      "Trained batch 1129 batch loss 1.16342592 epoch total loss 1.15273201\n",
      "Trained batch 1130 batch loss 1.11266387 epoch total loss 1.15269661\n",
      "Trained batch 1131 batch loss 1.07638538 epoch total loss 1.15262914\n",
      "Trained batch 1132 batch loss 0.862035751 epoch total loss 1.15237248\n",
      "Trained batch 1133 batch loss 1.04793692 epoch total loss 1.15228033\n",
      "Trained batch 1134 batch loss 1.32450855 epoch total loss 1.15243208\n",
      "Trained batch 1135 batch loss 1.23683858 epoch total loss 1.15250647\n",
      "Trained batch 1136 batch loss 1.14673328 epoch total loss 1.15250134\n",
      "Trained batch 1137 batch loss 1.0733583 epoch total loss 1.15243185\n",
      "Trained batch 1138 batch loss 1.07287312 epoch total loss 1.15236187\n",
      "Trained batch 1139 batch loss 1.08512771 epoch total loss 1.15230286\n",
      "Trained batch 1140 batch loss 1.12298727 epoch total loss 1.15227711\n",
      "Trained batch 1141 batch loss 1.17097807 epoch total loss 1.15229356\n",
      "Trained batch 1142 batch loss 1.08442533 epoch total loss 1.1522342\n",
      "Trained batch 1143 batch loss 1.10456717 epoch total loss 1.15219247\n",
      "Trained batch 1144 batch loss 1.16701901 epoch total loss 1.15220547\n",
      "Trained batch 1145 batch loss 1.29913902 epoch total loss 1.15233386\n",
      "Trained batch 1146 batch loss 1.04845214 epoch total loss 1.15224314\n",
      "Trained batch 1147 batch loss 1.08403945 epoch total loss 1.15218365\n",
      "Trained batch 1148 batch loss 1.10226083 epoch total loss 1.15214026\n",
      "Trained batch 1149 batch loss 1.00809634 epoch total loss 1.15201485\n",
      "Trained batch 1150 batch loss 0.976666808 epoch total loss 1.15186238\n",
      "Trained batch 1151 batch loss 0.951223552 epoch total loss 1.15168798\n",
      "Trained batch 1152 batch loss 1.27298832 epoch total loss 1.15179324\n",
      "Trained batch 1153 batch loss 1.14838362 epoch total loss 1.15179038\n",
      "Trained batch 1154 batch loss 1.17008102 epoch total loss 1.15180612\n",
      "Trained batch 1155 batch loss 1.14807463 epoch total loss 1.1518029\n",
      "Trained batch 1156 batch loss 1.200791 epoch total loss 1.15184534\n",
      "Trained batch 1157 batch loss 1.09976804 epoch total loss 1.15180027\n",
      "Trained batch 1158 batch loss 0.999637604 epoch total loss 1.15166891\n",
      "Trained batch 1159 batch loss 1.02930987 epoch total loss 1.15156329\n",
      "Trained batch 1160 batch loss 1.1559844 epoch total loss 1.1515671\n",
      "Trained batch 1161 batch loss 1.13836789 epoch total loss 1.15155578\n",
      "Trained batch 1162 batch loss 1.14961767 epoch total loss 1.15155423\n",
      "Trained batch 1163 batch loss 1.22807145 epoch total loss 1.15161991\n",
      "Trained batch 1164 batch loss 1.21206903 epoch total loss 1.15167189\n",
      "Trained batch 1165 batch loss 1.27858901 epoch total loss 1.15178072\n",
      "Trained batch 1166 batch loss 1.2127912 epoch total loss 1.15183306\n",
      "Trained batch 1167 batch loss 1.30939674 epoch total loss 1.15196812\n",
      "Trained batch 1168 batch loss 1.20953965 epoch total loss 1.15201747\n",
      "Trained batch 1169 batch loss 1.31010687 epoch total loss 1.15215266\n",
      "Trained batch 1170 batch loss 1.26281285 epoch total loss 1.15224719\n",
      "Trained batch 1171 batch loss 1.17335796 epoch total loss 1.15226531\n",
      "Trained batch 1172 batch loss 1.03381932 epoch total loss 1.15216422\n",
      "Trained batch 1173 batch loss 1.06234682 epoch total loss 1.15208769\n",
      "Trained batch 1174 batch loss 1.06466675 epoch total loss 1.15201318\n",
      "Trained batch 1175 batch loss 1.03658414 epoch total loss 1.15191495\n",
      "Trained batch 1176 batch loss 1.25640857 epoch total loss 1.15200377\n",
      "Trained batch 1177 batch loss 1.19530296 epoch total loss 1.1520406\n",
      "Trained batch 1178 batch loss 1.15762091 epoch total loss 1.15204537\n",
      "Trained batch 1179 batch loss 1.12155211 epoch total loss 1.1520195\n",
      "Trained batch 1180 batch loss 1.28114176 epoch total loss 1.15212893\n",
      "Trained batch 1181 batch loss 1.28008747 epoch total loss 1.15223718\n",
      "Trained batch 1182 batch loss 1.23279834 epoch total loss 1.15230536\n",
      "Trained batch 1183 batch loss 1.13038659 epoch total loss 1.15228677\n",
      "Trained batch 1184 batch loss 1.16021919 epoch total loss 1.15229356\n",
      "Trained batch 1185 batch loss 1.0564847 epoch total loss 1.15221274\n",
      "Trained batch 1186 batch loss 1.24194431 epoch total loss 1.15228832\n",
      "Trained batch 1187 batch loss 1.13943744 epoch total loss 1.15227747\n",
      "Trained batch 1188 batch loss 1.29646301 epoch total loss 1.15239894\n",
      "Trained batch 1189 batch loss 1.22680378 epoch total loss 1.15246153\n",
      "Trained batch 1190 batch loss 1.2610153 epoch total loss 1.15255272\n",
      "Trained batch 1191 batch loss 1.00919366 epoch total loss 1.15243232\n",
      "Trained batch 1192 batch loss 1.24189425 epoch total loss 1.15250742\n",
      "Trained batch 1193 batch loss 1.2235353 epoch total loss 1.15256691\n",
      "Trained batch 1194 batch loss 1.1998477 epoch total loss 1.15260649\n",
      "Trained batch 1195 batch loss 1.17500639 epoch total loss 1.15262532\n",
      "Trained batch 1196 batch loss 1.22415006 epoch total loss 1.15268505\n",
      "Trained batch 1197 batch loss 1.2515769 epoch total loss 1.15276766\n",
      "Trained batch 1198 batch loss 1.0889678 epoch total loss 1.15271449\n",
      "Trained batch 1199 batch loss 1.16404831 epoch total loss 1.15272391\n",
      "Trained batch 1200 batch loss 1.14229143 epoch total loss 1.15271521\n",
      "Trained batch 1201 batch loss 1.20139253 epoch total loss 1.15275586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1202 batch loss 1.19043636 epoch total loss 1.15278709\n",
      "Trained batch 1203 batch loss 1.05740678 epoch total loss 1.15270782\n",
      "Trained batch 1204 batch loss 1.2066952 epoch total loss 1.15275264\n",
      "Trained batch 1205 batch loss 1.19011879 epoch total loss 1.15278363\n",
      "Trained batch 1206 batch loss 1.077034 epoch total loss 1.15272081\n",
      "Trained batch 1207 batch loss 1.15066779 epoch total loss 1.15271902\n",
      "Trained batch 1208 batch loss 1.19071031 epoch total loss 1.15275049\n",
      "Trained batch 1209 batch loss 1.101246 epoch total loss 1.15270782\n",
      "Trained batch 1210 batch loss 1.13336074 epoch total loss 1.15269184\n",
      "Trained batch 1211 batch loss 1.17498398 epoch total loss 1.1527102\n",
      "Trained batch 1212 batch loss 1.08546174 epoch total loss 1.15265465\n",
      "Trained batch 1213 batch loss 1.12226748 epoch total loss 1.15262961\n",
      "Trained batch 1214 batch loss 1.13545585 epoch total loss 1.15261555\n",
      "Trained batch 1215 batch loss 1.17259109 epoch total loss 1.152632\n",
      "Trained batch 1216 batch loss 1.21828234 epoch total loss 1.152686\n",
      "Trained batch 1217 batch loss 1.16259909 epoch total loss 1.15269411\n",
      "Trained batch 1218 batch loss 1.25089931 epoch total loss 1.15277469\n",
      "Trained batch 1219 batch loss 1.19576335 epoch total loss 1.15281\n",
      "Trained batch 1220 batch loss 1.37556922 epoch total loss 1.15299261\n",
      "Trained batch 1221 batch loss 1.41752458 epoch total loss 1.15320921\n",
      "Trained batch 1222 batch loss 1.35529852 epoch total loss 1.15337467\n",
      "Trained batch 1223 batch loss 1.09801579 epoch total loss 1.15332937\n",
      "Trained batch 1224 batch loss 1.08871365 epoch total loss 1.15327668\n",
      "Trained batch 1225 batch loss 1.167467 epoch total loss 1.15328825\n",
      "Trained batch 1226 batch loss 0.922708154 epoch total loss 1.15310013\n",
      "Trained batch 1227 batch loss 1.09193635 epoch total loss 1.1530503\n",
      "Trained batch 1228 batch loss 1.01257074 epoch total loss 1.15293586\n",
      "Trained batch 1229 batch loss 1.14507711 epoch total loss 1.15292943\n",
      "Trained batch 1230 batch loss 0.913300395 epoch total loss 1.15273464\n",
      "Trained batch 1231 batch loss 0.946467102 epoch total loss 1.15256703\n",
      "Trained batch 1232 batch loss 0.966934919 epoch total loss 1.15241635\n",
      "Trained batch 1233 batch loss 1.24305212 epoch total loss 1.1524899\n",
      "Trained batch 1234 batch loss 1.02528095 epoch total loss 1.15238678\n",
      "Trained batch 1235 batch loss 1.0007689 epoch total loss 1.152264\n",
      "Trained batch 1236 batch loss 0.978779793 epoch total loss 1.15212357\n",
      "Trained batch 1237 batch loss 1.11124468 epoch total loss 1.15209055\n",
      "Trained batch 1238 batch loss 1.09637475 epoch total loss 1.15204561\n",
      "Trained batch 1239 batch loss 1.05938482 epoch total loss 1.15197074\n",
      "Trained batch 1240 batch loss 1.00512314 epoch total loss 1.15185237\n",
      "Trained batch 1241 batch loss 0.96282351 epoch total loss 1.1516999\n",
      "Trained batch 1242 batch loss 1.18327713 epoch total loss 1.15172529\n",
      "Trained batch 1243 batch loss 1.27101839 epoch total loss 1.15182126\n",
      "Trained batch 1244 batch loss 1.20787883 epoch total loss 1.15186632\n",
      "Trained batch 1245 batch loss 1.32480049 epoch total loss 1.15200531\n",
      "Trained batch 1246 batch loss 1.09847808 epoch total loss 1.1519624\n",
      "Trained batch 1247 batch loss 0.975827456 epoch total loss 1.15182114\n",
      "Trained batch 1248 batch loss 1.13354921 epoch total loss 1.15180647\n",
      "Trained batch 1249 batch loss 1.19819307 epoch total loss 1.15184367\n",
      "Trained batch 1250 batch loss 1.21641588 epoch total loss 1.15189528\n",
      "Trained batch 1251 batch loss 1.16509151 epoch total loss 1.15190578\n",
      "Trained batch 1252 batch loss 1.18516803 epoch total loss 1.15193236\n",
      "Trained batch 1253 batch loss 1.39316654 epoch total loss 1.15212488\n",
      "Trained batch 1254 batch loss 1.2644943 epoch total loss 1.15221453\n",
      "Trained batch 1255 batch loss 1.18880105 epoch total loss 1.15224373\n",
      "Trained batch 1256 batch loss 1.11821723 epoch total loss 1.15221667\n",
      "Trained batch 1257 batch loss 1.0478785 epoch total loss 1.15213358\n",
      "Trained batch 1258 batch loss 1.12514412 epoch total loss 1.15211213\n",
      "Trained batch 1259 batch loss 1.16097689 epoch total loss 1.15211916\n",
      "Trained batch 1260 batch loss 1.08733988 epoch total loss 1.15206778\n",
      "Trained batch 1261 batch loss 1.15407836 epoch total loss 1.15206933\n",
      "Trained batch 1262 batch loss 1.21758449 epoch total loss 1.15212119\n",
      "Trained batch 1263 batch loss 1.15531766 epoch total loss 1.15212369\n",
      "Trained batch 1264 batch loss 1.01645887 epoch total loss 1.1520164\n",
      "Trained batch 1265 batch loss 1.0435071 epoch total loss 1.15193057\n",
      "Trained batch 1266 batch loss 1.15554214 epoch total loss 1.15193343\n",
      "Trained batch 1267 batch loss 1.18781507 epoch total loss 1.15196168\n",
      "Trained batch 1268 batch loss 1.24128675 epoch total loss 1.15203226\n",
      "Trained batch 1269 batch loss 1.0908438 epoch total loss 1.15198398\n",
      "Trained batch 1270 batch loss 1.02443063 epoch total loss 1.15188348\n",
      "Trained batch 1271 batch loss 1.10211897 epoch total loss 1.15184438\n",
      "Trained batch 1272 batch loss 1.1300571 epoch total loss 1.15182722\n",
      "Trained batch 1273 batch loss 1.0650959 epoch total loss 1.15175915\n",
      "Trained batch 1274 batch loss 1.14357758 epoch total loss 1.15175271\n",
      "Trained batch 1275 batch loss 1.20463836 epoch total loss 1.15179408\n",
      "Trained batch 1276 batch loss 1.03531909 epoch total loss 1.15170276\n",
      "Trained batch 1277 batch loss 0.98405 epoch total loss 1.15157151\n",
      "Trained batch 1278 batch loss 1.13336587 epoch total loss 1.15155721\n",
      "Trained batch 1279 batch loss 1.04084158 epoch total loss 1.15147078\n",
      "Trained batch 1280 batch loss 1.03155386 epoch total loss 1.15137696\n",
      "Trained batch 1281 batch loss 1.02924502 epoch total loss 1.15128171\n",
      "Trained batch 1282 batch loss 0.980781794 epoch total loss 1.1511488\n",
      "Trained batch 1283 batch loss 1.00255299 epoch total loss 1.15103292\n",
      "Trained batch 1284 batch loss 1.10829377 epoch total loss 1.15099967\n",
      "Trained batch 1285 batch loss 1.14994192 epoch total loss 1.15099883\n",
      "Trained batch 1286 batch loss 1.32628512 epoch total loss 1.15113509\n",
      "Trained batch 1287 batch loss 1.2482096 epoch total loss 1.15121055\n",
      "Trained batch 1288 batch loss 1.30474246 epoch total loss 1.15132964\n",
      "Trained batch 1289 batch loss 1.16540897 epoch total loss 1.1513406\n",
      "Trained batch 1290 batch loss 1.16071141 epoch total loss 1.15134788\n",
      "Trained batch 1291 batch loss 1.0584358 epoch total loss 1.15127599\n",
      "Trained batch 1292 batch loss 1.19834352 epoch total loss 1.15131235\n",
      "Trained batch 1293 batch loss 1.19880629 epoch total loss 1.15134919\n",
      "Trained batch 1294 batch loss 1.26362181 epoch total loss 1.15143597\n",
      "Trained batch 1295 batch loss 1.20787096 epoch total loss 1.1514796\n",
      "Trained batch 1296 batch loss 1.0863955 epoch total loss 1.1514293\n",
      "Trained batch 1297 batch loss 1.2266742 epoch total loss 1.15148735\n",
      "Trained batch 1298 batch loss 1.38022232 epoch total loss 1.15166366\n",
      "Trained batch 1299 batch loss 1.21911049 epoch total loss 1.15171552\n",
      "Trained batch 1300 batch loss 1.15299702 epoch total loss 1.15171647\n",
      "Trained batch 1301 batch loss 1.1307503 epoch total loss 1.15170038\n",
      "Trained batch 1302 batch loss 1.13892615 epoch total loss 1.1516906\n",
      "Trained batch 1303 batch loss 1.38612342 epoch total loss 1.15187049\n",
      "Trained batch 1304 batch loss 1.29033804 epoch total loss 1.15197659\n",
      "Trained batch 1305 batch loss 1.35872889 epoch total loss 1.15213501\n",
      "Trained batch 1306 batch loss 1.28867197 epoch total loss 1.15223968\n",
      "Trained batch 1307 batch loss 1.27486289 epoch total loss 1.1523335\n",
      "Trained batch 1308 batch loss 1.29524 epoch total loss 1.15244281\n",
      "Trained batch 1309 batch loss 1.15676188 epoch total loss 1.15244603\n",
      "Trained batch 1310 batch loss 1.2563591 epoch total loss 1.15252531\n",
      "Trained batch 1311 batch loss 1.18970478 epoch total loss 1.15255368\n",
      "Trained batch 1312 batch loss 1.20048451 epoch total loss 1.15259027\n",
      "Trained batch 1313 batch loss 1.11268866 epoch total loss 1.15255976\n",
      "Trained batch 1314 batch loss 1.19281209 epoch total loss 1.15259051\n",
      "Trained batch 1315 batch loss 1.14285254 epoch total loss 1.15258312\n",
      "Trained batch 1316 batch loss 1.04962635 epoch total loss 1.15250492\n",
      "Trained batch 1317 batch loss 1.00536954 epoch total loss 1.1523931\n",
      "Trained batch 1318 batch loss 0.997078896 epoch total loss 1.15227532\n",
      "Trained batch 1319 batch loss 1.09420466 epoch total loss 1.15223134\n",
      "Trained batch 1320 batch loss 1.25215816 epoch total loss 1.15230703\n",
      "Trained batch 1321 batch loss 1.24382031 epoch total loss 1.15237629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1322 batch loss 1.33754206 epoch total loss 1.15251637\n",
      "Trained batch 1323 batch loss 1.36302161 epoch total loss 1.15267551\n",
      "Trained batch 1324 batch loss 1.42194438 epoch total loss 1.15287888\n",
      "Trained batch 1325 batch loss 1.44619465 epoch total loss 1.15310025\n",
      "Trained batch 1326 batch loss 1.14850986 epoch total loss 1.1530968\n",
      "Trained batch 1327 batch loss 1.04567611 epoch total loss 1.15301585\n",
      "Trained batch 1328 batch loss 1.06708705 epoch total loss 1.15295112\n",
      "Trained batch 1329 batch loss 1.22912776 epoch total loss 1.15300846\n",
      "Trained batch 1330 batch loss 1.167943 epoch total loss 1.15301967\n",
      "Trained batch 1331 batch loss 1.20185387 epoch total loss 1.1530565\n",
      "Trained batch 1332 batch loss 1.23318386 epoch total loss 1.15311658\n",
      "Trained batch 1333 batch loss 1.27396977 epoch total loss 1.15320718\n",
      "Trained batch 1334 batch loss 1.35539627 epoch total loss 1.1533587\n",
      "Trained batch 1335 batch loss 1.29875 epoch total loss 1.15346766\n",
      "Trained batch 1336 batch loss 1.27624643 epoch total loss 1.15355957\n",
      "Trained batch 1337 batch loss 1.18634367 epoch total loss 1.15358412\n",
      "Trained batch 1338 batch loss 1.22519815 epoch total loss 1.15363765\n",
      "Trained batch 1339 batch loss 1.15833414 epoch total loss 1.1536411\n",
      "Trained batch 1340 batch loss 1.13426828 epoch total loss 1.15362668\n",
      "Trained batch 1341 batch loss 1.20756638 epoch total loss 1.15366685\n",
      "Trained batch 1342 batch loss 1.08295667 epoch total loss 1.15361416\n",
      "Trained batch 1343 batch loss 1.15446758 epoch total loss 1.15361476\n",
      "Trained batch 1344 batch loss 1.13475657 epoch total loss 1.15360081\n",
      "Trained batch 1345 batch loss 1.03428006 epoch total loss 1.15351212\n",
      "Trained batch 1346 batch loss 1.18886864 epoch total loss 1.15353835\n",
      "Trained batch 1347 batch loss 1.30871582 epoch total loss 1.1536535\n",
      "Trained batch 1348 batch loss 1.11152816 epoch total loss 1.15362227\n",
      "Trained batch 1349 batch loss 1.19653654 epoch total loss 1.1536541\n",
      "Trained batch 1350 batch loss 1.19012094 epoch total loss 1.15368104\n",
      "Trained batch 1351 batch loss 1.31384754 epoch total loss 1.15379965\n",
      "Trained batch 1352 batch loss 1.24127209 epoch total loss 1.15386438\n",
      "Trained batch 1353 batch loss 1.22279763 epoch total loss 1.15391529\n",
      "Trained batch 1354 batch loss 1.3376199 epoch total loss 1.15405107\n",
      "Trained batch 1355 batch loss 1.1668905 epoch total loss 1.15406048\n",
      "Trained batch 1356 batch loss 1.20494592 epoch total loss 1.15409803\n",
      "Trained batch 1357 batch loss 1.28312492 epoch total loss 1.15419304\n",
      "Trained batch 1358 batch loss 1.13056028 epoch total loss 1.15417576\n",
      "Trained batch 1359 batch loss 1.25600243 epoch total loss 1.15425062\n",
      "Trained batch 1360 batch loss 1.18824244 epoch total loss 1.15427566\n",
      "Trained batch 1361 batch loss 1.2393887 epoch total loss 1.15433812\n",
      "Trained batch 1362 batch loss 1.18783939 epoch total loss 1.1543628\n",
      "Trained batch 1363 batch loss 1.10083 epoch total loss 1.15432346\n",
      "Trained batch 1364 batch loss 1.25484443 epoch total loss 1.15439725\n",
      "Trained batch 1365 batch loss 1.20160246 epoch total loss 1.15443182\n",
      "Trained batch 1366 batch loss 1.23774469 epoch total loss 1.15449286\n",
      "Trained batch 1367 batch loss 1.24715066 epoch total loss 1.15456069\n",
      "Trained batch 1368 batch loss 1.11182356 epoch total loss 1.15452945\n",
      "Trained batch 1369 batch loss 1.16200399 epoch total loss 1.15453482\n",
      "Trained batch 1370 batch loss 1.15637171 epoch total loss 1.15453625\n",
      "Trained batch 1371 batch loss 1.16453481 epoch total loss 1.15454352\n",
      "Trained batch 1372 batch loss 1.17781544 epoch total loss 1.15456045\n",
      "Trained batch 1373 batch loss 1.16331077 epoch total loss 1.15456688\n",
      "Trained batch 1374 batch loss 1.14911652 epoch total loss 1.15456295\n",
      "Trained batch 1375 batch loss 1.2667377 epoch total loss 1.15464449\n",
      "Trained batch 1376 batch loss 1.3159678 epoch total loss 1.15476179\n",
      "Trained batch 1377 batch loss 1.27148032 epoch total loss 1.15484655\n",
      "Trained batch 1378 batch loss 1.2911346 epoch total loss 1.15494537\n",
      "Trained batch 1379 batch loss 1.15527415 epoch total loss 1.15494561\n",
      "Trained batch 1380 batch loss 1.08549941 epoch total loss 1.15489531\n",
      "Trained batch 1381 batch loss 1.1847055 epoch total loss 1.15491688\n",
      "Trained batch 1382 batch loss 1.15300751 epoch total loss 1.15491545\n",
      "Trained batch 1383 batch loss 1.17005634 epoch total loss 1.15492642\n",
      "Trained batch 1384 batch loss 1.17924297 epoch total loss 1.15494394\n",
      "Trained batch 1385 batch loss 1.20576727 epoch total loss 1.15498066\n",
      "Trained batch 1386 batch loss 1.16141307 epoch total loss 1.15498531\n",
      "Trained batch 1387 batch loss 1.16178441 epoch total loss 1.15499008\n",
      "Trained batch 1388 batch loss 1.24843 epoch total loss 1.15505743\n",
      "Epoch 6 train loss 1.155057430267334\n",
      "Validated batch 1 batch loss 1.23060751\n",
      "Validated batch 2 batch loss 1.12973452\n",
      "Validated batch 3 batch loss 1.13205576\n",
      "Validated batch 4 batch loss 1.10392523\n",
      "Validated batch 5 batch loss 1.11234\n",
      "Validated batch 6 batch loss 1.21975708\n",
      "Validated batch 7 batch loss 1.18501651\n",
      "Validated batch 8 batch loss 1.10525036\n",
      "Validated batch 9 batch loss 1.26681447\n",
      "Validated batch 10 batch loss 1.21474481\n",
      "Validated batch 11 batch loss 1.09098279\n",
      "Validated batch 12 batch loss 1.06055808\n",
      "Validated batch 13 batch loss 1.21043622\n",
      "Validated batch 14 batch loss 1.24059379\n",
      "Validated batch 15 batch loss 1.34635127\n",
      "Validated batch 16 batch loss 1.30257273\n",
      "Validated batch 17 batch loss 1.16618323\n",
      "Validated batch 18 batch loss 1.30290854\n",
      "Validated batch 19 batch loss 1.20425689\n",
      "Validated batch 20 batch loss 1.16705585\n",
      "Validated batch 21 batch loss 1.21973932\n",
      "Validated batch 22 batch loss 0.934342265\n",
      "Validated batch 23 batch loss 1.17324424\n",
      "Validated batch 24 batch loss 1.19498634\n",
      "Validated batch 25 batch loss 1.10134923\n",
      "Validated batch 26 batch loss 1.20600748\n",
      "Validated batch 27 batch loss 1.07631934\n",
      "Validated batch 28 batch loss 1.17101264\n",
      "Validated batch 29 batch loss 1.20227027\n",
      "Validated batch 30 batch loss 1.2119453\n",
      "Validated batch 31 batch loss 1.30415916\n",
      "Validated batch 32 batch loss 1.23161125\n",
      "Validated batch 33 batch loss 1.17730212\n",
      "Validated batch 34 batch loss 1.2231648\n",
      "Validated batch 35 batch loss 1.2771579\n",
      "Validated batch 36 batch loss 1.26069808\n",
      "Validated batch 37 batch loss 1.25197864\n",
      "Validated batch 38 batch loss 1.20035958\n",
      "Validated batch 39 batch loss 1.2633456\n",
      "Validated batch 40 batch loss 1.23896146\n",
      "Validated batch 41 batch loss 1.08877206\n",
      "Validated batch 42 batch loss 1.15288937\n",
      "Validated batch 43 batch loss 1.231143\n",
      "Validated batch 44 batch loss 1.20223451\n",
      "Validated batch 45 batch loss 1.15936518\n",
      "Validated batch 46 batch loss 1.16096449\n",
      "Validated batch 47 batch loss 1.13978648\n",
      "Validated batch 48 batch loss 1.06055331\n",
      "Validated batch 49 batch loss 1.1366322\n",
      "Validated batch 50 batch loss 1.11254287\n",
      "Validated batch 51 batch loss 1.14537036\n",
      "Validated batch 52 batch loss 1.20948875\n",
      "Validated batch 53 batch loss 1.20196164\n",
      "Validated batch 54 batch loss 1.1256659\n",
      "Validated batch 55 batch loss 1.18851364\n",
      "Validated batch 56 batch loss 1.1721313\n",
      "Validated batch 57 batch loss 1.16099596\n",
      "Validated batch 58 batch loss 1.22407365\n",
      "Validated batch 59 batch loss 1.09132588\n",
      "Validated batch 60 batch loss 1.1269592\n",
      "Validated batch 61 batch loss 1.21183574\n",
      "Validated batch 62 batch loss 1.12578833\n",
      "Validated batch 63 batch loss 1.32205331\n",
      "Validated batch 64 batch loss 1.23182988\n",
      "Validated batch 65 batch loss 1.05553389\n",
      "Validated batch 66 batch loss 1.23848689\n",
      "Validated batch 67 batch loss 1.16384602\n",
      "Validated batch 68 batch loss 1.08693779\n",
      "Validated batch 69 batch loss 1.1833365\n",
      "Validated batch 70 batch loss 1.05346894\n",
      "Validated batch 71 batch loss 1.17828023\n",
      "Validated batch 72 batch loss 1.23539352\n",
      "Validated batch 73 batch loss 1.13558948\n",
      "Validated batch 74 batch loss 1.24370742\n",
      "Validated batch 75 batch loss 1.34828401\n",
      "Validated batch 76 batch loss 1.08863902\n",
      "Validated batch 77 batch loss 1.17705846\n",
      "Validated batch 78 batch loss 1.18995047\n",
      "Validated batch 79 batch loss 1.24247539\n",
      "Validated batch 80 batch loss 1.22433019\n",
      "Validated batch 81 batch loss 1.13212109\n",
      "Validated batch 82 batch loss 1.01344883\n",
      "Validated batch 83 batch loss 1.18515205\n",
      "Validated batch 84 batch loss 1.13767898\n",
      "Validated batch 85 batch loss 1.15525651\n",
      "Validated batch 86 batch loss 1.2069639\n",
      "Validated batch 87 batch loss 1.11435294\n",
      "Validated batch 88 batch loss 1.19603527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 89 batch loss 1.26156378\n",
      "Validated batch 90 batch loss 1.27626717\n",
      "Validated batch 91 batch loss 1.20424354\n",
      "Validated batch 92 batch loss 1.06999874\n",
      "Validated batch 93 batch loss 1.24898052\n",
      "Validated batch 94 batch loss 1.25044703\n",
      "Validated batch 95 batch loss 1.19376731\n",
      "Validated batch 96 batch loss 1.19553614\n",
      "Validated batch 97 batch loss 1.22890067\n",
      "Validated batch 98 batch loss 1.40988088\n",
      "Validated batch 99 batch loss 1.16958\n",
      "Validated batch 100 batch loss 1.2591418\n",
      "Validated batch 101 batch loss 1.17157614\n",
      "Validated batch 102 batch loss 1.22080231\n",
      "Validated batch 103 batch loss 1.21608305\n",
      "Validated batch 104 batch loss 1.14085579\n",
      "Validated batch 105 batch loss 1.28936529\n",
      "Validated batch 106 batch loss 1.189206\n",
      "Validated batch 107 batch loss 1.28580689\n",
      "Validated batch 108 batch loss 1.19818163\n",
      "Validated batch 109 batch loss 1.27274811\n",
      "Validated batch 110 batch loss 1.10837471\n",
      "Validated batch 111 batch loss 1.17950916\n",
      "Validated batch 112 batch loss 1.2330929\n",
      "Validated batch 113 batch loss 1.14031446\n",
      "Validated batch 114 batch loss 1.27050936\n",
      "Validated batch 115 batch loss 1.16352046\n",
      "Validated batch 116 batch loss 1.21497858\n",
      "Validated batch 117 batch loss 1.19780445\n",
      "Validated batch 118 batch loss 1.15544415\n",
      "Validated batch 119 batch loss 1.0586592\n",
      "Validated batch 120 batch loss 1.12855089\n",
      "Validated batch 121 batch loss 1.28730154\n",
      "Validated batch 122 batch loss 1.06683326\n",
      "Validated batch 123 batch loss 1.02606571\n",
      "Validated batch 124 batch loss 1.13569617\n",
      "Validated batch 125 batch loss 1.13975036\n",
      "Validated batch 126 batch loss 1.06661212\n",
      "Validated batch 127 batch loss 1.18539703\n",
      "Validated batch 128 batch loss 1.13236618\n",
      "Validated batch 129 batch loss 1.08151937\n",
      "Validated batch 130 batch loss 1.22920465\n",
      "Validated batch 131 batch loss 1.2752645\n",
      "Validated batch 132 batch loss 1.0720737\n",
      "Validated batch 133 batch loss 1.31002474\n",
      "Validated batch 134 batch loss 0.975895643\n",
      "Validated batch 135 batch loss 1.1120317\n",
      "Validated batch 136 batch loss 1.14257562\n",
      "Validated batch 137 batch loss 1.17810047\n",
      "Validated batch 138 batch loss 1.26950288\n",
      "Validated batch 139 batch loss 1.30572593\n",
      "Validated batch 140 batch loss 1.21438\n",
      "Validated batch 141 batch loss 1.19362724\n",
      "Validated batch 142 batch loss 1.18861699\n",
      "Validated batch 143 batch loss 1.0898993\n",
      "Validated batch 144 batch loss 1.19233549\n",
      "Validated batch 145 batch loss 1.22027707\n",
      "Validated batch 146 batch loss 1.09809589\n",
      "Validated batch 147 batch loss 1.11680555\n",
      "Validated batch 148 batch loss 1.24281394\n",
      "Validated batch 149 batch loss 1.15445709\n",
      "Validated batch 150 batch loss 1.26350045\n",
      "Validated batch 151 batch loss 1.23558831\n",
      "Validated batch 152 batch loss 1.0736928\n",
      "Validated batch 153 batch loss 1.14003444\n",
      "Validated batch 154 batch loss 1.21610761\n",
      "Validated batch 155 batch loss 1.17133534\n",
      "Validated batch 156 batch loss 1.19320607\n",
      "Validated batch 157 batch loss 1.19494557\n",
      "Validated batch 158 batch loss 1.32670212\n",
      "Validated batch 159 batch loss 1.33156919\n",
      "Validated batch 160 batch loss 1.20131707\n",
      "Validated batch 161 batch loss 1.08523488\n",
      "Validated batch 162 batch loss 1.0761869\n",
      "Validated batch 163 batch loss 1.18799949\n",
      "Validated batch 164 batch loss 1.19650602\n",
      "Validated batch 165 batch loss 1.07416177\n",
      "Validated batch 166 batch loss 1.16245091\n",
      "Validated batch 167 batch loss 1.19003034\n",
      "Validated batch 168 batch loss 1.2490139\n",
      "Validated batch 169 batch loss 1.27995038\n",
      "Validated batch 170 batch loss 1.25133634\n",
      "Validated batch 171 batch loss 1.15112305\n",
      "Validated batch 172 batch loss 1.18205202\n",
      "Validated batch 173 batch loss 1.19655287\n",
      "Validated batch 174 batch loss 1.14668977\n",
      "Validated batch 175 batch loss 1.27737296\n",
      "Validated batch 176 batch loss 1.35513735\n",
      "Validated batch 177 batch loss 1.15846992\n",
      "Validated batch 178 batch loss 1.26610661\n",
      "Validated batch 179 batch loss 1.19799387\n",
      "Validated batch 180 batch loss 1.09590864\n",
      "Validated batch 181 batch loss 1.16338825\n",
      "Validated batch 182 batch loss 1.05918741\n",
      "Validated batch 183 batch loss 1.22383022\n",
      "Validated batch 184 batch loss 1.14023304\n",
      "Validated batch 185 batch loss 1.19137406\n",
      "Epoch 6 val loss 1.183213710784912\n",
      "Model /aiffel/aiffel/mpii/a/model-epoch-6-loss-1.1832.h5 saved.\n",
      "Start epoch 7 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.02662492 epoch total loss 1.02662492\n",
      "Trained batch 2 batch loss 1.13276196 epoch total loss 1.07969344\n",
      "Trained batch 3 batch loss 1.0348 epoch total loss 1.06472898\n",
      "Trained batch 4 batch loss 1.08777881 epoch total loss 1.07049143\n",
      "Trained batch 5 batch loss 1.20835912 epoch total loss 1.09806502\n",
      "Trained batch 6 batch loss 1.13510954 epoch total loss 1.10423911\n",
      "Trained batch 7 batch loss 1.14690495 epoch total loss 1.11033416\n",
      "Trained batch 8 batch loss 1.21543717 epoch total loss 1.12347209\n",
      "Trained batch 9 batch loss 1.16424417 epoch total loss 1.12800241\n",
      "Trained batch 10 batch loss 1.14791012 epoch total loss 1.1299932\n",
      "Trained batch 11 batch loss 1.16879416 epoch total loss 1.13352048\n",
      "Trained batch 12 batch loss 1.17771029 epoch total loss 1.13720298\n",
      "Trained batch 13 batch loss 1.08842158 epoch total loss 1.13345063\n",
      "Trained batch 14 batch loss 1.01764297 epoch total loss 1.12517858\n",
      "Trained batch 15 batch loss 1.17004061 epoch total loss 1.12816942\n",
      "Trained batch 16 batch loss 1.183038 epoch total loss 1.13159871\n",
      "Trained batch 17 batch loss 1.31411099 epoch total loss 1.14233482\n",
      "Trained batch 18 batch loss 1.20210648 epoch total loss 1.14565539\n",
      "Trained batch 19 batch loss 1.15872097 epoch total loss 1.14634299\n",
      "Trained batch 20 batch loss 0.999372 epoch total loss 1.13899446\n",
      "Trained batch 21 batch loss 1.19685721 epoch total loss 1.14174986\n",
      "Trained batch 22 batch loss 1.15236473 epoch total loss 1.14223242\n",
      "Trained batch 23 batch loss 1.16192865 epoch total loss 1.1430887\n",
      "Trained batch 24 batch loss 1.25154686 epoch total loss 1.1476078\n",
      "Trained batch 25 batch loss 1.06959128 epoch total loss 1.14448714\n",
      "Trained batch 26 batch loss 1.0655067 epoch total loss 1.14144945\n",
      "Trained batch 27 batch loss 1.0727489 epoch total loss 1.13890493\n",
      "Trained batch 28 batch loss 1.06482732 epoch total loss 1.13625932\n",
      "Trained batch 29 batch loss 1.12835145 epoch total loss 1.13598657\n",
      "Trained batch 30 batch loss 1.14238906 epoch total loss 1.1362\n",
      "Trained batch 31 batch loss 1.1360805 epoch total loss 1.13619614\n",
      "Trained batch 32 batch loss 1.07812572 epoch total loss 1.13438141\n",
      "Trained batch 33 batch loss 1.08440328 epoch total loss 1.13286698\n",
      "Trained batch 34 batch loss 1.0792135 epoch total loss 1.13128889\n",
      "Trained batch 35 batch loss 1.05147612 epoch total loss 1.12900853\n",
      "Trained batch 36 batch loss 1.18221343 epoch total loss 1.13048637\n",
      "Trained batch 37 batch loss 1.02488458 epoch total loss 1.12763226\n",
      "Trained batch 38 batch loss 1.12774229 epoch total loss 1.12763512\n",
      "Trained batch 39 batch loss 1.10140109 epoch total loss 1.12696254\n",
      "Trained batch 40 batch loss 1.10025382 epoch total loss 1.12629485\n",
      "Trained batch 41 batch loss 1.11706316 epoch total loss 1.12606955\n",
      "Trained batch 42 batch loss 1.02811408 epoch total loss 1.12373734\n",
      "Trained batch 43 batch loss 1.03116202 epoch total loss 1.12158442\n",
      "Trained batch 44 batch loss 1.05035162 epoch total loss 1.11996543\n",
      "Trained batch 45 batch loss 1.09165967 epoch total loss 1.11933649\n",
      "Trained batch 46 batch loss 1.21859801 epoch total loss 1.12149429\n",
      "Trained batch 47 batch loss 1.18061388 epoch total loss 1.12275219\n",
      "Trained batch 48 batch loss 1.30087018 epoch total loss 1.12646294\n",
      "Trained batch 49 batch loss 1.30205536 epoch total loss 1.13004649\n",
      "Trained batch 50 batch loss 1.33627057 epoch total loss 1.13417089\n",
      "Trained batch 51 batch loss 1.1449976 epoch total loss 1.1343832\n",
      "Trained batch 52 batch loss 1.16005337 epoch total loss 1.13487685\n",
      "Trained batch 53 batch loss 1.10485053 epoch total loss 1.13431036\n",
      "Trained batch 54 batch loss 1.17278266 epoch total loss 1.13502276\n",
      "Trained batch 55 batch loss 1.16581273 epoch total loss 1.13558257\n",
      "Trained batch 56 batch loss 1.29364371 epoch total loss 1.13840508\n",
      "Trained batch 57 batch loss 1.23198807 epoch total loss 1.14004695\n",
      "Trained batch 58 batch loss 1.25507593 epoch total loss 1.14203012\n",
      "Trained batch 59 batch loss 1.26620281 epoch total loss 1.14413476\n",
      "Trained batch 60 batch loss 1.43406165 epoch total loss 1.14896691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 61 batch loss 1.36458778 epoch total loss 1.15250158\n",
      "Trained batch 62 batch loss 1.31129706 epoch total loss 1.15506279\n",
      "Trained batch 63 batch loss 1.17243588 epoch total loss 1.15533864\n",
      "Trained batch 64 batch loss 1.20977318 epoch total loss 1.15618908\n",
      "Trained batch 65 batch loss 1.30769908 epoch total loss 1.15852\n",
      "Trained batch 66 batch loss 1.41266561 epoch total loss 1.16237068\n",
      "Trained batch 67 batch loss 1.19444191 epoch total loss 1.16284943\n",
      "Trained batch 68 batch loss 1.0142715 epoch total loss 1.16066456\n",
      "Trained batch 69 batch loss 1.06047392 epoch total loss 1.15921247\n",
      "Trained batch 70 batch loss 1.24275899 epoch total loss 1.16040599\n",
      "Trained batch 71 batch loss 1.2705766 epoch total loss 1.16195762\n",
      "Trained batch 72 batch loss 1.22308254 epoch total loss 1.16280663\n",
      "Trained batch 73 batch loss 1.21855581 epoch total loss 1.1635704\n",
      "Trained batch 74 batch loss 1.24144888 epoch total loss 1.16462278\n",
      "Trained batch 75 batch loss 1.18250144 epoch total loss 1.1648612\n",
      "Trained batch 76 batch loss 1.10677207 epoch total loss 1.16409683\n",
      "Trained batch 77 batch loss 1.28833282 epoch total loss 1.16571021\n",
      "Trained batch 78 batch loss 1.15043795 epoch total loss 1.16551447\n",
      "Trained batch 79 batch loss 1.09982395 epoch total loss 1.16468287\n",
      "Trained batch 80 batch loss 1.1312952 epoch total loss 1.16426551\n",
      "Trained batch 81 batch loss 1.18573606 epoch total loss 1.16453063\n",
      "Trained batch 82 batch loss 1.2056725 epoch total loss 1.16503239\n",
      "Trained batch 83 batch loss 1.18519056 epoch total loss 1.16527522\n",
      "Trained batch 84 batch loss 1.13756561 epoch total loss 1.16494536\n",
      "Trained batch 85 batch loss 1.1936754 epoch total loss 1.16528332\n",
      "Trained batch 86 batch loss 1.24632061 epoch total loss 1.16622555\n",
      "Trained batch 87 batch loss 1.06938994 epoch total loss 1.1651125\n",
      "Trained batch 88 batch loss 1.16369629 epoch total loss 1.1650964\n",
      "Trained batch 89 batch loss 1.41064262 epoch total loss 1.16785538\n",
      "Trained batch 90 batch loss 1.40425348 epoch total loss 1.17048204\n",
      "Trained batch 91 batch loss 1.26886463 epoch total loss 1.17156315\n",
      "Trained batch 92 batch loss 1.28131521 epoch total loss 1.1727562\n",
      "Trained batch 93 batch loss 1.17759967 epoch total loss 1.17280829\n",
      "Trained batch 94 batch loss 1.24385643 epoch total loss 1.17356408\n",
      "Trained batch 95 batch loss 1.16170025 epoch total loss 1.17343915\n",
      "Trained batch 96 batch loss 1.19258881 epoch total loss 1.1736387\n",
      "Trained batch 97 batch loss 1.10390663 epoch total loss 1.17291975\n",
      "Trained batch 98 batch loss 1.19855618 epoch total loss 1.1731813\n",
      "Trained batch 99 batch loss 1.18895817 epoch total loss 1.17334068\n",
      "Trained batch 100 batch loss 1.14043164 epoch total loss 1.17301166\n",
      "Trained batch 101 batch loss 1.0452317 epoch total loss 1.17174649\n",
      "Trained batch 102 batch loss 1.01479149 epoch total loss 1.17020774\n",
      "Trained batch 103 batch loss 0.964142084 epoch total loss 1.16820717\n",
      "Trained batch 104 batch loss 0.945409536 epoch total loss 1.16606486\n",
      "Trained batch 105 batch loss 1.22649622 epoch total loss 1.1666404\n",
      "Trained batch 106 batch loss 1.21977341 epoch total loss 1.16714156\n",
      "Trained batch 107 batch loss 1.21540713 epoch total loss 1.16759264\n",
      "Trained batch 108 batch loss 1.26402259 epoch total loss 1.16848552\n",
      "Trained batch 109 batch loss 1.30044568 epoch total loss 1.16969621\n",
      "Trained batch 110 batch loss 1.40682352 epoch total loss 1.17185199\n",
      "Trained batch 111 batch loss 1.21802592 epoch total loss 1.17226803\n",
      "Trained batch 112 batch loss 1.0793457 epoch total loss 1.17143834\n",
      "Trained batch 113 batch loss 1.09192634 epoch total loss 1.17073464\n",
      "Trained batch 114 batch loss 1.22735155 epoch total loss 1.17123127\n",
      "Trained batch 115 batch loss 1.11340022 epoch total loss 1.17072845\n",
      "Trained batch 116 batch loss 1.09112453 epoch total loss 1.17004228\n",
      "Trained batch 117 batch loss 1.18389869 epoch total loss 1.17016065\n",
      "Trained batch 118 batch loss 1.18142748 epoch total loss 1.17025614\n",
      "Trained batch 119 batch loss 1.25971496 epoch total loss 1.17100799\n",
      "Trained batch 120 batch loss 1.2703253 epoch total loss 1.17183554\n",
      "Trained batch 121 batch loss 1.27412105 epoch total loss 1.17268097\n",
      "Trained batch 122 batch loss 1.15056336 epoch total loss 1.17249966\n",
      "Trained batch 123 batch loss 1.07967639 epoch total loss 1.17174494\n",
      "Trained batch 124 batch loss 1.0719707 epoch total loss 1.1709404\n",
      "Trained batch 125 batch loss 1.25018191 epoch total loss 1.17157435\n",
      "Trained batch 126 batch loss 1.09071398 epoch total loss 1.17093253\n",
      "Trained batch 127 batch loss 1.11766076 epoch total loss 1.17051315\n",
      "Trained batch 128 batch loss 1.09168315 epoch total loss 1.16989732\n",
      "Trained batch 129 batch loss 1.06420016 epoch total loss 1.16907787\n",
      "Trained batch 130 batch loss 1.14336801 epoch total loss 1.16888022\n",
      "Trained batch 131 batch loss 1.06902611 epoch total loss 1.168118\n",
      "Trained batch 132 batch loss 1.00149167 epoch total loss 1.16685569\n",
      "Trained batch 133 batch loss 1.03298235 epoch total loss 1.16584921\n",
      "Trained batch 134 batch loss 1.02307045 epoch total loss 1.1647836\n",
      "Trained batch 135 batch loss 1.0692575 epoch total loss 1.16407609\n",
      "Trained batch 136 batch loss 1.10952413 epoch total loss 1.16367495\n",
      "Trained batch 137 batch loss 1.30995417 epoch total loss 1.16474271\n",
      "Trained batch 138 batch loss 1.22691476 epoch total loss 1.1651932\n",
      "Trained batch 139 batch loss 1.1945982 epoch total loss 1.16540468\n",
      "Trained batch 140 batch loss 1.16695166 epoch total loss 1.16541576\n",
      "Trained batch 141 batch loss 1.19690633 epoch total loss 1.16563904\n",
      "Trained batch 142 batch loss 1.05050731 epoch total loss 1.16482818\n",
      "Trained batch 143 batch loss 0.951409936 epoch total loss 1.1633358\n",
      "Trained batch 144 batch loss 1.04201698 epoch total loss 1.16249335\n",
      "Trained batch 145 batch loss 1.24736261 epoch total loss 1.16307867\n",
      "Trained batch 146 batch loss 1.24645293 epoch total loss 1.1636498\n",
      "Trained batch 147 batch loss 1.09944773 epoch total loss 1.16321301\n",
      "Trained batch 148 batch loss 1.15842175 epoch total loss 1.16318059\n",
      "Trained batch 149 batch loss 1.09682834 epoch total loss 1.16273534\n",
      "Trained batch 150 batch loss 1.12616491 epoch total loss 1.16249144\n",
      "Trained batch 151 batch loss 1.09696043 epoch total loss 1.1620574\n",
      "Trained batch 152 batch loss 0.963035405 epoch total loss 1.160748\n",
      "Trained batch 153 batch loss 1.0891124 epoch total loss 1.16027987\n",
      "Trained batch 154 batch loss 1.29523337 epoch total loss 1.16115606\n",
      "Trained batch 155 batch loss 1.12303388 epoch total loss 1.16091013\n",
      "Trained batch 156 batch loss 1.19059563 epoch total loss 1.16110039\n",
      "Trained batch 157 batch loss 1.23054624 epoch total loss 1.16154277\n",
      "Trained batch 158 batch loss 1.34446025 epoch total loss 1.16270053\n",
      "Trained batch 159 batch loss 1.36044788 epoch total loss 1.16394413\n",
      "Trained batch 160 batch loss 1.04137564 epoch total loss 1.16317821\n",
      "Trained batch 161 batch loss 1.02706754 epoch total loss 1.16233277\n",
      "Trained batch 162 batch loss 1.18911123 epoch total loss 1.16249812\n",
      "Trained batch 163 batch loss 1.04747343 epoch total loss 1.1617924\n",
      "Trained batch 164 batch loss 1.02638412 epoch total loss 1.16096675\n",
      "Trained batch 165 batch loss 0.98761 epoch total loss 1.15991604\n",
      "Trained batch 166 batch loss 1.00544989 epoch total loss 1.1589855\n",
      "Trained batch 167 batch loss 0.890984297 epoch total loss 1.15738082\n",
      "Trained batch 168 batch loss 0.860861421 epoch total loss 1.15561581\n",
      "Trained batch 169 batch loss 1.02888787 epoch total loss 1.15486586\n",
      "Trained batch 170 batch loss 1.04487634 epoch total loss 1.15421891\n",
      "Trained batch 171 batch loss 1.07768071 epoch total loss 1.15377128\n",
      "Trained batch 172 batch loss 0.983298421 epoch total loss 1.15278018\n",
      "Trained batch 173 batch loss 0.937522471 epoch total loss 1.15153587\n",
      "Trained batch 174 batch loss 0.98246789 epoch total loss 1.15056419\n",
      "Trained batch 175 batch loss 0.911416292 epoch total loss 1.1491977\n",
      "Trained batch 176 batch loss 0.96859622 epoch total loss 1.14817154\n",
      "Trained batch 177 batch loss 1.04794335 epoch total loss 1.1476053\n",
      "Trained batch 178 batch loss 1.19341898 epoch total loss 1.14786267\n",
      "Trained batch 179 batch loss 1.09420919 epoch total loss 1.14756286\n",
      "Trained batch 180 batch loss 1.24986422 epoch total loss 1.14813125\n",
      "Trained batch 181 batch loss 1.17886555 epoch total loss 1.14830101\n",
      "Trained batch 182 batch loss 1.01159132 epoch total loss 1.14754987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 183 batch loss 0.961568117 epoch total loss 1.14653361\n",
      "Trained batch 184 batch loss 1.0527581 epoch total loss 1.14602399\n",
      "Trained batch 185 batch loss 1.17176294 epoch total loss 1.14616311\n",
      "Trained batch 186 batch loss 1.20917606 epoch total loss 1.1465019\n",
      "Trained batch 187 batch loss 1.12625027 epoch total loss 1.14639366\n",
      "Trained batch 188 batch loss 1.1321125 epoch total loss 1.14631772\n",
      "Trained batch 189 batch loss 1.14223957 epoch total loss 1.14629614\n",
      "Trained batch 190 batch loss 1.10383606 epoch total loss 1.14607263\n",
      "Trained batch 191 batch loss 1.15527773 epoch total loss 1.14612079\n",
      "Trained batch 192 batch loss 1.09608507 epoch total loss 1.1458602\n",
      "Trained batch 193 batch loss 1.15393496 epoch total loss 1.14590204\n",
      "Trained batch 194 batch loss 1.11034703 epoch total loss 1.14571881\n",
      "Trained batch 195 batch loss 1.20390022 epoch total loss 1.14601719\n",
      "Trained batch 196 batch loss 1.38984776 epoch total loss 1.14726114\n",
      "Trained batch 197 batch loss 1.25760806 epoch total loss 1.14782131\n",
      "Trained batch 198 batch loss 1.20938611 epoch total loss 1.14813221\n",
      "Trained batch 199 batch loss 1.11006355 epoch total loss 1.14794099\n",
      "Trained batch 200 batch loss 1.14198256 epoch total loss 1.14791119\n",
      "Trained batch 201 batch loss 1.20449209 epoch total loss 1.14819264\n",
      "Trained batch 202 batch loss 1.11818409 epoch total loss 1.14804411\n",
      "Trained batch 203 batch loss 1.00487232 epoch total loss 1.14733875\n",
      "Trained batch 204 batch loss 1.04792368 epoch total loss 1.14685154\n",
      "Trained batch 205 batch loss 0.973076105 epoch total loss 1.14600384\n",
      "Trained batch 206 batch loss 1.11238265 epoch total loss 1.14584064\n",
      "Trained batch 207 batch loss 1.01825845 epoch total loss 1.14522433\n",
      "Trained batch 208 batch loss 0.998458862 epoch total loss 1.14451873\n",
      "Trained batch 209 batch loss 1.16767025 epoch total loss 1.14462948\n",
      "Trained batch 210 batch loss 1.09965193 epoch total loss 1.14441526\n",
      "Trained batch 211 batch loss 1.10189652 epoch total loss 1.1442138\n",
      "Trained batch 212 batch loss 1.02475572 epoch total loss 1.14365029\n",
      "Trained batch 213 batch loss 1.10655451 epoch total loss 1.14347613\n",
      "Trained batch 214 batch loss 1.20104384 epoch total loss 1.14374518\n",
      "Trained batch 215 batch loss 1.28701329 epoch total loss 1.14441156\n",
      "Trained batch 216 batch loss 1.17572093 epoch total loss 1.14455652\n",
      "Trained batch 217 batch loss 1.16967106 epoch total loss 1.14467227\n",
      "Trained batch 218 batch loss 1.09717417 epoch total loss 1.14445436\n",
      "Trained batch 219 batch loss 1.07958019 epoch total loss 1.14415812\n",
      "Trained batch 220 batch loss 1.16683459 epoch total loss 1.14426112\n",
      "Trained batch 221 batch loss 1.21275938 epoch total loss 1.14457107\n",
      "Trained batch 222 batch loss 1.05765009 epoch total loss 1.14417958\n",
      "Trained batch 223 batch loss 1.07540429 epoch total loss 1.14387119\n",
      "Trained batch 224 batch loss 1.11189985 epoch total loss 1.14372838\n",
      "Trained batch 225 batch loss 0.956104875 epoch total loss 1.14289451\n",
      "Trained batch 226 batch loss 0.92621249 epoch total loss 1.14193583\n",
      "Trained batch 227 batch loss 0.884561181 epoch total loss 1.14080191\n",
      "Trained batch 228 batch loss 0.90895617 epoch total loss 1.13978505\n",
      "Trained batch 229 batch loss 1.08983207 epoch total loss 1.13956702\n",
      "Trained batch 230 batch loss 1.02354217 epoch total loss 1.13906252\n",
      "Trained batch 231 batch loss 0.928586841 epoch total loss 1.13815141\n",
      "Trained batch 232 batch loss 1.07868958 epoch total loss 1.13789499\n",
      "Trained batch 233 batch loss 0.933272123 epoch total loss 1.13701677\n",
      "Trained batch 234 batch loss 1.19907856 epoch total loss 1.13728189\n",
      "Trained batch 235 batch loss 1.13262618 epoch total loss 1.13726211\n",
      "Trained batch 236 batch loss 1.20763767 epoch total loss 1.13756037\n",
      "Trained batch 237 batch loss 1.21591258 epoch total loss 1.13789093\n",
      "Trained batch 238 batch loss 1.16673756 epoch total loss 1.13801217\n",
      "Trained batch 239 batch loss 1.14788103 epoch total loss 1.13805354\n",
      "Trained batch 240 batch loss 1.1205852 epoch total loss 1.1379807\n",
      "Trained batch 241 batch loss 1.16950393 epoch total loss 1.13811147\n",
      "Trained batch 242 batch loss 1.22214246 epoch total loss 1.13845861\n",
      "Trained batch 243 batch loss 1.2372328 epoch total loss 1.13886511\n",
      "Trained batch 244 batch loss 1.10788059 epoch total loss 1.13873816\n",
      "Trained batch 245 batch loss 1.14256251 epoch total loss 1.13875365\n",
      "Trained batch 246 batch loss 1.05359495 epoch total loss 1.13840747\n",
      "Trained batch 247 batch loss 1.0457598 epoch total loss 1.13803232\n",
      "Trained batch 248 batch loss 1.12091517 epoch total loss 1.13796329\n",
      "Trained batch 249 batch loss 1.12533629 epoch total loss 1.13791263\n",
      "Trained batch 250 batch loss 1.1160996 epoch total loss 1.13782537\n",
      "Trained batch 251 batch loss 0.990775764 epoch total loss 1.13723946\n",
      "Trained batch 252 batch loss 1.05069852 epoch total loss 1.13689601\n",
      "Trained batch 253 batch loss 1.01953697 epoch total loss 1.13643217\n",
      "Trained batch 254 batch loss 1.41338718 epoch total loss 1.13752258\n",
      "Trained batch 255 batch loss 1.20207477 epoch total loss 1.13777578\n",
      "Trained batch 256 batch loss 1.05448222 epoch total loss 1.13745034\n",
      "Trained batch 257 batch loss 0.97189194 epoch total loss 1.13680613\n",
      "Trained batch 258 batch loss 0.908839107 epoch total loss 1.13592255\n",
      "Trained batch 259 batch loss 0.768607 epoch total loss 1.13450444\n",
      "Trained batch 260 batch loss 0.94200182 epoch total loss 1.13376403\n",
      "Trained batch 261 batch loss 1.0645442 epoch total loss 1.13349891\n",
      "Trained batch 262 batch loss 1.03236735 epoch total loss 1.13311291\n",
      "Trained batch 263 batch loss 0.932188153 epoch total loss 1.1323489\n",
      "Trained batch 264 batch loss 0.969672143 epoch total loss 1.1317327\n",
      "Trained batch 265 batch loss 0.953625321 epoch total loss 1.1310606\n",
      "Trained batch 266 batch loss 1.06356347 epoch total loss 1.1308068\n",
      "Trained batch 267 batch loss 1.10146713 epoch total loss 1.13069701\n",
      "Trained batch 268 batch loss 1.12201059 epoch total loss 1.13066459\n",
      "Trained batch 269 batch loss 0.994739532 epoch total loss 1.13015926\n",
      "Trained batch 270 batch loss 1.20354521 epoch total loss 1.13043106\n",
      "Trained batch 271 batch loss 1.25831807 epoch total loss 1.13090312\n",
      "Trained batch 272 batch loss 1.27937961 epoch total loss 1.13144898\n",
      "Trained batch 273 batch loss 1.19040918 epoch total loss 1.13166487\n",
      "Trained batch 274 batch loss 1.12291276 epoch total loss 1.13163304\n",
      "Trained batch 275 batch loss 1.00437641 epoch total loss 1.13117015\n",
      "Trained batch 276 batch loss 1.15896106 epoch total loss 1.13127089\n",
      "Trained batch 277 batch loss 1.18606949 epoch total loss 1.13146877\n",
      "Trained batch 278 batch loss 1.28364384 epoch total loss 1.13201606\n",
      "Trained batch 279 batch loss 1.23964858 epoch total loss 1.13240182\n",
      "Trained batch 280 batch loss 1.18472612 epoch total loss 1.13258874\n",
      "Trained batch 281 batch loss 1.07356286 epoch total loss 1.1323787\n",
      "Trained batch 282 batch loss 1.09932399 epoch total loss 1.13226151\n",
      "Trained batch 283 batch loss 1.1493758 epoch total loss 1.13232207\n",
      "Trained batch 284 batch loss 1.19040203 epoch total loss 1.13252652\n",
      "Trained batch 285 batch loss 1.24188662 epoch total loss 1.13291025\n",
      "Trained batch 286 batch loss 1.21449387 epoch total loss 1.13319552\n",
      "Trained batch 287 batch loss 1.26641536 epoch total loss 1.13365972\n",
      "Trained batch 288 batch loss 1.1937983 epoch total loss 1.13386858\n",
      "Trained batch 289 batch loss 1.2713002 epoch total loss 1.1343441\n",
      "Trained batch 290 batch loss 1.23455393 epoch total loss 1.13468969\n",
      "Trained batch 291 batch loss 1.21006298 epoch total loss 1.13494861\n",
      "Trained batch 292 batch loss 1.1446228 epoch total loss 1.13498175\n",
      "Trained batch 293 batch loss 1.22372699 epoch total loss 1.13528466\n",
      "Trained batch 294 batch loss 1.23082757 epoch total loss 1.13560963\n",
      "Trained batch 295 batch loss 1.21642053 epoch total loss 1.13588357\n",
      "Trained batch 296 batch loss 1.19636059 epoch total loss 1.13608789\n",
      "Trained batch 297 batch loss 1.18263793 epoch total loss 1.13624465\n",
      "Trained batch 298 batch loss 1.03635478 epoch total loss 1.13590944\n",
      "Trained batch 299 batch loss 1.03660464 epoch total loss 1.1355772\n",
      "Trained batch 300 batch loss 1.05622387 epoch total loss 1.13531268\n",
      "Trained batch 301 batch loss 1.09882736 epoch total loss 1.13519144\n",
      "Trained batch 302 batch loss 1.1803143 epoch total loss 1.13534093\n",
      "Trained batch 303 batch loss 1.14705169 epoch total loss 1.13537955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 304 batch loss 1.06766069 epoch total loss 1.13515687\n",
      "Trained batch 305 batch loss 1.00040483 epoch total loss 1.13471496\n",
      "Trained batch 306 batch loss 1.0642724 epoch total loss 1.13448477\n",
      "Trained batch 307 batch loss 1.03303468 epoch total loss 1.13415432\n",
      "Trained batch 308 batch loss 1.12650061 epoch total loss 1.13412941\n",
      "Trained batch 309 batch loss 1.13310862 epoch total loss 1.13412619\n",
      "Trained batch 310 batch loss 0.94219625 epoch total loss 1.13350701\n",
      "Trained batch 311 batch loss 1.03331399 epoch total loss 1.13318491\n",
      "Trained batch 312 batch loss 1.21963239 epoch total loss 1.13346195\n",
      "Trained batch 313 batch loss 1.0515331 epoch total loss 1.13320029\n",
      "Trained batch 314 batch loss 1.1466732 epoch total loss 1.1332432\n",
      "Trained batch 315 batch loss 1.09969747 epoch total loss 1.13313663\n",
      "Trained batch 316 batch loss 0.982376933 epoch total loss 1.13265967\n",
      "Trained batch 317 batch loss 1.10533333 epoch total loss 1.13257349\n",
      "Trained batch 318 batch loss 1.04060972 epoch total loss 1.13228428\n",
      "Trained batch 319 batch loss 1.22620344 epoch total loss 1.13257873\n",
      "Trained batch 320 batch loss 1.21253884 epoch total loss 1.13282847\n",
      "Trained batch 321 batch loss 1.31096685 epoch total loss 1.13338351\n",
      "Trained batch 322 batch loss 1.29644585 epoch total loss 1.13388991\n",
      "Trained batch 323 batch loss 1.06273496 epoch total loss 1.13366961\n",
      "Trained batch 324 batch loss 1.0721972 epoch total loss 1.13348\n",
      "Trained batch 325 batch loss 1.11157155 epoch total loss 1.13341248\n",
      "Trained batch 326 batch loss 1.23261523 epoch total loss 1.13371682\n",
      "Trained batch 327 batch loss 1.07775521 epoch total loss 1.13354564\n",
      "Trained batch 328 batch loss 1.23159981 epoch total loss 1.13384461\n",
      "Trained batch 329 batch loss 1.11169314 epoch total loss 1.13377726\n",
      "Trained batch 330 batch loss 1.18307018 epoch total loss 1.13392663\n",
      "Trained batch 331 batch loss 1.16979265 epoch total loss 1.13403499\n",
      "Trained batch 332 batch loss 1.06637907 epoch total loss 1.13383126\n",
      "Trained batch 333 batch loss 1.0821923 epoch total loss 1.13367617\n",
      "Trained batch 334 batch loss 1.14739275 epoch total loss 1.1337173\n",
      "Trained batch 335 batch loss 1.17626607 epoch total loss 1.13384426\n",
      "Trained batch 336 batch loss 1.24490952 epoch total loss 1.13417482\n",
      "Trained batch 337 batch loss 1.21776736 epoch total loss 1.1344229\n",
      "Trained batch 338 batch loss 1.22682047 epoch total loss 1.13469625\n",
      "Trained batch 339 batch loss 1.306391 epoch total loss 1.13520265\n",
      "Trained batch 340 batch loss 1.22766197 epoch total loss 1.13547456\n",
      "Trained batch 341 batch loss 1.11935902 epoch total loss 1.13542736\n",
      "Trained batch 342 batch loss 1.16938853 epoch total loss 1.13552666\n",
      "Trained batch 343 batch loss 1.15201008 epoch total loss 1.1355747\n",
      "Trained batch 344 batch loss 1.08246446 epoch total loss 1.13542032\n",
      "Trained batch 345 batch loss 1.08596039 epoch total loss 1.13527703\n",
      "Trained batch 346 batch loss 1.04059529 epoch total loss 1.13500333\n",
      "Trained batch 347 batch loss 1.01247454 epoch total loss 1.13465023\n",
      "Trained batch 348 batch loss 1.05510819 epoch total loss 1.13442171\n",
      "Trained batch 349 batch loss 1.20334768 epoch total loss 1.13461912\n",
      "Trained batch 350 batch loss 1.12309957 epoch total loss 1.13458622\n",
      "Trained batch 351 batch loss 1.24274528 epoch total loss 1.13489437\n",
      "Trained batch 352 batch loss 1.28480828 epoch total loss 1.13532031\n",
      "Trained batch 353 batch loss 1.09887147 epoch total loss 1.13521707\n",
      "Trained batch 354 batch loss 1.14589548 epoch total loss 1.13524723\n",
      "Trained batch 355 batch loss 1.17926049 epoch total loss 1.13537121\n",
      "Trained batch 356 batch loss 1.34771526 epoch total loss 1.13596773\n",
      "Trained batch 357 batch loss 1.31103873 epoch total loss 1.13645816\n",
      "Trained batch 358 batch loss 1.05002987 epoch total loss 1.13621664\n",
      "Trained batch 359 batch loss 1.17291808 epoch total loss 1.13631892\n",
      "Trained batch 360 batch loss 1.17133927 epoch total loss 1.13641608\n",
      "Trained batch 361 batch loss 1.16134441 epoch total loss 1.13648522\n",
      "Trained batch 362 batch loss 1.13709152 epoch total loss 1.13648689\n",
      "Trained batch 363 batch loss 1.16316521 epoch total loss 1.13656032\n",
      "Trained batch 364 batch loss 1.17611837 epoch total loss 1.13666904\n",
      "Trained batch 365 batch loss 1.09917867 epoch total loss 1.13656628\n",
      "Trained batch 366 batch loss 1.22948813 epoch total loss 1.1368202\n",
      "Trained batch 367 batch loss 1.19629097 epoch total loss 1.13698232\n",
      "Trained batch 368 batch loss 1.14254522 epoch total loss 1.13699734\n",
      "Trained batch 369 batch loss 1.05007362 epoch total loss 1.13676178\n",
      "Trained batch 370 batch loss 0.864136279 epoch total loss 1.13602495\n",
      "Trained batch 371 batch loss 1.09026301 epoch total loss 1.13590169\n",
      "Trained batch 372 batch loss 1.19129467 epoch total loss 1.13605058\n",
      "Trained batch 373 batch loss 1.14172506 epoch total loss 1.13606572\n",
      "Trained batch 374 batch loss 1.25500584 epoch total loss 1.13638377\n",
      "Trained batch 375 batch loss 1.17745423 epoch total loss 1.13649333\n",
      "Trained batch 376 batch loss 1.11143899 epoch total loss 1.13642669\n",
      "Trained batch 377 batch loss 1.01818836 epoch total loss 1.13611305\n",
      "Trained batch 378 batch loss 0.9762609 epoch total loss 1.13569021\n",
      "Trained batch 379 batch loss 1.11900675 epoch total loss 1.13564622\n",
      "Trained batch 380 batch loss 1.01850915 epoch total loss 1.13533795\n",
      "Trained batch 381 batch loss 1.12482357 epoch total loss 1.13531041\n",
      "Trained batch 382 batch loss 1.26772928 epoch total loss 1.13565707\n",
      "Trained batch 383 batch loss 1.17652142 epoch total loss 1.13576365\n",
      "Trained batch 384 batch loss 1.22903061 epoch total loss 1.13600659\n",
      "Trained batch 385 batch loss 1.19528735 epoch total loss 1.13616061\n",
      "Trained batch 386 batch loss 1.25483274 epoch total loss 1.13646793\n",
      "Trained batch 387 batch loss 1.19079685 epoch total loss 1.13660836\n",
      "Trained batch 388 batch loss 1.22775495 epoch total loss 1.1368432\n",
      "Trained batch 389 batch loss 1.27766395 epoch total loss 1.13720524\n",
      "Trained batch 390 batch loss 1.12333655 epoch total loss 1.1371696\n",
      "Trained batch 391 batch loss 1.09082818 epoch total loss 1.13705111\n",
      "Trained batch 392 batch loss 1.2086693 epoch total loss 1.13723385\n",
      "Trained batch 393 batch loss 1.36588657 epoch total loss 1.13781559\n",
      "Trained batch 394 batch loss 1.27550948 epoch total loss 1.13816512\n",
      "Trained batch 395 batch loss 1.3018589 epoch total loss 1.13857949\n",
      "Trained batch 396 batch loss 1.14805794 epoch total loss 1.13860345\n",
      "Trained batch 397 batch loss 1.09052157 epoch total loss 1.13848233\n",
      "Trained batch 398 batch loss 1.14640772 epoch total loss 1.13850224\n",
      "Trained batch 399 batch loss 1.0838728 epoch total loss 1.13836527\n",
      "Trained batch 400 batch loss 1.19291544 epoch total loss 1.13850164\n",
      "Trained batch 401 batch loss 1.08285117 epoch total loss 1.13836277\n",
      "Trained batch 402 batch loss 1.0371865 epoch total loss 1.13811111\n",
      "Trained batch 403 batch loss 0.99638778 epoch total loss 1.13775957\n",
      "Trained batch 404 batch loss 0.880445719 epoch total loss 1.13712263\n",
      "Trained batch 405 batch loss 0.933662415 epoch total loss 1.13662016\n",
      "Trained batch 406 batch loss 1.13020182 epoch total loss 1.13660431\n",
      "Trained batch 407 batch loss 1.48871529 epoch total loss 1.13746941\n",
      "Trained batch 408 batch loss 1.36999702 epoch total loss 1.13803935\n",
      "Trained batch 409 batch loss 1.25953245 epoch total loss 1.13833642\n",
      "Trained batch 410 batch loss 1.25920808 epoch total loss 1.13863122\n",
      "Trained batch 411 batch loss 1.30013096 epoch total loss 1.13902426\n",
      "Trained batch 412 batch loss 1.27774811 epoch total loss 1.1393609\n",
      "Trained batch 413 batch loss 1.12243748 epoch total loss 1.1393199\n",
      "Trained batch 414 batch loss 1.19687712 epoch total loss 1.13945889\n",
      "Trained batch 415 batch loss 1.07964301 epoch total loss 1.13931477\n",
      "Trained batch 416 batch loss 1.14035702 epoch total loss 1.13931727\n",
      "Trained batch 417 batch loss 1.20423436 epoch total loss 1.13947296\n",
      "Trained batch 418 batch loss 1.19242549 epoch total loss 1.13959956\n",
      "Trained batch 419 batch loss 1.0955795 epoch total loss 1.13949454\n",
      "Trained batch 420 batch loss 1.23159 epoch total loss 1.13971388\n",
      "Trained batch 421 batch loss 1.1606096 epoch total loss 1.13976347\n",
      "Trained batch 422 batch loss 1.18798864 epoch total loss 1.1398778\n",
      "Trained batch 423 batch loss 1.26393247 epoch total loss 1.14017105\n",
      "Trained batch 424 batch loss 1.17760885 epoch total loss 1.14025939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 425 batch loss 1.14356112 epoch total loss 1.14026713\n",
      "Trained batch 426 batch loss 1.19829094 epoch total loss 1.14040339\n",
      "Trained batch 427 batch loss 1.07049382 epoch total loss 1.1402396\n",
      "Trained batch 428 batch loss 1.12913418 epoch total loss 1.14021361\n",
      "Trained batch 429 batch loss 1.11446166 epoch total loss 1.14015365\n",
      "Trained batch 430 batch loss 1.08888793 epoch total loss 1.14003444\n",
      "Trained batch 431 batch loss 1.07244873 epoch total loss 1.13987768\n",
      "Trained batch 432 batch loss 1.04503489 epoch total loss 1.13965809\n",
      "Trained batch 433 batch loss 1.00241804 epoch total loss 1.13934112\n",
      "Trained batch 434 batch loss 1.0423919 epoch total loss 1.13911772\n",
      "Trained batch 435 batch loss 1.06694114 epoch total loss 1.1389519\n",
      "Trained batch 436 batch loss 1.30201304 epoch total loss 1.13932586\n",
      "Trained batch 437 batch loss 1.20567048 epoch total loss 1.13947761\n",
      "Trained batch 438 batch loss 1.16817749 epoch total loss 1.13954318\n",
      "Trained batch 439 batch loss 0.89544332 epoch total loss 1.13898718\n",
      "Trained batch 440 batch loss 0.952917397 epoch total loss 1.13856423\n",
      "Trained batch 441 batch loss 1.0750953 epoch total loss 1.13842034\n",
      "Trained batch 442 batch loss 1.29689598 epoch total loss 1.13877892\n",
      "Trained batch 443 batch loss 1.22797978 epoch total loss 1.13898027\n",
      "Trained batch 444 batch loss 1.18179178 epoch total loss 1.13907659\n",
      "Trained batch 445 batch loss 1.17905474 epoch total loss 1.13916647\n",
      "Trained batch 446 batch loss 1.02201331 epoch total loss 1.13890374\n",
      "Trained batch 447 batch loss 1.09063804 epoch total loss 1.13879573\n",
      "Trained batch 448 batch loss 1.09738064 epoch total loss 1.13870335\n",
      "Trained batch 449 batch loss 1.21247065 epoch total loss 1.13886762\n",
      "Trained batch 450 batch loss 1.2806679 epoch total loss 1.13918269\n",
      "Trained batch 451 batch loss 1.15154421 epoch total loss 1.1392101\n",
      "Trained batch 452 batch loss 1.01190686 epoch total loss 1.13892841\n",
      "Trained batch 453 batch loss 1.05663157 epoch total loss 1.13874674\n",
      "Trained batch 454 batch loss 1.01178288 epoch total loss 1.13846707\n",
      "Trained batch 455 batch loss 1.20548034 epoch total loss 1.13861442\n",
      "Trained batch 456 batch loss 1.24318361 epoch total loss 1.13884377\n",
      "Trained batch 457 batch loss 1.22608864 epoch total loss 1.13903463\n",
      "Trained batch 458 batch loss 1.29744673 epoch total loss 1.13938046\n",
      "Trained batch 459 batch loss 1.19241095 epoch total loss 1.13949597\n",
      "Trained batch 460 batch loss 1.24475241 epoch total loss 1.13972473\n",
      "Trained batch 461 batch loss 1.26068425 epoch total loss 1.13998711\n",
      "Trained batch 462 batch loss 1.13904989 epoch total loss 1.13998508\n",
      "Trained batch 463 batch loss 1.05928695 epoch total loss 1.13981068\n",
      "Trained batch 464 batch loss 1.13964689 epoch total loss 1.13981032\n",
      "Trained batch 465 batch loss 0.973953 epoch total loss 1.13945365\n",
      "Trained batch 466 batch loss 1.00371706 epoch total loss 1.13916242\n",
      "Trained batch 467 batch loss 1.06656444 epoch total loss 1.13900697\n",
      "Trained batch 468 batch loss 0.976391435 epoch total loss 1.13865948\n",
      "Trained batch 469 batch loss 0.845113575 epoch total loss 1.13803351\n",
      "Trained batch 470 batch loss 0.89289391 epoch total loss 1.13751197\n",
      "Trained batch 471 batch loss 0.820473254 epoch total loss 1.13683891\n",
      "Trained batch 472 batch loss 1.05668759 epoch total loss 1.13666904\n",
      "Trained batch 473 batch loss 1.14200139 epoch total loss 1.13668048\n",
      "Trained batch 474 batch loss 1.07388115 epoch total loss 1.13654792\n",
      "Trained batch 475 batch loss 1.03986144 epoch total loss 1.13634431\n",
      "Trained batch 476 batch loss 1.01234865 epoch total loss 1.13608372\n",
      "Trained batch 477 batch loss 1.17439365 epoch total loss 1.13616407\n",
      "Trained batch 478 batch loss 1.22620273 epoch total loss 1.13635242\n",
      "Trained batch 479 batch loss 1.22541964 epoch total loss 1.13653827\n",
      "Trained batch 480 batch loss 1.12951648 epoch total loss 1.13652372\n",
      "Trained batch 481 batch loss 1.18301666 epoch total loss 1.1366204\n",
      "Trained batch 482 batch loss 1.08206058 epoch total loss 1.13650715\n",
      "Trained batch 483 batch loss 1.2044543 epoch total loss 1.13664782\n",
      "Trained batch 484 batch loss 1.17820215 epoch total loss 1.13673377\n",
      "Trained batch 485 batch loss 1.15924942 epoch total loss 1.13678014\n",
      "Trained batch 486 batch loss 1.15352821 epoch total loss 1.13681459\n",
      "Trained batch 487 batch loss 1.14471889 epoch total loss 1.13683081\n",
      "Trained batch 488 batch loss 1.15379906 epoch total loss 1.13686562\n",
      "Trained batch 489 batch loss 1.09808588 epoch total loss 1.13678622\n",
      "Trained batch 490 batch loss 1.21522832 epoch total loss 1.13694632\n",
      "Trained batch 491 batch loss 1.17540169 epoch total loss 1.13702464\n",
      "Trained batch 492 batch loss 1.03262019 epoch total loss 1.13681245\n",
      "Trained batch 493 batch loss 0.95634532 epoch total loss 1.13644636\n",
      "Trained batch 494 batch loss 0.985206425 epoch total loss 1.13614023\n",
      "Trained batch 495 batch loss 0.970689893 epoch total loss 1.13580608\n",
      "Trained batch 496 batch loss 1.11064565 epoch total loss 1.1357553\n",
      "Trained batch 497 batch loss 1.02385163 epoch total loss 1.13553023\n",
      "Trained batch 498 batch loss 0.912616849 epoch total loss 1.1350826\n",
      "Trained batch 499 batch loss 1.08073664 epoch total loss 1.13497365\n",
      "Trained batch 500 batch loss 1.2195313 epoch total loss 1.1351428\n",
      "Trained batch 501 batch loss 1.11326146 epoch total loss 1.13509917\n",
      "Trained batch 502 batch loss 1.22147465 epoch total loss 1.13527131\n",
      "Trained batch 503 batch loss 1.17997527 epoch total loss 1.13536024\n",
      "Trained batch 504 batch loss 1.16200817 epoch total loss 1.13541305\n",
      "Trained batch 505 batch loss 1.13380432 epoch total loss 1.13540983\n",
      "Trained batch 506 batch loss 1.16413927 epoch total loss 1.13546658\n",
      "Trained batch 507 batch loss 1.22995913 epoch total loss 1.13565302\n",
      "Trained batch 508 batch loss 1.05330801 epoch total loss 1.13549089\n",
      "Trained batch 509 batch loss 1.0894165 epoch total loss 1.1354003\n",
      "Trained batch 510 batch loss 1.05445933 epoch total loss 1.13524163\n",
      "Trained batch 511 batch loss 1.09673643 epoch total loss 1.13516629\n",
      "Trained batch 512 batch loss 1.07364738 epoch total loss 1.13504612\n",
      "Trained batch 513 batch loss 1.07909203 epoch total loss 1.13493705\n",
      "Trained batch 514 batch loss 0.96929 epoch total loss 1.13461483\n",
      "Trained batch 515 batch loss 0.924572885 epoch total loss 1.13420689\n",
      "Trained batch 516 batch loss 1.13428235 epoch total loss 1.13420713\n",
      "Trained batch 517 batch loss 1.11548853 epoch total loss 1.13417089\n",
      "Trained batch 518 batch loss 1.17808974 epoch total loss 1.13425565\n",
      "Trained batch 519 batch loss 1.18033767 epoch total loss 1.13434446\n",
      "Trained batch 520 batch loss 1.14155579 epoch total loss 1.13435829\n",
      "Trained batch 521 batch loss 1.19779527 epoch total loss 1.13448012\n",
      "Trained batch 522 batch loss 1.13336897 epoch total loss 1.13447797\n",
      "Trained batch 523 batch loss 1.2372849 epoch total loss 1.13467455\n",
      "Trained batch 524 batch loss 1.17336178 epoch total loss 1.13474834\n",
      "Trained batch 525 batch loss 1.07186472 epoch total loss 1.13462853\n",
      "Trained batch 526 batch loss 1.0787046 epoch total loss 1.1345222\n",
      "Trained batch 527 batch loss 1.01667941 epoch total loss 1.13429856\n",
      "Trained batch 528 batch loss 1.03247464 epoch total loss 1.13410568\n",
      "Trained batch 529 batch loss 1.01980376 epoch total loss 1.13388956\n",
      "Trained batch 530 batch loss 1.06147039 epoch total loss 1.13375294\n",
      "Trained batch 531 batch loss 0.938558698 epoch total loss 1.1333853\n",
      "Trained batch 532 batch loss 0.990808904 epoch total loss 1.1331172\n",
      "Trained batch 533 batch loss 1.01542532 epoch total loss 1.13289642\n",
      "Trained batch 534 batch loss 1.22004318 epoch total loss 1.13305962\n",
      "Trained batch 535 batch loss 1.08620787 epoch total loss 1.132972\n",
      "Trained batch 536 batch loss 1.15840352 epoch total loss 1.13301945\n",
      "Trained batch 537 batch loss 1.1524471 epoch total loss 1.13305557\n",
      "Trained batch 538 batch loss 1.08974588 epoch total loss 1.1329751\n",
      "Trained batch 539 batch loss 1.04250598 epoch total loss 1.13280714\n",
      "Trained batch 540 batch loss 1.09032917 epoch total loss 1.13272858\n",
      "Trained batch 541 batch loss 1.20003676 epoch total loss 1.13285291\n",
      "Trained batch 542 batch loss 1.10800242 epoch total loss 1.13280714\n",
      "Trained batch 543 batch loss 1.15733492 epoch total loss 1.13285232\n",
      "Trained batch 544 batch loss 1.12527561 epoch total loss 1.13283837\n",
      "Trained batch 545 batch loss 1.12418222 epoch total loss 1.13282263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 546 batch loss 1.12563157 epoch total loss 1.1328094\n",
      "Trained batch 547 batch loss 1.16743946 epoch total loss 1.1328727\n",
      "Trained batch 548 batch loss 1.05349684 epoch total loss 1.13272774\n",
      "Trained batch 549 batch loss 1.14072895 epoch total loss 1.1327424\n",
      "Trained batch 550 batch loss 1.11244524 epoch total loss 1.13270545\n",
      "Trained batch 551 batch loss 0.980167508 epoch total loss 1.13242853\n",
      "Trained batch 552 batch loss 1.07467389 epoch total loss 1.13232386\n",
      "Trained batch 553 batch loss 1.10311007 epoch total loss 1.13227105\n",
      "Trained batch 554 batch loss 1.14049709 epoch total loss 1.13228583\n",
      "Trained batch 555 batch loss 1.24946225 epoch total loss 1.13249695\n",
      "Trained batch 556 batch loss 1.08010244 epoch total loss 1.13240266\n",
      "Trained batch 557 batch loss 1.20127749 epoch total loss 1.1325264\n",
      "Trained batch 558 batch loss 1.13263226 epoch total loss 1.13252652\n",
      "Trained batch 559 batch loss 1.09470367 epoch total loss 1.13245893\n",
      "Trained batch 560 batch loss 1.01261091 epoch total loss 1.13224494\n",
      "Trained batch 561 batch loss 1.14583838 epoch total loss 1.13226914\n",
      "Trained batch 562 batch loss 1.02729893 epoch total loss 1.13208234\n",
      "Trained batch 563 batch loss 1.06176162 epoch total loss 1.13195741\n",
      "Trained batch 564 batch loss 1.16789579 epoch total loss 1.13202119\n",
      "Trained batch 565 batch loss 1.33385682 epoch total loss 1.13237846\n",
      "Trained batch 566 batch loss 1.06313145 epoch total loss 1.13225603\n",
      "Trained batch 567 batch loss 1.25880587 epoch total loss 1.13247919\n",
      "Trained batch 568 batch loss 1.18897867 epoch total loss 1.13257873\n",
      "Trained batch 569 batch loss 1.20774889 epoch total loss 1.13271081\n",
      "Trained batch 570 batch loss 1.14447165 epoch total loss 1.13273144\n",
      "Trained batch 571 batch loss 1.13344121 epoch total loss 1.13273263\n",
      "Trained batch 572 batch loss 1.19579172 epoch total loss 1.1328429\n",
      "Trained batch 573 batch loss 1.26564074 epoch total loss 1.13307464\n",
      "Trained batch 574 batch loss 1.28321278 epoch total loss 1.13333619\n",
      "Trained batch 575 batch loss 1.21350789 epoch total loss 1.13347566\n",
      "Trained batch 576 batch loss 1.22987378 epoch total loss 1.13364291\n",
      "Trained batch 577 batch loss 1.06927 epoch total loss 1.13353133\n",
      "Trained batch 578 batch loss 1.22125673 epoch total loss 1.1336832\n",
      "Trained batch 579 batch loss 1.12138343 epoch total loss 1.13366199\n",
      "Trained batch 580 batch loss 1.21169376 epoch total loss 1.13379645\n",
      "Trained batch 581 batch loss 1.13734221 epoch total loss 1.13380253\n",
      "Trained batch 582 batch loss 1.15144229 epoch total loss 1.13383281\n",
      "Trained batch 583 batch loss 1.2145772 epoch total loss 1.13397133\n",
      "Trained batch 584 batch loss 1.18880117 epoch total loss 1.13406515\n",
      "Trained batch 585 batch loss 1.15735531 epoch total loss 1.13410497\n",
      "Trained batch 586 batch loss 1.20273447 epoch total loss 1.13422215\n",
      "Trained batch 587 batch loss 1.10609233 epoch total loss 1.13417423\n",
      "Trained batch 588 batch loss 1.09387612 epoch total loss 1.13410568\n",
      "Trained batch 589 batch loss 1.09293509 epoch total loss 1.13403583\n",
      "Trained batch 590 batch loss 1.21356511 epoch total loss 1.13417053\n",
      "Trained batch 591 batch loss 1.31157625 epoch total loss 1.13447082\n",
      "Trained batch 592 batch loss 1.18777835 epoch total loss 1.13456082\n",
      "Trained batch 593 batch loss 1.19155443 epoch total loss 1.13465691\n",
      "Trained batch 594 batch loss 1.18292677 epoch total loss 1.13473821\n",
      "Trained batch 595 batch loss 1.20275354 epoch total loss 1.13485253\n",
      "Trained batch 596 batch loss 1.21618056 epoch total loss 1.13498902\n",
      "Trained batch 597 batch loss 1.09110618 epoch total loss 1.13491547\n",
      "Trained batch 598 batch loss 1.08478129 epoch total loss 1.13483167\n",
      "Trained batch 599 batch loss 1.12807226 epoch total loss 1.13482034\n",
      "Trained batch 600 batch loss 1.14008367 epoch total loss 1.13482904\n",
      "Trained batch 601 batch loss 0.9966048 epoch total loss 1.13459909\n",
      "Trained batch 602 batch loss 0.976958871 epoch total loss 1.13433719\n",
      "Trained batch 603 batch loss 1.13748395 epoch total loss 1.13434243\n",
      "Trained batch 604 batch loss 1.09431398 epoch total loss 1.13427615\n",
      "Trained batch 605 batch loss 1.07615733 epoch total loss 1.13418007\n",
      "Trained batch 606 batch loss 1.02562225 epoch total loss 1.13400102\n",
      "Trained batch 607 batch loss 1.05903518 epoch total loss 1.1338774\n",
      "Trained batch 608 batch loss 1.01005626 epoch total loss 1.13367379\n",
      "Trained batch 609 batch loss 1.0631938 epoch total loss 1.13355803\n",
      "Trained batch 610 batch loss 1.10216737 epoch total loss 1.13350666\n",
      "Trained batch 611 batch loss 1.13736272 epoch total loss 1.13351297\n",
      "Trained batch 612 batch loss 1.11094141 epoch total loss 1.13347614\n",
      "Trained batch 613 batch loss 1.22500324 epoch total loss 1.13362539\n",
      "Trained batch 614 batch loss 1.31031811 epoch total loss 1.13391316\n",
      "Trained batch 615 batch loss 1.32742953 epoch total loss 1.13422787\n",
      "Trained batch 616 batch loss 1.2938031 epoch total loss 1.13448691\n",
      "Trained batch 617 batch loss 1.29992235 epoch total loss 1.13475502\n",
      "Trained batch 618 batch loss 1.18932605 epoch total loss 1.13484335\n",
      "Trained batch 619 batch loss 1.06682801 epoch total loss 1.13473344\n",
      "Trained batch 620 batch loss 1.18268085 epoch total loss 1.13481081\n",
      "Trained batch 621 batch loss 1.25929809 epoch total loss 1.1350112\n",
      "Trained batch 622 batch loss 1.16505241 epoch total loss 1.13505948\n",
      "Trained batch 623 batch loss 1.12888503 epoch total loss 1.13504958\n",
      "Trained batch 624 batch loss 1.01073694 epoch total loss 1.13485038\n",
      "Trained batch 625 batch loss 1.01602864 epoch total loss 1.13466036\n",
      "Trained batch 626 batch loss 1.07374251 epoch total loss 1.13456297\n",
      "Trained batch 627 batch loss 1.10093915 epoch total loss 1.13450944\n",
      "Trained batch 628 batch loss 1.10433555 epoch total loss 1.13446128\n",
      "Trained batch 629 batch loss 1.17404604 epoch total loss 1.13452435\n",
      "Trained batch 630 batch loss 1.18253839 epoch total loss 1.13460052\n",
      "Trained batch 631 batch loss 1.11621761 epoch total loss 1.13457143\n",
      "Trained batch 632 batch loss 1.05385864 epoch total loss 1.13444364\n",
      "Trained batch 633 batch loss 0.907673836 epoch total loss 1.13408542\n",
      "Trained batch 634 batch loss 0.930186868 epoch total loss 1.13376379\n",
      "Trained batch 635 batch loss 1.383618 epoch total loss 1.13415718\n",
      "Trained batch 636 batch loss 1.23283863 epoch total loss 1.13431239\n",
      "Trained batch 637 batch loss 1.10894227 epoch total loss 1.13427258\n",
      "Trained batch 638 batch loss 1.12226748 epoch total loss 1.13425374\n",
      "Trained batch 639 batch loss 1.08539224 epoch total loss 1.13417721\n",
      "Trained batch 640 batch loss 1.08131182 epoch total loss 1.1340946\n",
      "Trained batch 641 batch loss 1.034024 epoch total loss 1.13393843\n",
      "Trained batch 642 batch loss 1.05600452 epoch total loss 1.13381708\n",
      "Trained batch 643 batch loss 1.13537884 epoch total loss 1.13381958\n",
      "Trained batch 644 batch loss 1.12239623 epoch total loss 1.13380182\n",
      "Trained batch 645 batch loss 1.14676476 epoch total loss 1.13382185\n",
      "Trained batch 646 batch loss 1.14172983 epoch total loss 1.13383412\n",
      "Trained batch 647 batch loss 1.15375376 epoch total loss 1.13386488\n",
      "Trained batch 648 batch loss 1.07317352 epoch total loss 1.1337713\n",
      "Trained batch 649 batch loss 1.00853455 epoch total loss 1.1335783\n",
      "Trained batch 650 batch loss 1.10983932 epoch total loss 1.13354182\n",
      "Trained batch 651 batch loss 1.0854001 epoch total loss 1.13346791\n",
      "Trained batch 652 batch loss 1.02180493 epoch total loss 1.13329661\n",
      "Trained batch 653 batch loss 1.07613087 epoch total loss 1.13320899\n",
      "Trained batch 654 batch loss 1.20838797 epoch total loss 1.13332391\n",
      "Trained batch 655 batch loss 1.11710179 epoch total loss 1.13329923\n",
      "Trained batch 656 batch loss 1.13940728 epoch total loss 1.13330853\n",
      "Trained batch 657 batch loss 1.16247833 epoch total loss 1.13335288\n",
      "Trained batch 658 batch loss 1.02275026 epoch total loss 1.13318479\n",
      "Trained batch 659 batch loss 1.18221223 epoch total loss 1.13325918\n",
      "Trained batch 660 batch loss 1.10477555 epoch total loss 1.13321602\n",
      "Trained batch 661 batch loss 1.11298895 epoch total loss 1.13318551\n",
      "Trained batch 662 batch loss 0.978773057 epoch total loss 1.13295221\n",
      "Trained batch 663 batch loss 1.03094459 epoch total loss 1.13279831\n",
      "Trained batch 664 batch loss 1.12017202 epoch total loss 1.13277936\n",
      "Trained batch 665 batch loss 1.12035024 epoch total loss 1.13276064\n",
      "Trained batch 666 batch loss 1.19355083 epoch total loss 1.13285196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 667 batch loss 1.12787211 epoch total loss 1.13284445\n",
      "Trained batch 668 batch loss 0.914995193 epoch total loss 1.13251829\n",
      "Trained batch 669 batch loss 0.886753082 epoch total loss 1.13215101\n",
      "Trained batch 670 batch loss 0.834731281 epoch total loss 1.13170707\n",
      "Trained batch 671 batch loss 0.970972538 epoch total loss 1.13146746\n",
      "Trained batch 672 batch loss 0.936236799 epoch total loss 1.13117695\n",
      "Trained batch 673 batch loss 1.09858525 epoch total loss 1.13112843\n",
      "Trained batch 674 batch loss 1.08033359 epoch total loss 1.13105309\n",
      "Trained batch 675 batch loss 0.98570168 epoch total loss 1.1308378\n",
      "Trained batch 676 batch loss 0.962083638 epoch total loss 1.13058817\n",
      "Trained batch 677 batch loss 0.945724845 epoch total loss 1.13031507\n",
      "Trained batch 678 batch loss 1.14144909 epoch total loss 1.13033152\n",
      "Trained batch 679 batch loss 1.08484292 epoch total loss 1.13026452\n",
      "Trained batch 680 batch loss 1.16586804 epoch total loss 1.13031697\n",
      "Trained batch 681 batch loss 1.28242981 epoch total loss 1.13054025\n",
      "Trained batch 682 batch loss 1.02069509 epoch total loss 1.1303792\n",
      "Trained batch 683 batch loss 0.998260736 epoch total loss 1.13018584\n",
      "Trained batch 684 batch loss 1.08794773 epoch total loss 1.13012409\n",
      "Trained batch 685 batch loss 1.16132677 epoch total loss 1.13016963\n",
      "Trained batch 686 batch loss 1.15103233 epoch total loss 1.13020015\n",
      "Trained batch 687 batch loss 1.13894844 epoch total loss 1.1302129\n",
      "Trained batch 688 batch loss 1.23208368 epoch total loss 1.13036084\n",
      "Trained batch 689 batch loss 1.3884654 epoch total loss 1.13073552\n",
      "Trained batch 690 batch loss 1.16863096 epoch total loss 1.13079047\n",
      "Trained batch 691 batch loss 1.13439608 epoch total loss 1.13079572\n",
      "Trained batch 692 batch loss 1.07297146 epoch total loss 1.13071215\n",
      "Trained batch 693 batch loss 1.07475829 epoch total loss 1.13063145\n",
      "Trained batch 694 batch loss 1.04874444 epoch total loss 1.13051343\n",
      "Trained batch 695 batch loss 1.04787457 epoch total loss 1.13039458\n",
      "Trained batch 696 batch loss 1.04104495 epoch total loss 1.13026607\n",
      "Trained batch 697 batch loss 1.18981695 epoch total loss 1.13035154\n",
      "Trained batch 698 batch loss 1.19530928 epoch total loss 1.13044465\n",
      "Trained batch 699 batch loss 1.21065831 epoch total loss 1.13055933\n",
      "Trained batch 700 batch loss 1.13300538 epoch total loss 1.13056278\n",
      "Trained batch 701 batch loss 1.18568695 epoch total loss 1.13064146\n",
      "Trained batch 702 batch loss 1.2341578 epoch total loss 1.13078892\n",
      "Trained batch 703 batch loss 1.11337268 epoch total loss 1.13076401\n",
      "Trained batch 704 batch loss 1.29377258 epoch total loss 1.13099563\n",
      "Trained batch 705 batch loss 1.19894958 epoch total loss 1.13109195\n",
      "Trained batch 706 batch loss 1.31858766 epoch total loss 1.13135755\n",
      "Trained batch 707 batch loss 1.16580546 epoch total loss 1.13140631\n",
      "Trained batch 708 batch loss 1.13343811 epoch total loss 1.13140917\n",
      "Trained batch 709 batch loss 1.16565144 epoch total loss 1.13145745\n",
      "Trained batch 710 batch loss 1.16092777 epoch total loss 1.13149905\n",
      "Trained batch 711 batch loss 1.11374068 epoch total loss 1.13147414\n",
      "Trained batch 712 batch loss 1.20081604 epoch total loss 1.13157153\n",
      "Trained batch 713 batch loss 1.07539141 epoch total loss 1.13149261\n",
      "Trained batch 714 batch loss 1.08441162 epoch total loss 1.13142669\n",
      "Trained batch 715 batch loss 1.08064961 epoch total loss 1.13135564\n",
      "Trained batch 716 batch loss 1.0999999 epoch total loss 1.13131189\n",
      "Trained batch 717 batch loss 1.20887733 epoch total loss 1.13142\n",
      "Trained batch 718 batch loss 1.08463 epoch total loss 1.13135493\n",
      "Trained batch 719 batch loss 1.12144709 epoch total loss 1.1313411\n",
      "Trained batch 720 batch loss 1.14760339 epoch total loss 1.13136363\n",
      "Trained batch 721 batch loss 1.16172886 epoch total loss 1.13140583\n",
      "Trained batch 722 batch loss 1.16760361 epoch total loss 1.1314559\n",
      "Trained batch 723 batch loss 1.11175787 epoch total loss 1.13142872\n",
      "Trained batch 724 batch loss 1.04360414 epoch total loss 1.13130736\n",
      "Trained batch 725 batch loss 1.04986596 epoch total loss 1.13119507\n",
      "Trained batch 726 batch loss 1.09803355 epoch total loss 1.13114929\n",
      "Trained batch 727 batch loss 1.24944758 epoch total loss 1.13131201\n",
      "Trained batch 728 batch loss 1.17915332 epoch total loss 1.1313777\n",
      "Trained batch 729 batch loss 1.14945912 epoch total loss 1.13140261\n",
      "Trained batch 730 batch loss 1.15355444 epoch total loss 1.13143289\n",
      "Trained batch 731 batch loss 1.09957922 epoch total loss 1.13138938\n",
      "Trained batch 732 batch loss 1.10027778 epoch total loss 1.13134694\n",
      "Trained batch 733 batch loss 0.99542141 epoch total loss 1.13116145\n",
      "Trained batch 734 batch loss 1.16453791 epoch total loss 1.13120699\n",
      "Trained batch 735 batch loss 1.19944859 epoch total loss 1.13129985\n",
      "Trained batch 736 batch loss 1.1524502 epoch total loss 1.13132858\n",
      "Trained batch 737 batch loss 1.2568891 epoch total loss 1.13149893\n",
      "Trained batch 738 batch loss 1.25246537 epoch total loss 1.13166285\n",
      "Trained batch 739 batch loss 1.09253848 epoch total loss 1.13160992\n",
      "Trained batch 740 batch loss 1.24148822 epoch total loss 1.13175845\n",
      "Trained batch 741 batch loss 1.22646093 epoch total loss 1.13188612\n",
      "Trained batch 742 batch loss 1.10996771 epoch total loss 1.13185668\n",
      "Trained batch 743 batch loss 1.16723847 epoch total loss 1.13190424\n",
      "Trained batch 744 batch loss 1.12817204 epoch total loss 1.13189924\n",
      "Trained batch 745 batch loss 1.24726677 epoch total loss 1.13205409\n",
      "Trained batch 746 batch loss 1.2486757 epoch total loss 1.13221037\n",
      "Trained batch 747 batch loss 0.998368502 epoch total loss 1.1320312\n",
      "Trained batch 748 batch loss 0.912014127 epoch total loss 1.13173699\n",
      "Trained batch 749 batch loss 1.07263684 epoch total loss 1.13165808\n",
      "Trained batch 750 batch loss 1.08752477 epoch total loss 1.13159931\n",
      "Trained batch 751 batch loss 1.05144536 epoch total loss 1.13149261\n",
      "Trained batch 752 batch loss 1.11662006 epoch total loss 1.13147283\n",
      "Trained batch 753 batch loss 1.10783148 epoch total loss 1.13144147\n",
      "Trained batch 754 batch loss 1.22999895 epoch total loss 1.13157213\n",
      "Trained batch 755 batch loss 1.22919726 epoch total loss 1.13170147\n",
      "Trained batch 756 batch loss 1.11874866 epoch total loss 1.1316843\n",
      "Trained batch 757 batch loss 1.36412156 epoch total loss 1.13199139\n",
      "Trained batch 758 batch loss 1.28877354 epoch total loss 1.13219821\n",
      "Trained batch 759 batch loss 1.24009681 epoch total loss 1.13234043\n",
      "Trained batch 760 batch loss 1.28244841 epoch total loss 1.13253796\n",
      "Trained batch 761 batch loss 1.07216918 epoch total loss 1.13245857\n",
      "Trained batch 762 batch loss 1.14287376 epoch total loss 1.13247228\n",
      "Trained batch 763 batch loss 1.18976235 epoch total loss 1.13254738\n",
      "Trained batch 764 batch loss 1.00226045 epoch total loss 1.13237679\n",
      "Trained batch 765 batch loss 1.10055089 epoch total loss 1.13233519\n",
      "Trained batch 766 batch loss 1.29629326 epoch total loss 1.13254917\n",
      "Trained batch 767 batch loss 1.30173802 epoch total loss 1.13276982\n",
      "Trained batch 768 batch loss 1.19392991 epoch total loss 1.13284934\n",
      "Trained batch 769 batch loss 0.942672908 epoch total loss 1.1326021\n",
      "Trained batch 770 batch loss 0.957622349 epoch total loss 1.13237488\n",
      "Trained batch 771 batch loss 0.978017092 epoch total loss 1.13217473\n",
      "Trained batch 772 batch loss 0.920736611 epoch total loss 1.13190079\n",
      "Trained batch 773 batch loss 0.981096745 epoch total loss 1.13170564\n",
      "Trained batch 774 batch loss 0.892017066 epoch total loss 1.13139594\n",
      "Trained batch 775 batch loss 1.00930214 epoch total loss 1.13123846\n",
      "Trained batch 776 batch loss 1.06456232 epoch total loss 1.13115251\n",
      "Trained batch 777 batch loss 1.04462767 epoch total loss 1.13104117\n",
      "Trained batch 778 batch loss 1.13688862 epoch total loss 1.13104868\n",
      "Trained batch 779 batch loss 1.18032086 epoch total loss 1.13111186\n",
      "Trained batch 780 batch loss 1.14530766 epoch total loss 1.1311301\n",
      "Trained batch 781 batch loss 1.16923094 epoch total loss 1.13117898\n",
      "Trained batch 782 batch loss 1.23684573 epoch total loss 1.13131404\n",
      "Trained batch 783 batch loss 1.26639187 epoch total loss 1.13148654\n",
      "Trained batch 784 batch loss 1.13214827 epoch total loss 1.13148737\n",
      "Trained batch 785 batch loss 1.11623549 epoch total loss 1.13146794\n",
      "Trained batch 786 batch loss 1.02359354 epoch total loss 1.13133073\n",
      "Trained batch 787 batch loss 1.10066438 epoch total loss 1.13129175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 788 batch loss 1.13525736 epoch total loss 1.13129675\n",
      "Trained batch 789 batch loss 1.19870472 epoch total loss 1.13138223\n",
      "Trained batch 790 batch loss 1.14038968 epoch total loss 1.13139367\n",
      "Trained batch 791 batch loss 1.07058597 epoch total loss 1.13131666\n",
      "Trained batch 792 batch loss 1.18124509 epoch total loss 1.13137984\n",
      "Trained batch 793 batch loss 1.22378039 epoch total loss 1.13149631\n",
      "Trained batch 794 batch loss 1.07761812 epoch total loss 1.13142848\n",
      "Trained batch 795 batch loss 1.05919468 epoch total loss 1.13133764\n",
      "Trained batch 796 batch loss 1.11170077 epoch total loss 1.13131297\n",
      "Trained batch 797 batch loss 1.17224371 epoch total loss 1.13136423\n",
      "Trained batch 798 batch loss 1.13712955 epoch total loss 1.1313715\n",
      "Trained batch 799 batch loss 1.1522752 epoch total loss 1.13139772\n",
      "Trained batch 800 batch loss 1.01437068 epoch total loss 1.13125134\n",
      "Trained batch 801 batch loss 1.20476079 epoch total loss 1.13134313\n",
      "Trained batch 802 batch loss 1.07728815 epoch total loss 1.13127577\n",
      "Trained batch 803 batch loss 1.12292337 epoch total loss 1.13126528\n",
      "Trained batch 804 batch loss 1.15073109 epoch total loss 1.1312896\n",
      "Trained batch 805 batch loss 1.2149291 epoch total loss 1.13139343\n",
      "Trained batch 806 batch loss 1.12983108 epoch total loss 1.13139153\n",
      "Trained batch 807 batch loss 1.27953959 epoch total loss 1.13157511\n",
      "Trained batch 808 batch loss 1.24010324 epoch total loss 1.13170946\n",
      "Trained batch 809 batch loss 1.06508398 epoch total loss 1.13162708\n",
      "Trained batch 810 batch loss 1.16128719 epoch total loss 1.13166368\n",
      "Trained batch 811 batch loss 1.27645254 epoch total loss 1.13184214\n",
      "Trained batch 812 batch loss 1.27370501 epoch total loss 1.1320169\n",
      "Trained batch 813 batch loss 1.00819755 epoch total loss 1.13186455\n",
      "Trained batch 814 batch loss 1.11796987 epoch total loss 1.1318475\n",
      "Trained batch 815 batch loss 0.99640125 epoch total loss 1.13168132\n",
      "Trained batch 816 batch loss 1.14201546 epoch total loss 1.13169396\n",
      "Trained batch 817 batch loss 1.17974675 epoch total loss 1.13175285\n",
      "Trained batch 818 batch loss 1.17823112 epoch total loss 1.13180959\n",
      "Trained batch 819 batch loss 1.22545743 epoch total loss 1.13192391\n",
      "Trained batch 820 batch loss 1.14587617 epoch total loss 1.13194096\n",
      "Trained batch 821 batch loss 1.08152485 epoch total loss 1.13187957\n",
      "Trained batch 822 batch loss 1.09911454 epoch total loss 1.13183975\n",
      "Trained batch 823 batch loss 1.05699706 epoch total loss 1.1317488\n",
      "Trained batch 824 batch loss 1.13520539 epoch total loss 1.13175297\n",
      "Trained batch 825 batch loss 1.14840007 epoch total loss 1.13177311\n",
      "Trained batch 826 batch loss 1.1563282 epoch total loss 1.1318028\n",
      "Trained batch 827 batch loss 1.14709699 epoch total loss 1.13182127\n",
      "Trained batch 828 batch loss 1.09850848 epoch total loss 1.1317811\n",
      "Trained batch 829 batch loss 1.16871262 epoch total loss 1.13182569\n",
      "Trained batch 830 batch loss 1.15302587 epoch total loss 1.1318512\n",
      "Trained batch 831 batch loss 0.980879784 epoch total loss 1.13166952\n",
      "Trained batch 832 batch loss 1.106529 epoch total loss 1.13163924\n",
      "Trained batch 833 batch loss 1.13602519 epoch total loss 1.13164461\n",
      "Trained batch 834 batch loss 1.08325529 epoch total loss 1.13158655\n",
      "Trained batch 835 batch loss 1.01261806 epoch total loss 1.1314441\n",
      "Trained batch 836 batch loss 1.15950477 epoch total loss 1.13147759\n",
      "Trained batch 837 batch loss 1.02458501 epoch total loss 1.13134992\n",
      "Trained batch 838 batch loss 1.04879212 epoch total loss 1.13125134\n",
      "Trained batch 839 batch loss 1.17131937 epoch total loss 1.13129914\n",
      "Trained batch 840 batch loss 1.0478251 epoch total loss 1.13119984\n",
      "Trained batch 841 batch loss 1.08844686 epoch total loss 1.13114893\n",
      "Trained batch 842 batch loss 1.06514835 epoch total loss 1.13107049\n",
      "Trained batch 843 batch loss 1.01409078 epoch total loss 1.13093174\n",
      "Trained batch 844 batch loss 1.11664474 epoch total loss 1.13091481\n",
      "Trained batch 845 batch loss 1.2712636 epoch total loss 1.13108087\n",
      "Trained batch 846 batch loss 1.17419279 epoch total loss 1.13113189\n",
      "Trained batch 847 batch loss 1.09398925 epoch total loss 1.13108802\n",
      "Trained batch 848 batch loss 1.09307277 epoch total loss 1.1310432\n",
      "Trained batch 849 batch loss 0.988804758 epoch total loss 1.13087571\n",
      "Trained batch 850 batch loss 1.15261817 epoch total loss 1.13090122\n",
      "Trained batch 851 batch loss 1.06349421 epoch total loss 1.13082206\n",
      "Trained batch 852 batch loss 1.10213709 epoch total loss 1.13078833\n",
      "Trained batch 853 batch loss 1.02376807 epoch total loss 1.1306628\n",
      "Trained batch 854 batch loss 1.0575217 epoch total loss 1.13057709\n",
      "Trained batch 855 batch loss 0.931675315 epoch total loss 1.13034451\n",
      "Trained batch 856 batch loss 0.915460706 epoch total loss 1.13009346\n",
      "Trained batch 857 batch loss 1.05231214 epoch total loss 1.13000274\n",
      "Trained batch 858 batch loss 1.22653568 epoch total loss 1.13011527\n",
      "Trained batch 859 batch loss 1.09820247 epoch total loss 1.1300782\n",
      "Trained batch 860 batch loss 1.22509253 epoch total loss 1.13018858\n",
      "Trained batch 861 batch loss 1.12584162 epoch total loss 1.13018358\n",
      "Trained batch 862 batch loss 0.997348309 epoch total loss 1.13002956\n",
      "Trained batch 863 batch loss 1.01327348 epoch total loss 1.12989426\n",
      "Trained batch 864 batch loss 0.994429111 epoch total loss 1.12973738\n",
      "Trained batch 865 batch loss 1.0778141 epoch total loss 1.12967741\n",
      "Trained batch 866 batch loss 1.16292822 epoch total loss 1.1297158\n",
      "Trained batch 867 batch loss 1.13094747 epoch total loss 1.12971711\n",
      "Trained batch 868 batch loss 1.05785143 epoch total loss 1.12963438\n",
      "Trained batch 869 batch loss 1.12876606 epoch total loss 1.12963343\n",
      "Trained batch 870 batch loss 0.94098109 epoch total loss 1.12941658\n",
      "Trained batch 871 batch loss 0.99907881 epoch total loss 1.12926698\n",
      "Trained batch 872 batch loss 1.05583847 epoch total loss 1.1291827\n",
      "Trained batch 873 batch loss 0.951059401 epoch total loss 1.12897873\n",
      "Trained batch 874 batch loss 0.938865364 epoch total loss 1.12876117\n",
      "Trained batch 875 batch loss 1.07739687 epoch total loss 1.1287024\n",
      "Trained batch 876 batch loss 1.03417301 epoch total loss 1.12859452\n",
      "Trained batch 877 batch loss 0.910838 epoch total loss 1.1283462\n",
      "Trained batch 878 batch loss 0.901545286 epoch total loss 1.12808788\n",
      "Trained batch 879 batch loss 0.983490825 epoch total loss 1.12792349\n",
      "Trained batch 880 batch loss 1.1036489 epoch total loss 1.12789583\n",
      "Trained batch 881 batch loss 1.22764325 epoch total loss 1.12800908\n",
      "Trained batch 882 batch loss 1.25167286 epoch total loss 1.12814927\n",
      "Trained batch 883 batch loss 1.1932137 epoch total loss 1.12822294\n",
      "Trained batch 884 batch loss 1.2141161 epoch total loss 1.1283201\n",
      "Trained batch 885 batch loss 1.26218104 epoch total loss 1.12847137\n",
      "Trained batch 886 batch loss 1.14499 epoch total loss 1.12849009\n",
      "Trained batch 887 batch loss 1.09328747 epoch total loss 1.12845039\n",
      "Trained batch 888 batch loss 1.05497742 epoch total loss 1.12836766\n",
      "Trained batch 889 batch loss 1.0320375 epoch total loss 1.1282593\n",
      "Trained batch 890 batch loss 0.990506887 epoch total loss 1.12810445\n",
      "Trained batch 891 batch loss 1.06734228 epoch total loss 1.12803626\n",
      "Trained batch 892 batch loss 1.02981603 epoch total loss 1.12792623\n",
      "Trained batch 893 batch loss 1.05302167 epoch total loss 1.12784231\n",
      "Trained batch 894 batch loss 1.12251949 epoch total loss 1.12783635\n",
      "Trained batch 895 batch loss 1.13392651 epoch total loss 1.12784314\n",
      "Trained batch 896 batch loss 1.16624415 epoch total loss 1.12788606\n",
      "Trained batch 897 batch loss 1.35086012 epoch total loss 1.12813461\n",
      "Trained batch 898 batch loss 1.10249925 epoch total loss 1.128106\n",
      "Trained batch 899 batch loss 1.16763365 epoch total loss 1.12815\n",
      "Trained batch 900 batch loss 1.12730658 epoch total loss 1.12814903\n",
      "Trained batch 901 batch loss 1.05940485 epoch total loss 1.12807274\n",
      "Trained batch 902 batch loss 1.18069267 epoch total loss 1.12813103\n",
      "Trained batch 903 batch loss 1.19209754 epoch total loss 1.12820184\n",
      "Trained batch 904 batch loss 1.28480268 epoch total loss 1.12837505\n",
      "Trained batch 905 batch loss 1.03923798 epoch total loss 1.12827659\n",
      "Trained batch 906 batch loss 1.15948713 epoch total loss 1.12831104\n",
      "Trained batch 907 batch loss 1.33652425 epoch total loss 1.12854064\n",
      "Trained batch 908 batch loss 1.2674793 epoch total loss 1.1286937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 909 batch loss 1.17881835 epoch total loss 1.12874889\n",
      "Trained batch 910 batch loss 1.06569982 epoch total loss 1.12867951\n",
      "Trained batch 911 batch loss 1.13626063 epoch total loss 1.12868786\n",
      "Trained batch 912 batch loss 1.29219401 epoch total loss 1.12886715\n",
      "Trained batch 913 batch loss 1.17403591 epoch total loss 1.12891662\n",
      "Trained batch 914 batch loss 0.907599568 epoch total loss 1.12867451\n",
      "Trained batch 915 batch loss 0.946521759 epoch total loss 1.12847543\n",
      "Trained batch 916 batch loss 0.994849205 epoch total loss 1.12832963\n",
      "Trained batch 917 batch loss 1.01405692 epoch total loss 1.12820494\n",
      "Trained batch 918 batch loss 1.14485478 epoch total loss 1.12822318\n",
      "Trained batch 919 batch loss 1.11542702 epoch total loss 1.12820935\n",
      "Trained batch 920 batch loss 1.16317964 epoch total loss 1.12824738\n",
      "Trained batch 921 batch loss 1.03815842 epoch total loss 1.12814963\n",
      "Trained batch 922 batch loss 1.13902 epoch total loss 1.12816143\n",
      "Trained batch 923 batch loss 1.21640086 epoch total loss 1.12825704\n",
      "Trained batch 924 batch loss 1.21264374 epoch total loss 1.12834835\n",
      "Trained batch 925 batch loss 1.38760567 epoch total loss 1.12862861\n",
      "Trained batch 926 batch loss 1.40380907 epoch total loss 1.1289258\n",
      "Trained batch 927 batch loss 1.20842052 epoch total loss 1.12901151\n",
      "Trained batch 928 batch loss 1.03310871 epoch total loss 1.12890804\n",
      "Trained batch 929 batch loss 1.17449284 epoch total loss 1.12895715\n",
      "Trained batch 930 batch loss 1.30909872 epoch total loss 1.12915075\n",
      "Trained batch 931 batch loss 1.19601679 epoch total loss 1.12922263\n",
      "Trained batch 932 batch loss 1.30079114 epoch total loss 1.12940669\n",
      "Trained batch 933 batch loss 1.1466105 epoch total loss 1.12942517\n",
      "Trained batch 934 batch loss 1.1400212 epoch total loss 1.12943649\n",
      "Trained batch 935 batch loss 1.1510092 epoch total loss 1.1294595\n",
      "Trained batch 936 batch loss 1.21110463 epoch total loss 1.12954676\n",
      "Trained batch 937 batch loss 1.22538483 epoch total loss 1.12964892\n",
      "Trained batch 938 batch loss 1.28500235 epoch total loss 1.12981462\n",
      "Trained batch 939 batch loss 1.3720541 epoch total loss 1.13007259\n",
      "Trained batch 940 batch loss 1.19636321 epoch total loss 1.13014317\n",
      "Trained batch 941 batch loss 1.01220345 epoch total loss 1.13001788\n",
      "Trained batch 942 batch loss 0.969204187 epoch total loss 1.12984717\n",
      "Trained batch 943 batch loss 0.864091873 epoch total loss 1.12956536\n",
      "Trained batch 944 batch loss 1.13860857 epoch total loss 1.12957489\n",
      "Trained batch 945 batch loss 0.988962293 epoch total loss 1.12942612\n",
      "Trained batch 946 batch loss 0.887051761 epoch total loss 1.12917006\n",
      "Trained batch 947 batch loss 0.796172321 epoch total loss 1.12881839\n",
      "Trained batch 948 batch loss 0.948581636 epoch total loss 1.12862825\n",
      "Trained batch 949 batch loss 1.06085706 epoch total loss 1.12855685\n",
      "Trained batch 950 batch loss 1.02883923 epoch total loss 1.12845194\n",
      "Trained batch 951 batch loss 1.00773823 epoch total loss 1.12832487\n",
      "Trained batch 952 batch loss 1.0661633 epoch total loss 1.12825966\n",
      "Trained batch 953 batch loss 1.14062285 epoch total loss 1.12827253\n",
      "Trained batch 954 batch loss 1.12254775 epoch total loss 1.12826657\n",
      "Trained batch 955 batch loss 1.12132955 epoch total loss 1.1282593\n",
      "Trained batch 956 batch loss 1.12315392 epoch total loss 1.12825406\n",
      "Trained batch 957 batch loss 1.00188971 epoch total loss 1.12812197\n",
      "Trained batch 958 batch loss 1.2134378 epoch total loss 1.1282109\n",
      "Trained batch 959 batch loss 1.17897093 epoch total loss 1.12826383\n",
      "Trained batch 960 batch loss 1.08657312 epoch total loss 1.12822032\n",
      "Trained batch 961 batch loss 1.00921524 epoch total loss 1.12809646\n",
      "Trained batch 962 batch loss 1.17320895 epoch total loss 1.12814343\n",
      "Trained batch 963 batch loss 1.08520353 epoch total loss 1.12809885\n",
      "Trained batch 964 batch loss 1.25313115 epoch total loss 1.12822855\n",
      "Trained batch 965 batch loss 1.16062856 epoch total loss 1.12826216\n",
      "Trained batch 966 batch loss 0.951648295 epoch total loss 1.1280793\n",
      "Trained batch 967 batch loss 1.08497441 epoch total loss 1.12803471\n",
      "Trained batch 968 batch loss 1.07391405 epoch total loss 1.12797892\n",
      "Trained batch 969 batch loss 1.14171505 epoch total loss 1.12799311\n",
      "Trained batch 970 batch loss 1.10504103 epoch total loss 1.12796938\n",
      "Trained batch 971 batch loss 1.0880146 epoch total loss 1.12792814\n",
      "Trained batch 972 batch loss 1.10675895 epoch total loss 1.12790644\n",
      "Trained batch 973 batch loss 0.981743 epoch total loss 1.12775624\n",
      "Trained batch 974 batch loss 1.04869843 epoch total loss 1.12767506\n",
      "Trained batch 975 batch loss 1.04878819 epoch total loss 1.12759411\n",
      "Trained batch 976 batch loss 1.04543912 epoch total loss 1.12751\n",
      "Trained batch 977 batch loss 1.20524025 epoch total loss 1.12758946\n",
      "Trained batch 978 batch loss 1.12911916 epoch total loss 1.12759101\n",
      "Trained batch 979 batch loss 1.16768467 epoch total loss 1.12763202\n",
      "Trained batch 980 batch loss 1.1727016 epoch total loss 1.12767804\n",
      "Trained batch 981 batch loss 1.15740013 epoch total loss 1.12770832\n",
      "Trained batch 982 batch loss 1.09285343 epoch total loss 1.12767291\n",
      "Trained batch 983 batch loss 1.05411196 epoch total loss 1.12759805\n",
      "Trained batch 984 batch loss 1.16058159 epoch total loss 1.12763143\n",
      "Trained batch 985 batch loss 0.926697373 epoch total loss 1.12742758\n",
      "Trained batch 986 batch loss 0.917118 epoch total loss 1.12721419\n",
      "Trained batch 987 batch loss 1.0473932 epoch total loss 1.12713337\n",
      "Trained batch 988 batch loss 1.04670382 epoch total loss 1.12705195\n",
      "Trained batch 989 batch loss 1.14002097 epoch total loss 1.12706506\n",
      "Trained batch 990 batch loss 0.977586865 epoch total loss 1.12691402\n",
      "Trained batch 991 batch loss 1.02975225 epoch total loss 1.12681603\n",
      "Trained batch 992 batch loss 1.06945801 epoch total loss 1.12675822\n",
      "Trained batch 993 batch loss 1.05067527 epoch total loss 1.12668157\n",
      "Trained batch 994 batch loss 1.14027929 epoch total loss 1.12669528\n",
      "Trained batch 995 batch loss 1.10750747 epoch total loss 1.12667596\n",
      "Trained batch 996 batch loss 1.22058439 epoch total loss 1.12677026\n",
      "Trained batch 997 batch loss 1.17128551 epoch total loss 1.12681484\n",
      "Trained batch 998 batch loss 1.16958857 epoch total loss 1.12685776\n",
      "Trained batch 999 batch loss 1.18628597 epoch total loss 1.12691724\n",
      "Trained batch 1000 batch loss 1.15822756 epoch total loss 1.12694848\n",
      "Trained batch 1001 batch loss 1.06033027 epoch total loss 1.12688196\n",
      "Trained batch 1002 batch loss 1.06890631 epoch total loss 1.12682402\n",
      "Trained batch 1003 batch loss 1.1681869 epoch total loss 1.12686527\n",
      "Trained batch 1004 batch loss 1.32134044 epoch total loss 1.12705886\n",
      "Trained batch 1005 batch loss 1.25812316 epoch total loss 1.1271894\n",
      "Trained batch 1006 batch loss 1.095487 epoch total loss 1.12715781\n",
      "Trained batch 1007 batch loss 1.27977788 epoch total loss 1.12730944\n",
      "Trained batch 1008 batch loss 1.13012123 epoch total loss 1.12731218\n",
      "Trained batch 1009 batch loss 1.10355902 epoch total loss 1.12728858\n",
      "Trained batch 1010 batch loss 1.14387786 epoch total loss 1.12730503\n",
      "Trained batch 1011 batch loss 1.13954151 epoch total loss 1.12731719\n",
      "Trained batch 1012 batch loss 1.13280082 epoch total loss 1.12732255\n",
      "Trained batch 1013 batch loss 1.23426414 epoch total loss 1.12742817\n",
      "Trained batch 1014 batch loss 1.32586169 epoch total loss 1.1276238\n",
      "Trained batch 1015 batch loss 1.12282836 epoch total loss 1.12761903\n",
      "Trained batch 1016 batch loss 1.25449693 epoch total loss 1.12774396\n",
      "Trained batch 1017 batch loss 1.23633277 epoch total loss 1.12785065\n",
      "Trained batch 1018 batch loss 1.20583129 epoch total loss 1.1279273\n",
      "Trained batch 1019 batch loss 1.17979026 epoch total loss 1.12797821\n",
      "Trained batch 1020 batch loss 1.03243327 epoch total loss 1.12788463\n",
      "Trained batch 1021 batch loss 1.17196178 epoch total loss 1.12792778\n",
      "Trained batch 1022 batch loss 1.28770828 epoch total loss 1.12808418\n",
      "Trained batch 1023 batch loss 1.25307512 epoch total loss 1.12820625\n",
      "Trained batch 1024 batch loss 1.33607161 epoch total loss 1.12840927\n",
      "Trained batch 1025 batch loss 1.25326037 epoch total loss 1.1285311\n",
      "Trained batch 1026 batch loss 1.2463336 epoch total loss 1.1286459\n",
      "Trained batch 1027 batch loss 1.07413101 epoch total loss 1.12859285\n",
      "Trained batch 1028 batch loss 1.12094831 epoch total loss 1.12858546\n",
      "Trained batch 1029 batch loss 1.06765068 epoch total loss 1.12852621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1030 batch loss 1.06225967 epoch total loss 1.12846184\n",
      "Trained batch 1031 batch loss 1.26572049 epoch total loss 1.12859499\n",
      "Trained batch 1032 batch loss 1.21213043 epoch total loss 1.12867594\n",
      "Trained batch 1033 batch loss 1.13451672 epoch total loss 1.12868166\n",
      "Trained batch 1034 batch loss 1.05380177 epoch total loss 1.12860918\n",
      "Trained batch 1035 batch loss 1.00023866 epoch total loss 1.1284852\n",
      "Trained batch 1036 batch loss 0.982067227 epoch total loss 1.12834382\n",
      "Trained batch 1037 batch loss 1.06651676 epoch total loss 1.12828422\n",
      "Trained batch 1038 batch loss 1.04984808 epoch total loss 1.12820864\n",
      "Trained batch 1039 batch loss 1.00893 epoch total loss 1.12809384\n",
      "Trained batch 1040 batch loss 1.10317171 epoch total loss 1.12806988\n",
      "Trained batch 1041 batch loss 1.20031571 epoch total loss 1.12813926\n",
      "Trained batch 1042 batch loss 1.28007603 epoch total loss 1.12828505\n",
      "Trained batch 1043 batch loss 1.26914883 epoch total loss 1.12842011\n",
      "Trained batch 1044 batch loss 1.16665721 epoch total loss 1.12845671\n",
      "Trained batch 1045 batch loss 1.24644899 epoch total loss 1.1285696\n",
      "Trained batch 1046 batch loss 1.28573191 epoch total loss 1.12871993\n",
      "Trained batch 1047 batch loss 1.20770371 epoch total loss 1.12879539\n",
      "Trained batch 1048 batch loss 1.04897439 epoch total loss 1.12871921\n",
      "Trained batch 1049 batch loss 1.14138639 epoch total loss 1.12873125\n",
      "Trained batch 1050 batch loss 1.15465307 epoch total loss 1.12875593\n",
      "Trained batch 1051 batch loss 1.16364992 epoch total loss 1.12878919\n",
      "Trained batch 1052 batch loss 1.14817905 epoch total loss 1.12880766\n",
      "Trained batch 1053 batch loss 1.20317316 epoch total loss 1.12887824\n",
      "Trained batch 1054 batch loss 1.22043848 epoch total loss 1.12896514\n",
      "Trained batch 1055 batch loss 1.17617869 epoch total loss 1.12900984\n",
      "Trained batch 1056 batch loss 1.23279595 epoch total loss 1.12910807\n",
      "Trained batch 1057 batch loss 1.17604709 epoch total loss 1.12915242\n",
      "Trained batch 1058 batch loss 0.953575253 epoch total loss 1.1289866\n",
      "Trained batch 1059 batch loss 1.05187118 epoch total loss 1.12891376\n",
      "Trained batch 1060 batch loss 1.05779517 epoch total loss 1.12884665\n",
      "Trained batch 1061 batch loss 1.19693053 epoch total loss 1.12891078\n",
      "Trained batch 1062 batch loss 1.12290955 epoch total loss 1.12890506\n",
      "Trained batch 1063 batch loss 1.02899909 epoch total loss 1.12881112\n",
      "Trained batch 1064 batch loss 0.881862581 epoch total loss 1.12857902\n",
      "Trained batch 1065 batch loss 0.987269878 epoch total loss 1.12844634\n",
      "Trained batch 1066 batch loss 1.11286139 epoch total loss 1.1284318\n",
      "Trained batch 1067 batch loss 1.27110815 epoch total loss 1.12856555\n",
      "Trained batch 1068 batch loss 1.42381024 epoch total loss 1.128842\n",
      "Trained batch 1069 batch loss 1.29656887 epoch total loss 1.12899888\n",
      "Trained batch 1070 batch loss 1.0053755 epoch total loss 1.12888336\n",
      "Trained batch 1071 batch loss 1.06871009 epoch total loss 1.1288271\n",
      "Trained batch 1072 batch loss 1.18777454 epoch total loss 1.12888205\n",
      "Trained batch 1073 batch loss 1.19665551 epoch total loss 1.12894523\n",
      "Trained batch 1074 batch loss 1.1532259 epoch total loss 1.12896788\n",
      "Trained batch 1075 batch loss 1.15179014 epoch total loss 1.12898898\n",
      "Trained batch 1076 batch loss 1.2342937 epoch total loss 1.12908685\n",
      "Trained batch 1077 batch loss 1.11003792 epoch total loss 1.12906909\n",
      "Trained batch 1078 batch loss 1.1273253 epoch total loss 1.12906754\n",
      "Trained batch 1079 batch loss 1.10468197 epoch total loss 1.12904501\n",
      "Trained batch 1080 batch loss 1.01255393 epoch total loss 1.12893713\n",
      "Trained batch 1081 batch loss 1.0869621 epoch total loss 1.12889826\n",
      "Trained batch 1082 batch loss 1.0883286 epoch total loss 1.12886083\n",
      "Trained batch 1083 batch loss 1.16917443 epoch total loss 1.12889802\n",
      "Trained batch 1084 batch loss 0.947599649 epoch total loss 1.12873077\n",
      "Trained batch 1085 batch loss 1.14889956 epoch total loss 1.12874937\n",
      "Trained batch 1086 batch loss 1.11968732 epoch total loss 1.12874103\n",
      "Trained batch 1087 batch loss 1.07331097 epoch total loss 1.12869012\n",
      "Trained batch 1088 batch loss 1.13944578 epoch total loss 1.1286999\n",
      "Trained batch 1089 batch loss 1.23872459 epoch total loss 1.12880099\n",
      "Trained batch 1090 batch loss 1.09909356 epoch total loss 1.12877381\n",
      "Trained batch 1091 batch loss 1.18364775 epoch total loss 1.128824\n",
      "Trained batch 1092 batch loss 1.07555449 epoch total loss 1.12877524\n",
      "Trained batch 1093 batch loss 1.17561638 epoch total loss 1.12881815\n",
      "Trained batch 1094 batch loss 1.08933377 epoch total loss 1.12878203\n",
      "Trained batch 1095 batch loss 0.990910113 epoch total loss 1.12865615\n",
      "Trained batch 1096 batch loss 1.05240536 epoch total loss 1.12858665\n",
      "Trained batch 1097 batch loss 1.18241501 epoch total loss 1.12863564\n",
      "Trained batch 1098 batch loss 1.26845407 epoch total loss 1.12876296\n",
      "Trained batch 1099 batch loss 1.37316144 epoch total loss 1.12898529\n",
      "Trained batch 1100 batch loss 1.28230715 epoch total loss 1.12912476\n",
      "Trained batch 1101 batch loss 1.28858304 epoch total loss 1.1292696\n",
      "Trained batch 1102 batch loss 1.06307435 epoch total loss 1.12920952\n",
      "Trained batch 1103 batch loss 1.00152278 epoch total loss 1.12909377\n",
      "Trained batch 1104 batch loss 0.931085765 epoch total loss 1.12891436\n",
      "Trained batch 1105 batch loss 0.843902 epoch total loss 1.12865639\n",
      "Trained batch 1106 batch loss 0.998016 epoch total loss 1.12853825\n",
      "Trained batch 1107 batch loss 1.07540798 epoch total loss 1.12849033\n",
      "Trained batch 1108 batch loss 1.15406883 epoch total loss 1.12851334\n",
      "Trained batch 1109 batch loss 1.13397908 epoch total loss 1.12851834\n",
      "Trained batch 1110 batch loss 1.20416391 epoch total loss 1.12858653\n",
      "Trained batch 1111 batch loss 1.08352399 epoch total loss 1.128546\n",
      "Trained batch 1112 batch loss 0.986003757 epoch total loss 1.12841773\n",
      "Trained batch 1113 batch loss 0.89990133 epoch total loss 1.12821245\n",
      "Trained batch 1114 batch loss 1.19789672 epoch total loss 1.12827492\n",
      "Trained batch 1115 batch loss 1.24760413 epoch total loss 1.12838197\n",
      "Trained batch 1116 batch loss 1.18753743 epoch total loss 1.1284349\n",
      "Trained batch 1117 batch loss 1.24707186 epoch total loss 1.12854111\n",
      "Trained batch 1118 batch loss 1.301018 epoch total loss 1.12869537\n",
      "Trained batch 1119 batch loss 1.20487761 epoch total loss 1.12876344\n",
      "Trained batch 1120 batch loss 1.28128242 epoch total loss 1.12889957\n",
      "Trained batch 1121 batch loss 1.20752168 epoch total loss 1.12896979\n",
      "Trained batch 1122 batch loss 1.09397757 epoch total loss 1.12893856\n",
      "Trained batch 1123 batch loss 1.24648213 epoch total loss 1.12904322\n",
      "Trained batch 1124 batch loss 1.15812874 epoch total loss 1.12906909\n",
      "Trained batch 1125 batch loss 1.3347435 epoch total loss 1.12925184\n",
      "Trained batch 1126 batch loss 1.16401 epoch total loss 1.12928271\n",
      "Trained batch 1127 batch loss 1.17343056 epoch total loss 1.12932193\n",
      "Trained batch 1128 batch loss 1.11820304 epoch total loss 1.12931204\n",
      "Trained batch 1129 batch loss 1.10623538 epoch total loss 1.12929165\n",
      "Trained batch 1130 batch loss 1.03374028 epoch total loss 1.12920702\n",
      "Trained batch 1131 batch loss 1.09837353 epoch total loss 1.12917972\n",
      "Trained batch 1132 batch loss 1.05429184 epoch total loss 1.12911367\n",
      "Trained batch 1133 batch loss 1.14434874 epoch total loss 1.12912714\n",
      "Trained batch 1134 batch loss 1.09733284 epoch total loss 1.12909901\n",
      "Trained batch 1135 batch loss 1.01490617 epoch total loss 1.1289984\n",
      "Trained batch 1136 batch loss 1.08916545 epoch total loss 1.12896335\n",
      "Trained batch 1137 batch loss 1.07430291 epoch total loss 1.12891531\n",
      "Trained batch 1138 batch loss 0.973035812 epoch total loss 1.12877834\n",
      "Trained batch 1139 batch loss 0.979579747 epoch total loss 1.12864733\n",
      "Trained batch 1140 batch loss 1.01782072 epoch total loss 1.12855\n",
      "Trained batch 1141 batch loss 1.14585876 epoch total loss 1.12856531\n",
      "Trained batch 1142 batch loss 1.159307 epoch total loss 1.12859225\n",
      "Trained batch 1143 batch loss 1.14037228 epoch total loss 1.1286025\n",
      "Trained batch 1144 batch loss 1.10857153 epoch total loss 1.12858498\n",
      "Trained batch 1145 batch loss 1.14810264 epoch total loss 1.12860203\n",
      "Trained batch 1146 batch loss 1.26308608 epoch total loss 1.12871933\n",
      "Trained batch 1147 batch loss 1.17170477 epoch total loss 1.12875688\n",
      "Trained batch 1148 batch loss 1.19030893 epoch total loss 1.12881041\n",
      "Trained batch 1149 batch loss 1.18085361 epoch total loss 1.12885582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1150 batch loss 1.14517117 epoch total loss 1.12886989\n",
      "Trained batch 1151 batch loss 1.09820426 epoch total loss 1.12884331\n",
      "Trained batch 1152 batch loss 0.997379959 epoch total loss 1.12872922\n",
      "Trained batch 1153 batch loss 0.939515114 epoch total loss 1.12856507\n",
      "Trained batch 1154 batch loss 1.00697899 epoch total loss 1.12845981\n",
      "Trained batch 1155 batch loss 1.12291884 epoch total loss 1.12845492\n",
      "Trained batch 1156 batch loss 1.22489309 epoch total loss 1.12853837\n",
      "Trained batch 1157 batch loss 1.0410943 epoch total loss 1.12846279\n",
      "Trained batch 1158 batch loss 1.29409599 epoch total loss 1.12860584\n",
      "Trained batch 1159 batch loss 1.17049432 epoch total loss 1.12864196\n",
      "Trained batch 1160 batch loss 1.12792969 epoch total loss 1.12864137\n",
      "Trained batch 1161 batch loss 1.13354897 epoch total loss 1.12864566\n",
      "Trained batch 1162 batch loss 1.10284209 epoch total loss 1.12862337\n",
      "Trained batch 1163 batch loss 1.1000526 epoch total loss 1.12859881\n",
      "Trained batch 1164 batch loss 1.16305137 epoch total loss 1.12862849\n",
      "Trained batch 1165 batch loss 0.838465571 epoch total loss 1.12837946\n",
      "Trained batch 1166 batch loss 0.895118 epoch total loss 1.12817943\n",
      "Trained batch 1167 batch loss 1.11032557 epoch total loss 1.12816405\n",
      "Trained batch 1168 batch loss 1.19617438 epoch total loss 1.12822235\n",
      "Trained batch 1169 batch loss 1.24667704 epoch total loss 1.12832367\n",
      "Trained batch 1170 batch loss 1.19043756 epoch total loss 1.12837672\n",
      "Trained batch 1171 batch loss 1.26050329 epoch total loss 1.12848961\n",
      "Trained batch 1172 batch loss 1.23194599 epoch total loss 1.12857783\n",
      "Trained batch 1173 batch loss 1.25407624 epoch total loss 1.12868476\n",
      "Trained batch 1174 batch loss 1.2269702 epoch total loss 1.12876844\n",
      "Trained batch 1175 batch loss 1.13356566 epoch total loss 1.1287725\n",
      "Trained batch 1176 batch loss 1.15369487 epoch total loss 1.12879372\n",
      "Trained batch 1177 batch loss 1.14913034 epoch total loss 1.128811\n",
      "Trained batch 1178 batch loss 1.14007378 epoch total loss 1.12882054\n",
      "Trained batch 1179 batch loss 1.14594972 epoch total loss 1.12883508\n",
      "Trained batch 1180 batch loss 1.14112532 epoch total loss 1.12884557\n",
      "Trained batch 1181 batch loss 1.16034722 epoch total loss 1.12887228\n",
      "Trained batch 1182 batch loss 1.11449218 epoch total loss 1.12886012\n",
      "Trained batch 1183 batch loss 1.1362 epoch total loss 1.12886631\n",
      "Trained batch 1184 batch loss 1.18248248 epoch total loss 1.12891161\n",
      "Trained batch 1185 batch loss 1.06229162 epoch total loss 1.12885535\n",
      "Trained batch 1186 batch loss 1.0813086 epoch total loss 1.12881529\n",
      "Trained batch 1187 batch loss 1.02914166 epoch total loss 1.12873137\n",
      "Trained batch 1188 batch loss 1.0558157 epoch total loss 1.12867\n",
      "Trained batch 1189 batch loss 1.02766776 epoch total loss 1.12858498\n",
      "Trained batch 1190 batch loss 1.01592851 epoch total loss 1.12849033\n",
      "Trained batch 1191 batch loss 1.13766527 epoch total loss 1.12849796\n",
      "Trained batch 1192 batch loss 1.06381917 epoch total loss 1.12844372\n",
      "Trained batch 1193 batch loss 1.19291353 epoch total loss 1.12849784\n",
      "Trained batch 1194 batch loss 1.21107137 epoch total loss 1.12856698\n",
      "Trained batch 1195 batch loss 1.17704737 epoch total loss 1.12860751\n",
      "Trained batch 1196 batch loss 0.98062706 epoch total loss 1.12848365\n",
      "Trained batch 1197 batch loss 1.00176942 epoch total loss 1.1283778\n",
      "Trained batch 1198 batch loss 1.06283724 epoch total loss 1.12832308\n",
      "Trained batch 1199 batch loss 1.09050941 epoch total loss 1.12829149\n",
      "Trained batch 1200 batch loss 1.20479286 epoch total loss 1.12835526\n",
      "Trained batch 1201 batch loss 1.18951988 epoch total loss 1.12840629\n",
      "Trained batch 1202 batch loss 1.16792011 epoch total loss 1.12843919\n",
      "Trained batch 1203 batch loss 1.22097468 epoch total loss 1.12851608\n",
      "Trained batch 1204 batch loss 1.21987653 epoch total loss 1.1285919\n",
      "Trained batch 1205 batch loss 1.26563907 epoch total loss 1.12870562\n",
      "Trained batch 1206 batch loss 1.27553678 epoch total loss 1.12882745\n",
      "Trained batch 1207 batch loss 1.23008525 epoch total loss 1.12891126\n",
      "Trained batch 1208 batch loss 1.09359479 epoch total loss 1.12888205\n",
      "Trained batch 1209 batch loss 1.07031441 epoch total loss 1.12883365\n",
      "Trained batch 1210 batch loss 1.11357915 epoch total loss 1.12882102\n",
      "Trained batch 1211 batch loss 1.03512383 epoch total loss 1.12874365\n",
      "Trained batch 1212 batch loss 1.03862619 epoch total loss 1.12866926\n",
      "Trained batch 1213 batch loss 1.11909938 epoch total loss 1.12866139\n",
      "Trained batch 1214 batch loss 1.00429022 epoch total loss 1.12855899\n",
      "Trained batch 1215 batch loss 1.27888572 epoch total loss 1.12868273\n",
      "Trained batch 1216 batch loss 1.1022104 epoch total loss 1.12866092\n",
      "Trained batch 1217 batch loss 1.24601603 epoch total loss 1.12875736\n",
      "Trained batch 1218 batch loss 1.12048864 epoch total loss 1.12875056\n",
      "Trained batch 1219 batch loss 1.22071886 epoch total loss 1.1288259\n",
      "Trained batch 1220 batch loss 1.26998186 epoch total loss 1.12894166\n",
      "Trained batch 1221 batch loss 1.13604808 epoch total loss 1.1289475\n",
      "Trained batch 1222 batch loss 1.03999257 epoch total loss 1.12887478\n",
      "Trained batch 1223 batch loss 1.14365852 epoch total loss 1.12888694\n",
      "Trained batch 1224 batch loss 1.20877218 epoch total loss 1.12895215\n",
      "Trained batch 1225 batch loss 1.29544687 epoch total loss 1.12908804\n",
      "Trained batch 1226 batch loss 1.20503497 epoch total loss 1.12915\n",
      "Trained batch 1227 batch loss 1.097247 epoch total loss 1.12912405\n",
      "Trained batch 1228 batch loss 1.23020172 epoch total loss 1.1292063\n",
      "Trained batch 1229 batch loss 1.17059398 epoch total loss 1.12924\n",
      "Trained batch 1230 batch loss 1.13922429 epoch total loss 1.12924826\n",
      "Trained batch 1231 batch loss 1.36656642 epoch total loss 1.12944102\n",
      "Trained batch 1232 batch loss 1.23160946 epoch total loss 1.12952399\n",
      "Trained batch 1233 batch loss 1.25652707 epoch total loss 1.12962687\n",
      "Trained batch 1234 batch loss 1.08831286 epoch total loss 1.12959337\n",
      "Trained batch 1235 batch loss 1.08292437 epoch total loss 1.12955558\n",
      "Trained batch 1236 batch loss 0.993106425 epoch total loss 1.1294452\n",
      "Trained batch 1237 batch loss 1.03357768 epoch total loss 1.12936771\n",
      "Trained batch 1238 batch loss 1.11949837 epoch total loss 1.12935972\n",
      "Trained batch 1239 batch loss 1.01883459 epoch total loss 1.12927043\n",
      "Trained batch 1240 batch loss 0.988234043 epoch total loss 1.12915683\n",
      "Trained batch 1241 batch loss 0.863580585 epoch total loss 1.12894273\n",
      "Trained batch 1242 batch loss 0.969686 epoch total loss 1.12881458\n",
      "Trained batch 1243 batch loss 1.16170835 epoch total loss 1.12884104\n",
      "Trained batch 1244 batch loss 1.14075065 epoch total loss 1.12885058\n",
      "Trained batch 1245 batch loss 1.28240085 epoch total loss 1.12897396\n",
      "Trained batch 1246 batch loss 1.33408272 epoch total loss 1.12913859\n",
      "Trained batch 1247 batch loss 1.25912189 epoch total loss 1.12924278\n",
      "Trained batch 1248 batch loss 1.26429343 epoch total loss 1.12935102\n",
      "Trained batch 1249 batch loss 1.08950162 epoch total loss 1.12931907\n",
      "Trained batch 1250 batch loss 0.994288564 epoch total loss 1.12921107\n",
      "Trained batch 1251 batch loss 0.998099208 epoch total loss 1.12910616\n",
      "Trained batch 1252 batch loss 1.14785707 epoch total loss 1.12912118\n",
      "Trained batch 1253 batch loss 1.18992651 epoch total loss 1.1291697\n",
      "Trained batch 1254 batch loss 1.0979023 epoch total loss 1.12914479\n",
      "Trained batch 1255 batch loss 1.12501097 epoch total loss 1.12914145\n",
      "Trained batch 1256 batch loss 1.15418398 epoch total loss 1.12916136\n",
      "Trained batch 1257 batch loss 1.14545166 epoch total loss 1.12917435\n",
      "Trained batch 1258 batch loss 1.10153651 epoch total loss 1.12915242\n",
      "Trained batch 1259 batch loss 0.897426605 epoch total loss 1.12896836\n",
      "Trained batch 1260 batch loss 0.990788221 epoch total loss 1.1288588\n",
      "Trained batch 1261 batch loss 1.22788835 epoch total loss 1.12893736\n",
      "Trained batch 1262 batch loss 1.215657 epoch total loss 1.12900603\n",
      "Trained batch 1263 batch loss 1.23825443 epoch total loss 1.12909257\n",
      "Trained batch 1264 batch loss 1.1043067 epoch total loss 1.1290729\n",
      "Trained batch 1265 batch loss 1.30256534 epoch total loss 1.12921011\n",
      "Trained batch 1266 batch loss 1.13397145 epoch total loss 1.12921381\n",
      "Trained batch 1267 batch loss 1.2495755 epoch total loss 1.12930894\n",
      "Trained batch 1268 batch loss 1.29880881 epoch total loss 1.12944257\n",
      "Trained batch 1269 batch loss 1.18325639 epoch total loss 1.12948501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1270 batch loss 1.11277556 epoch total loss 1.12947178\n",
      "Trained batch 1271 batch loss 1.04789984 epoch total loss 1.12940764\n",
      "Trained batch 1272 batch loss 1.00941682 epoch total loss 1.12931323\n",
      "Trained batch 1273 batch loss 0.857518256 epoch total loss 1.12909973\n",
      "Trained batch 1274 batch loss 1.03265142 epoch total loss 1.12902403\n",
      "Trained batch 1275 batch loss 1.13445854 epoch total loss 1.1290282\n",
      "Trained batch 1276 batch loss 1.18963087 epoch total loss 1.12907565\n",
      "Trained batch 1277 batch loss 1.17889667 epoch total loss 1.12911475\n",
      "Trained batch 1278 batch loss 1.16667247 epoch total loss 1.12914407\n",
      "Trained batch 1279 batch loss 1.23857355 epoch total loss 1.12922966\n",
      "Trained batch 1280 batch loss 1.10280561 epoch total loss 1.12920892\n",
      "Trained batch 1281 batch loss 1.0897522 epoch total loss 1.12917817\n",
      "Trained batch 1282 batch loss 1.1445756 epoch total loss 1.12919009\n",
      "Trained batch 1283 batch loss 0.962591 epoch total loss 1.12906027\n",
      "Trained batch 1284 batch loss 1.05885553 epoch total loss 1.12900555\n",
      "Trained batch 1285 batch loss 0.988781 epoch total loss 1.12889647\n",
      "Trained batch 1286 batch loss 1.05625725 epoch total loss 1.12884\n",
      "Trained batch 1287 batch loss 0.908850491 epoch total loss 1.12866902\n",
      "Trained batch 1288 batch loss 1.01344299 epoch total loss 1.12857962\n",
      "Trained batch 1289 batch loss 1.03800154 epoch total loss 1.12850928\n",
      "Trained batch 1290 batch loss 1.07046759 epoch total loss 1.12846422\n",
      "Trained batch 1291 batch loss 1.06343365 epoch total loss 1.12841392\n",
      "Trained batch 1292 batch loss 1.08433628 epoch total loss 1.12837982\n",
      "Trained batch 1293 batch loss 1.13323534 epoch total loss 1.12838352\n",
      "Trained batch 1294 batch loss 1.04904556 epoch total loss 1.12832224\n",
      "Trained batch 1295 batch loss 1.26640153 epoch total loss 1.12842882\n",
      "Trained batch 1296 batch loss 1.14253759 epoch total loss 1.12843978\n",
      "Trained batch 1297 batch loss 1.07585847 epoch total loss 1.12839913\n",
      "Trained batch 1298 batch loss 1.14167476 epoch total loss 1.12840939\n",
      "Trained batch 1299 batch loss 1.11825013 epoch total loss 1.12840164\n",
      "Trained batch 1300 batch loss 1.10206103 epoch total loss 1.12838137\n",
      "Trained batch 1301 batch loss 1.00906014 epoch total loss 1.12828958\n",
      "Trained batch 1302 batch loss 1.13935614 epoch total loss 1.12829816\n",
      "Trained batch 1303 batch loss 1.04213619 epoch total loss 1.128232\n",
      "Trained batch 1304 batch loss 1.09608936 epoch total loss 1.12820733\n",
      "Trained batch 1305 batch loss 1.04640019 epoch total loss 1.12814462\n",
      "Trained batch 1306 batch loss 1.09928656 epoch total loss 1.12812257\n",
      "Trained batch 1307 batch loss 1.12563872 epoch total loss 1.12812054\n",
      "Trained batch 1308 batch loss 1.13810849 epoch total loss 1.12812817\n",
      "Trained batch 1309 batch loss 1.14717948 epoch total loss 1.12814283\n",
      "Trained batch 1310 batch loss 1.17868555 epoch total loss 1.12818134\n",
      "Trained batch 1311 batch loss 1.07370162 epoch total loss 1.12813985\n",
      "Trained batch 1312 batch loss 1.09410512 epoch total loss 1.12811387\n",
      "Trained batch 1313 batch loss 1.14939857 epoch total loss 1.12813008\n",
      "Trained batch 1314 batch loss 1.16867661 epoch total loss 1.12816095\n",
      "Trained batch 1315 batch loss 1.16101718 epoch total loss 1.12818599\n",
      "Trained batch 1316 batch loss 1.16422653 epoch total loss 1.12821329\n",
      "Trained batch 1317 batch loss 1.18293726 epoch total loss 1.12825489\n",
      "Trained batch 1318 batch loss 1.12907851 epoch total loss 1.12825549\n",
      "Trained batch 1319 batch loss 1.00148535 epoch total loss 1.1281594\n",
      "Trained batch 1320 batch loss 1.11580884 epoch total loss 1.12815011\n",
      "Trained batch 1321 batch loss 1.19278288 epoch total loss 1.12819898\n",
      "Trained batch 1322 batch loss 1.11386573 epoch total loss 1.12818813\n",
      "Trained batch 1323 batch loss 1.11146927 epoch total loss 1.1281755\n",
      "Trained batch 1324 batch loss 1.09632194 epoch total loss 1.12815142\n",
      "Trained batch 1325 batch loss 1.17876625 epoch total loss 1.12818956\n",
      "Trained batch 1326 batch loss 1.12388587 epoch total loss 1.12818635\n",
      "Trained batch 1327 batch loss 1.20380187 epoch total loss 1.12824333\n",
      "Trained batch 1328 batch loss 1.31461537 epoch total loss 1.12838364\n",
      "Trained batch 1329 batch loss 1.28891218 epoch total loss 1.12850451\n",
      "Trained batch 1330 batch loss 1.19054985 epoch total loss 1.12855113\n",
      "Trained batch 1331 batch loss 1.15010965 epoch total loss 1.12856734\n",
      "Trained batch 1332 batch loss 1.10740077 epoch total loss 1.12855148\n",
      "Trained batch 1333 batch loss 1.07504392 epoch total loss 1.12851143\n",
      "Trained batch 1334 batch loss 1.17665267 epoch total loss 1.12854743\n",
      "Trained batch 1335 batch loss 1.25107932 epoch total loss 1.12863922\n",
      "Trained batch 1336 batch loss 1.31598043 epoch total loss 1.12877953\n",
      "Trained batch 1337 batch loss 1.32266104 epoch total loss 1.12892449\n",
      "Trained batch 1338 batch loss 1.04327 epoch total loss 1.12886047\n",
      "Trained batch 1339 batch loss 1.24430299 epoch total loss 1.12894666\n",
      "Trained batch 1340 batch loss 1.18361342 epoch total loss 1.12898743\n",
      "Trained batch 1341 batch loss 1.21148968 epoch total loss 1.12904894\n",
      "Trained batch 1342 batch loss 1.22055566 epoch total loss 1.12911713\n",
      "Trained batch 1343 batch loss 1.16971338 epoch total loss 1.12914741\n",
      "Trained batch 1344 batch loss 1.02342582 epoch total loss 1.12906873\n",
      "Trained batch 1345 batch loss 1.0442071 epoch total loss 1.12900567\n",
      "Trained batch 1346 batch loss 1.07341743 epoch total loss 1.1289643\n",
      "Trained batch 1347 batch loss 1.15496862 epoch total loss 1.12898362\n",
      "Trained batch 1348 batch loss 1.17615247 epoch total loss 1.12901866\n",
      "Trained batch 1349 batch loss 1.12645125 epoch total loss 1.12901676\n",
      "Trained batch 1350 batch loss 1.21692777 epoch total loss 1.12908185\n",
      "Trained batch 1351 batch loss 1.07671189 epoch total loss 1.1290431\n",
      "Trained batch 1352 batch loss 1.1611973 epoch total loss 1.12906682\n",
      "Trained batch 1353 batch loss 1.20372164 epoch total loss 1.12912202\n",
      "Trained batch 1354 batch loss 1.2263974 epoch total loss 1.1291939\n",
      "Trained batch 1355 batch loss 1.25231886 epoch total loss 1.12928486\n",
      "Trained batch 1356 batch loss 1.06902647 epoch total loss 1.12924027\n",
      "Trained batch 1357 batch loss 1.04715955 epoch total loss 1.12917984\n",
      "Trained batch 1358 batch loss 1.08119392 epoch total loss 1.12914443\n",
      "Trained batch 1359 batch loss 1.12668872 epoch total loss 1.12914264\n",
      "Trained batch 1360 batch loss 1.24501383 epoch total loss 1.12922788\n",
      "Trained batch 1361 batch loss 1.18098116 epoch total loss 1.1292659\n",
      "Trained batch 1362 batch loss 1.28995371 epoch total loss 1.12938392\n",
      "Trained batch 1363 batch loss 1.28131926 epoch total loss 1.12949538\n",
      "Trained batch 1364 batch loss 1.30324817 epoch total loss 1.1296227\n",
      "Trained batch 1365 batch loss 1.19006944 epoch total loss 1.12966704\n",
      "Trained batch 1366 batch loss 1.08229804 epoch total loss 1.12963235\n",
      "Trained batch 1367 batch loss 1.05814862 epoch total loss 1.12958\n",
      "Trained batch 1368 batch loss 1.02892184 epoch total loss 1.12950647\n",
      "Trained batch 1369 batch loss 1.15816653 epoch total loss 1.12952745\n",
      "Trained batch 1370 batch loss 1.09127903 epoch total loss 1.12949944\n",
      "Trained batch 1371 batch loss 1.06666279 epoch total loss 1.12945366\n",
      "Trained batch 1372 batch loss 1.01866853 epoch total loss 1.12937295\n",
      "Trained batch 1373 batch loss 1.04810309 epoch total loss 1.12931371\n",
      "Trained batch 1374 batch loss 1.0937748 epoch total loss 1.12928784\n",
      "Trained batch 1375 batch loss 1.10689902 epoch total loss 1.12927163\n",
      "Trained batch 1376 batch loss 1.11464357 epoch total loss 1.1292609\n",
      "Trained batch 1377 batch loss 1.21625113 epoch total loss 1.1293242\n",
      "Trained batch 1378 batch loss 1.06078541 epoch total loss 1.12927437\n",
      "Trained batch 1379 batch loss 1.09206164 epoch total loss 1.12924743\n",
      "Trained batch 1380 batch loss 1.05925298 epoch total loss 1.12919664\n",
      "Trained batch 1381 batch loss 1.09619069 epoch total loss 1.1291728\n",
      "Trained batch 1382 batch loss 1.02738082 epoch total loss 1.12909913\n",
      "Trained batch 1383 batch loss 1.2191478 epoch total loss 1.12916422\n",
      "Trained batch 1384 batch loss 1.09225357 epoch total loss 1.12913752\n",
      "Trained batch 1385 batch loss 1.2149477 epoch total loss 1.1291995\n",
      "Trained batch 1386 batch loss 0.982475638 epoch total loss 1.12909353\n",
      "Trained batch 1387 batch loss 1.12434745 epoch total loss 1.12909019\n",
      "Trained batch 1388 batch loss 1.08660221 epoch total loss 1.12905955\n",
      "Epoch 7 train loss 1.1290595531463623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 1.13863516\n",
      "Validated batch 2 batch loss 1.1363399\n",
      "Validated batch 3 batch loss 1.10370159\n",
      "Validated batch 4 batch loss 1.17313349\n",
      "Validated batch 5 batch loss 1.11260891\n",
      "Validated batch 6 batch loss 1.23367953\n",
      "Validated batch 7 batch loss 1.25413203\n",
      "Validated batch 8 batch loss 1.19763136\n",
      "Validated batch 9 batch loss 1.13833833\n",
      "Validated batch 10 batch loss 1.07598579\n",
      "Validated batch 11 batch loss 1.17051065\n",
      "Validated batch 12 batch loss 1.12782288\n",
      "Validated batch 13 batch loss 1.17438352\n",
      "Validated batch 14 batch loss 1.30514264\n",
      "Validated batch 15 batch loss 1.23325443\n",
      "Validated batch 16 batch loss 1.16088033\n",
      "Validated batch 17 batch loss 1.28616965\n",
      "Validated batch 18 batch loss 1.09049773\n",
      "Validated batch 19 batch loss 1.2576108\n",
      "Validated batch 20 batch loss 0.915092409\n",
      "Validated batch 21 batch loss 1.11054707\n",
      "Validated batch 22 batch loss 1.1876266\n",
      "Validated batch 23 batch loss 1.02113771\n",
      "Validated batch 24 batch loss 1.0852468\n",
      "Validated batch 25 batch loss 1.07017422\n",
      "Validated batch 26 batch loss 1.0969435\n",
      "Validated batch 27 batch loss 1.10903847\n",
      "Validated batch 28 batch loss 1.07493508\n",
      "Validated batch 29 batch loss 1.22117448\n",
      "Validated batch 30 batch loss 1.0900501\n",
      "Validated batch 31 batch loss 0.964963853\n",
      "Validated batch 32 batch loss 1.03244579\n",
      "Validated batch 33 batch loss 1.08146429\n",
      "Validated batch 34 batch loss 1.04042053\n",
      "Validated batch 35 batch loss 1.08375788\n",
      "Validated batch 36 batch loss 1.06595612\n",
      "Validated batch 37 batch loss 1.08661914\n",
      "Validated batch 38 batch loss 1.24518776\n",
      "Validated batch 39 batch loss 1.14350665\n",
      "Validated batch 40 batch loss 1.11499286\n",
      "Validated batch 41 batch loss 1.23389864\n",
      "Validated batch 42 batch loss 0.899214804\n",
      "Validated batch 43 batch loss 1.05602324\n",
      "Validated batch 44 batch loss 0.997086525\n",
      "Validated batch 45 batch loss 1.13712263\n",
      "Validated batch 46 batch loss 1.28112721\n",
      "Validated batch 47 batch loss 1.15747416\n",
      "Validated batch 48 batch loss 1.15268266\n",
      "Validated batch 49 batch loss 1.11494529\n",
      "Validated batch 50 batch loss 1.15762627\n",
      "Validated batch 51 batch loss 1.26624227\n",
      "Validated batch 52 batch loss 1.35766852\n",
      "Validated batch 53 batch loss 1.11995649\n",
      "Validated batch 54 batch loss 1.22815466\n",
      "Validated batch 55 batch loss 1.11618388\n",
      "Validated batch 56 batch loss 1.20452833\n",
      "Validated batch 57 batch loss 1.21177351\n",
      "Validated batch 58 batch loss 1.06927943\n",
      "Validated batch 59 batch loss 1.3356806\n",
      "Validated batch 60 batch loss 1.14260113\n",
      "Validated batch 61 batch loss 1.24444056\n",
      "Validated batch 62 batch loss 1.16597104\n",
      "Validated batch 63 batch loss 1.26999938\n",
      "Validated batch 64 batch loss 1.05862868\n",
      "Validated batch 65 batch loss 1.22064853\n",
      "Validated batch 66 batch loss 1.12073243\n",
      "Validated batch 67 batch loss 1.13835585\n",
      "Validated batch 68 batch loss 1.20953798\n",
      "Validated batch 69 batch loss 1.15272284\n",
      "Validated batch 70 batch loss 1.19127238\n",
      "Validated batch 71 batch loss 1.11714411\n",
      "Validated batch 72 batch loss 1.12612772\n",
      "Validated batch 73 batch loss 1.04894757\n",
      "Validated batch 74 batch loss 1.09626579\n",
      "Validated batch 75 batch loss 1.16357589\n",
      "Validated batch 76 batch loss 1.1616528\n",
      "Validated batch 77 batch loss 1.2289921\n",
      "Validated batch 78 batch loss 1.17077136\n",
      "Validated batch 79 batch loss 1.1208818\n",
      "Validated batch 80 batch loss 1.19011879\n",
      "Validated batch 81 batch loss 1.33985305\n",
      "Validated batch 82 batch loss 1.24876189\n",
      "Validated batch 83 batch loss 1.26208127\n",
      "Validated batch 84 batch loss 1.25150383\n",
      "Validated batch 85 batch loss 1.25204468\n",
      "Validated batch 86 batch loss 1.26091456\n",
      "Validated batch 87 batch loss 1.13095784\n",
      "Validated batch 88 batch loss 1.18153667\n",
      "Validated batch 89 batch loss 1.29268181\n",
      "Validated batch 90 batch loss 1.19952571\n",
      "Validated batch 91 batch loss 1.10016191\n",
      "Validated batch 92 batch loss 1.18970644\n",
      "Validated batch 93 batch loss 1.15746236\n",
      "Validated batch 94 batch loss 1.12290382\n",
      "Validated batch 95 batch loss 1.08461678\n",
      "Validated batch 96 batch loss 1.15294552\n",
      "Validated batch 97 batch loss 1.1336019\n",
      "Validated batch 98 batch loss 1.13784695\n",
      "Validated batch 99 batch loss 1.18929839\n",
      "Validated batch 100 batch loss 1.16763663\n",
      "Validated batch 101 batch loss 1.11227274\n",
      "Validated batch 102 batch loss 1.27083349\n",
      "Validated batch 103 batch loss 1.07114542\n",
      "Validated batch 104 batch loss 1.01473105\n",
      "Validated batch 105 batch loss 1.15753865\n",
      "Validated batch 106 batch loss 1.26226187\n",
      "Validated batch 107 batch loss 1.3071717\n",
      "Validated batch 108 batch loss 1.32792199\n",
      "Validated batch 109 batch loss 1.15833807\n",
      "Validated batch 110 batch loss 1.29667044\n",
      "Validated batch 111 batch loss 1.16768062\n",
      "Validated batch 112 batch loss 1.22115076\n",
      "Validated batch 113 batch loss 1.21124136\n",
      "Validated batch 114 batch loss 0.875317276\n",
      "Validated batch 115 batch loss 1.07818425\n",
      "Validated batch 116 batch loss 1.06382394\n",
      "Validated batch 117 batch loss 1.05662274\n",
      "Validated batch 118 batch loss 1.21832132\n",
      "Validated batch 119 batch loss 1.05453503\n",
      "Validated batch 120 batch loss 1.18765092\n",
      "Validated batch 121 batch loss 1.32285285\n",
      "Validated batch 122 batch loss 1.01210499\n",
      "Validated batch 123 batch loss 1.08665419\n",
      "Validated batch 124 batch loss 1.14195907\n",
      "Validated batch 125 batch loss 1.15266728\n",
      "Validated batch 126 batch loss 1.21500826\n",
      "Validated batch 127 batch loss 1.07575834\n",
      "Validated batch 128 batch loss 0.880997658\n",
      "Validated batch 129 batch loss 1.11412954\n",
      "Validated batch 130 batch loss 1.03914595\n",
      "Validated batch 131 batch loss 1.0617373\n",
      "Validated batch 132 batch loss 1.18696332\n",
      "Validated batch 133 batch loss 0.970266819\n",
      "Validated batch 134 batch loss 1.12704396\n",
      "Validated batch 135 batch loss 1.23069608\n",
      "Validated batch 136 batch loss 1.19373345\n",
      "Validated batch 137 batch loss 1.14855933\n",
      "Validated batch 138 batch loss 1.00054073\n",
      "Validated batch 139 batch loss 1.20612931\n",
      "Validated batch 140 batch loss 1.13247311\n",
      "Validated batch 141 batch loss 1.14796495\n",
      "Validated batch 142 batch loss 1.14056349\n",
      "Validated batch 143 batch loss 1.00504589\n",
      "Validated batch 144 batch loss 1.13266242\n",
      "Validated batch 145 batch loss 1.19671845\n",
      "Validated batch 146 batch loss 1.12713134\n",
      "Validated batch 147 batch loss 1.13721299\n",
      "Validated batch 148 batch loss 1.1828233\n",
      "Validated batch 149 batch loss 1.16271853\n",
      "Validated batch 150 batch loss 1.21032047\n",
      "Validated batch 151 batch loss 1.1808809\n",
      "Validated batch 152 batch loss 1.03203022\n",
      "Validated batch 153 batch loss 1.08978653\n",
      "Validated batch 154 batch loss 1.13432717\n",
      "Validated batch 155 batch loss 1.05847526\n",
      "Validated batch 156 batch loss 1.15182877\n",
      "Validated batch 157 batch loss 1.12337506\n",
      "Validated batch 158 batch loss 1.28973281\n",
      "Validated batch 159 batch loss 1.26791227\n",
      "Validated batch 160 batch loss 1.15535736\n",
      "Validated batch 161 batch loss 1.04500842\n",
      "Validated batch 162 batch loss 1.03811705\n",
      "Validated batch 163 batch loss 1.03798079\n",
      "Validated batch 164 batch loss 1.13344765\n",
      "Validated batch 165 batch loss 1.12404966\n",
      "Validated batch 166 batch loss 1.06286\n",
      "Validated batch 167 batch loss 1.19228959\n",
      "Validated batch 168 batch loss 1.09595096\n",
      "Validated batch 169 batch loss 1.14236951\n",
      "Validated batch 170 batch loss 1.25703812\n",
      "Validated batch 171 batch loss 0.981770158\n",
      "Validated batch 172 batch loss 1.19333434\n",
      "Validated batch 173 batch loss 1.14666939\n",
      "Validated batch 174 batch loss 1.03933263\n",
      "Validated batch 175 batch loss 1.17489493\n",
      "Validated batch 176 batch loss 1.17350209\n",
      "Validated batch 177 batch loss 1.06218743\n",
      "Validated batch 178 batch loss 1.24855638\n",
      "Validated batch 179 batch loss 1.18063951\n",
      "Validated batch 180 batch loss 1.23177278\n",
      "Validated batch 181 batch loss 1.03245628\n",
      "Validated batch 182 batch loss 1.21675897\n",
      "Validated batch 183 batch loss 1.11919594\n",
      "Validated batch 184 batch loss 1.04784393\n",
      "Validated batch 185 batch loss 1.18941295\n",
      "Epoch 7 val loss 1.1460052728652954\n",
      "Model /aiffel/aiffel/mpii/a/model-epoch-7-loss-1.1460.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 7\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b4ccd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-v0.0.1-epoch-2-loss-1.3072.h5')\n",
    "\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "# model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "# 이전의 학습하는 코드 블럭을 통해 학습하고 그 모델을 사용할 경우 아래 주석 처리된 코드를 사용하면 됩니다\n",
    "model.load_weights(best_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9bd07d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20e4ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "312579e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58000b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4361a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5e89b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e6039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82f65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bcc966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f795f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fb981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a944f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f226c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
